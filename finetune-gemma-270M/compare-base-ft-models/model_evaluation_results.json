[
  {
    "index": 0,
    "title": "Demo with QHub and Bodo",
    "body": "Create a demo using Bodo and HPC:\r\n* Seems like QHub is not in the position to demo Bodo at the moment;\r\n* Use HPC instead for now.\r\n* Ideally, multimode for now. \"The spirit of Bodo is lost if we don't do multinode\".",
    "actual_labels": [
      "enhancement",
      "documentation",
      "performance"
    ],
    "predicted_labels": "demo, QHub, HPC",
    "model_name": "base"
  },
  {
    "index": 0,
    "title": "Demo with QHub and Bodo",
    "body": "Create a demo using Bodo and HPC:\r\n* Seems like QHub is not in the position to demo Bodo at the moment;\r\n* Use HPC instead for now.\r\n* Ideally, multimode for now. \"The spirit of Bodo is lost if we don't do multinode\".",
    "actual_labels": [
      "enhancement",
      "documentation",
      "performance"
    ],
    "predicted_labels": "demo, maintenance, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 1,
    "title": "Kubeconfig state unavailable, Terraform defaults to localhost [bug]",
    "body": "Seen most often in the upgrade from 0.3.12 to main (0.4 candidate) of an AWS cluster, Terraform kubernetes provider sometimes tries to access the Kubernetes cluster at localhost instead of the EKS host.\r\n\r\nThe problem is with these two QHub Terraform modules: `kubernetes` and `kubernetes-initialization`.\r\n\r\nThe `kubernetes` module creates the cluster itself, and then `kubernetes-initialization` creates the namespace inside the cluster (plus some secrets).\r\n\r\n`kubernetes-initialization` is configured like this:\r\n```\r\nprovider \"kubernetes\" {\r\n  host                   = module.kubernetes.credentials.endpoint\r\n  cluster_ca_certificate = module.kubernetes.credentials.cluster_ca_certificate\r\n  token                  = module.kubernetes.credentials.token\r\n}\r\n```\r\n\r\nAt the moment, there is a `terraform apply` step run targeting both these modules at the same time. The problem is that, if the state of the `kubernetes` module is not known (e.g. not refreshed in the right way at the right time), there is no way we can expect the `kubernetes-initialization` to have the appropriate provider configured correctly - because that depends on outputs of the earlier module.\r\n\r\nWe do set `kubernetes-initialization` to `depends_on` the `kubernetes` module, so `terraform apply` shouldn't try to create the namespace before the cluster exists, for example, but the provider needs to be configured at the start of the whole `terraform apply` step, otherwise it's as though we never passed through the settings such as `host` so it defaults to localhost.\r\n\r\nThe best description of this nuance is [probably here](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1307#issuecomment-873089000).\r\n\r\nAnyway, I will try splitting out the two modules into separate steps. That means an extra `terraform apply` call of course, and it might be better to see if we can instead push `kubernetes-initialization` into a joint step with the following module (`kubernetes-ingress`) if we set up appropriate `depends_on` hierarchies.\r\n\r\nBut in general, we need a proper multistage definition as per issue #847 .\r\n\r\n\r\n\r\n\r\n\r\n",
    "actual_labels": [
      "bug",
      "aws",
      "configuration",
      "ci/cd"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 1,
    "title": "Kubeconfig state unavailable, Terraform defaults to localhost [bug]",
    "body": "Seen most often in the upgrade from 0.3.12 to main (0.4 candidate) of an AWS cluster, Terraform kubernetes provider sometimes tries to access the Kubernetes cluster at localhost instead of the EKS host.\r\n\r\nThe problem is with these two QHub Terraform modules: `kubernetes` and `kubernetes-initialization`.\r\n\r\nThe `kubernetes` module creates the cluster itself, and then `kubernetes-initialization` creates the namespace inside the cluster (plus some secrets).\r\n\r\n`kubernetes-initialization` is configured like this:\r\n```\r\nprovider \"kubernetes\" {\r\n  host                   = module.kubernetes.credentials.endpoint\r\n  cluster_ca_certificate = module.kubernetes.credentials.cluster_ca_certificate\r\n  token                  = module.kubernetes.credentials.token\r\n}\r\n```\r\n\r\nAt the moment, there is a `terraform apply` step run targeting both these modules at the same time. The problem is that, if the state of the `kubernetes` module is not known (e.g. not refreshed in the right way at the right time), there is no way we can expect the `kubernetes-initialization` to have the appropriate provider configured correctly - because that depends on outputs of the earlier module.\r\n\r\nWe do set `kubernetes-initialization` to `depends_on` the `kubernetes` module, so `terraform apply` shouldn't try to create the namespace before the cluster exists, for example, but the provider needs to be configured at the start of the whole `terraform apply` step, otherwise it's as though we never passed through the settings such as `host` so it defaults to localhost.\r\n\r\nThe best description of this nuance is [probably here](https://github.com/hashicorp/terraform-provider-kubernetes/issues/1307#issuecomment-873089000).\r\n\r\nAnyway, I will try splitting out the two modules into separate steps. That means an extra `terraform apply` call of course, and it might be better to see if we can instead push `kubernetes-initialization` into a joint step with the following module (`kubernetes-ingress`) if we set up appropriate `depends_on` hierarchies.\r\n\r\nBut in general, we need a proper multistage definition as per issue #847 .\r\n\r\n\r\n\r\n\r\n\r\n",
    "actual_labels": [
      "bug",
      "aws",
      "configuration",
      "ci/cd"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 2,
    "title": "Review nebari dependencies and identify which need updates",
    "body": "### Context\n\nWe need to update our stuff.\n\n### Value and/or benefit\n\nOur stuff will be updated\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "maintenance",
      "dependency-update"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 2,
    "title": "Review nebari dependencies and identify which need updates",
    "body": "### Context\n\nWe need to update our stuff.\n\n### Value and/or benefit\n\nOur stuff will be updated\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "maintenance",
      "dependency-update"
    ],
    "predicted_labels": "enhancement, maintenance, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 3,
    "title": "[ENH] - nebari cost options are not consistent with the rest of the CLI",
    "body": "### Feature description\n\nI'm aware of \r\n\r\nhttps://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L420-L421\r\n\r\nbut we should probably still have consistent UI.\r\n\r\nI have three issues:\r\n\r\n1. https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L386-L391\r\n   This is the same as `-o` / `--output` of `nebari deploy`, right? If so, they should have the same flag. Maybe \"output\" is not the perfect name here, but I would value consistency higher.\r\n2. https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L404-L409\r\n   `-c` is used as a required argument for the nebari configuration file in all the main commands. We should just drop that here and only accept `--currency`.\r\n3. https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L412\r\n   https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L400\r\n   https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L394\r\n\r\n   Quoting from the [Command Line Interface Guide](https://clig.dev/#arguments-and-flags):\r\n\r\n   > Only use one-letter flags for commonly used flags, particularly at the top-level when using subcommands. That way you don\u2019t \u201cpollute\u201d your namespace of short flags, forcing you to use convoluted letters and cases for flags you add in the future.\r\n\r\n   We should probably drop them here.\n\n### Value and/or benefit\n\nConsistent UI lowers the entrance burden for new users and frustration of more experienced ones.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 3,
    "title": "[ENH] - nebari cost options are not consistent with the rest of the CLI",
    "body": "### Feature description\n\nI'm aware of \r\n\r\nhttps://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L420-L421\r\n\r\nbut we should probably still have consistent UI.\r\n\r\nI have three issues:\r\n\r\n1. https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L386-L391\r\n   This is the same as `-o` / `--output` of `nebari deploy`, right? If so, they should have the same flag. Maybe \"output\" is not the perfect name here, but I would value consistency higher.\r\n2. https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L404-L409\r\n   `-c` is used as a required argument for the nebari configuration file in all the main commands. We should just drop that here and only accept `--currency`.\r\n3. https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L412\r\n   https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L400\r\n   https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/cli/main.py#L394\r\n\r\n   Quoting from the [Command Line Interface Guide](https://clig.dev/#arguments-and-flags):\r\n\r\n   > Only use one-letter flags for commonly used flags, particularly at the top-level when using subcommands. That way you don\u2019t \u201cpollute\u201d your namespace of short flags, forcing you to use convoluted letters and cases for flags you add in the future.\r\n\r\n   We should probably drop them here.\n\n### Value and/or benefit\n\nConsistent UI lowers the entrance burden for new users and frustration of more experienced ones.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 4,
    "title": "Keycloak TF: failed to retrieve cryptographic signature for provider",
    "body": "In the past week I have run into the following error a few times when testing the QHub deployment:\r\n- [during the Kubernetes test](https://github.com/Quansight/qhub/runs/5837231631?check_suite_focus=true#step:19:5011)\r\n  - using `mrparker/keycloak v3.7.0`\r\n- [during the end-to-end integration test](https://github.com/Quansight/qhub-integration-test/runs/5856676069?check_suite_focus=true#step:4:8536)\r\n  - using `mrparker/keycloak v3.3.0`\r\n\r\n\r\nI reran the Kubernetes test again and the issue seemed to go away. ",
    "actual_labels": [
      "bug",
      "ci/cd",
      "testing"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 4,
    "title": "Keycloak TF: failed to retrieve cryptographic signature for provider",
    "body": "In the past week I have run into the following error a few times when testing the QHub deployment:\r\n- [during the Kubernetes test](https://github.com/Quansight/qhub/runs/5837231631?check_suite_focus=true#step:19:5011)\r\n  - using `mrparker/keycloak v3.7.0`\r\n- [during the end-to-end integration test](https://github.com/Quansight/qhub-integration-test/runs/5856676069?check_suite_focus=true#step:4:8536)\r\n  - using `mrparker/keycloak v3.3.0`\r\n\r\n\r\nI reran the Kubernetes test again and the issue seemed to go away. ",
    "actual_labels": [
      "bug",
      "ci/cd",
      "testing"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 5,
    "title": "[BUG] - jupyterhub_admin client role not being assigned",
    "body": "### Describe the bug\n\nSo far (as of Nebari 2024.4.1), an incomplete permissions system means that any Nebari (JupyterHub) user who is trying to access another user's shared JHubApp must have the jupyterhub_admin role.\r\n\r\nOur team's workaround so that all users can access shared apps (but doesn't make them admins everywhere else in Nebari) is to instead assign the Keycloak client role jupyerhub_admin to the Analyst group.  So then, Analyst group members should be JupyterHub admins (but not admins in Keycloak, conda store, etc.)\r\n\r\nUntil 2024.4.1, this worked as expected.  A team member suspects the reason it no longer works is related to this commit: https://github.com/nebari-dev/nebari/commit/6a83adad44ac096f8723ec753035fd3a8ed39296\n\n### Expected behavior\n\nOnce the jupyterhub_admin client role is assigned to the Analyst group in Keycloak, any users who are members of Analyst should be jupyterhub_admin when they launch Nebari, which means they should be able to access other users' shared JHubApps\n\n### OS and architecture in which you are running Nebari\n\nUbuntu 22.04 amd64\n\n### How to Reproduce the problem?\n\nDeploy Nebari 2024.4.1\r\nWith user A, create a shared JHubApp.\r\nSet up User B who is a member of the 'analyst' group but not 'admin' in Keycloak\r\nUser B gets a 503 when trying to open User A's JHubApp\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration",
      "security"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 5,
    "title": "[BUG] - jupyterhub_admin client role not being assigned",
    "body": "### Describe the bug\n\nSo far (as of Nebari 2024.4.1), an incomplete permissions system means that any Nebari (JupyterHub) user who is trying to access another user's shared JHubApp must have the jupyterhub_admin role.\r\n\r\nOur team's workaround so that all users can access shared apps (but doesn't make them admins everywhere else in Nebari) is to instead assign the Keycloak client role jupyerhub_admin to the Analyst group.  So then, Analyst group members should be JupyterHub admins (but not admins in Keycloak, conda store, etc.)\r\n\r\nUntil 2024.4.1, this worked as expected.  A team member suspects the reason it no longer works is related to this commit: https://github.com/nebari-dev/nebari/commit/6a83adad44ac096f8723ec753035fd3a8ed39296\n\n### Expected behavior\n\nOnce the jupyterhub_admin client role is assigned to the Analyst group in Keycloak, any users who are members of Analyst should be jupyterhub_admin when they launch Nebari, which means they should be able to access other users' shared JHubApps\n\n### OS and architecture in which you are running Nebari\n\nUbuntu 22.04 amd64\n\n### How to Reproduce the problem?\n\nDeploy Nebari 2024.4.1\r\nWith user A, create a shared JHubApp.\r\nSet up User B who is a member of the 'analyst' group but not 'admin' in Keycloak\r\nUser B gets a 503 when trying to open User A's JHubApp\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration",
      "security"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 6,
    "title": "Add a 'how to test' section in the PR template",
    "body": "### Context\n\nCurrently, we have the [following sections in the PR template](https://github.com/nebari-dev/nebari/blob/develop/.github/PULL_REQUEST_TEMPLATE.md?plain=1#L6-L33). \r\n\r\nI propose we add a new *How to test* section where PR authors can write a set of steps to test and validate the proposed changes.\n\n### Value and/or benefit\n\nWhen reviewing a PR, sometimes it's really useful to have a guide on how to test the proposed changes and what to expect from that. This would make a reviewer's task easier and add context on what to expect from the changes.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 6,
    "title": "Add a 'how to test' section in the PR template",
    "body": "### Context\n\nCurrently, we have the [following sections in the PR template](https://github.com/nebari-dev/nebari/blob/develop/.github/PULL_REQUEST_TEMPLATE.md?plain=1#L6-L33). \r\n\r\nI propose we add a new *How to test* section where PR authors can write a set of steps to test and validate the proposed changes.\n\n### Value and/or benefit\n\nWhen reviewing a PR, sometimes it's really useful to have a guide on how to test the proposed changes and what to expect from that. This would make a reviewer's task easier and add context on what to expect from the changes.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "documentation"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 7,
    "title": "[ENH] - Only show conda environments that contain dask in Dask-Gateway options.",
    "body": "### Feature description\n\nCurrently all conda environments are shown in the Dask-Gateway options field. If you choose an environment which doesn't contain dask then creating a cluster will fail. \r\n\r\nI propose that only conda environments that contain dask should show up as options in the dask-gateway options choices.\r\n\r\nBelow, only the `filesystem/dask` is actually usable for Dask jobs. \r\n\r\n<img width=\"586\" alt=\"image\" src=\"https://user-images.githubusercontent.com/561301/169438195-06090cb9-e650-4277-b1ec-cfb7f25876ae.png\">\r\n\n\n### Value and/or benefit\n\ncombined with #1294, this is the cause of a lot of easy mistakes by the end user. eliminating unusable environments will make usage more robust and less error prone.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 7,
    "title": "[ENH] - Only show conda environments that contain dask in Dask-Gateway options.",
    "body": "### Feature description\n\nCurrently all conda environments are shown in the Dask-Gateway options field. If you choose an environment which doesn't contain dask then creating a cluster will fail. \r\n\r\nI propose that only conda environments that contain dask should show up as options in the dask-gateway options choices.\r\n\r\nBelow, only the `filesystem/dask` is actually usable for Dask jobs. \r\n\r\n<img width=\"586\" alt=\"image\" src=\"https://user-images.githubusercontent.com/561301/169438195-06090cb9-e650-4277-b1ec-cfb7f25876ae.png\">\r\n\n\n### Value and/or benefit\n\ncombined with #1294, this is the cause of a lot of easy mistakes by the end user. eliminating unusable environments will make usage more robust and less error prone.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 8,
    "title": "[BUG] - Conda store shows up in two places in menu bar",
    "body": "### Describe the bug\n\nIn 2024.3.2 we have `jupyterlab-conda-store` extension back. However, now we have two (three) menu items allowing to open conda store:\r\n- \"Services > Environment Management\" coming from nebari and opening conda-store in a new tab:\r\n  ![image](https://github.com/nebari-dev/nebari/assets/5832902/6cb1722c-05b0-4051-90ed-0df7a48c0f84)\r\n- \"Conda-Store > Conda Store Package Manager\" coming from `jupyterlab-conda-store`:\r\n   ![image](https://github.com/nebari-dev/nebari/assets/5832902/fc67933d-010a-46e0-bedf-cfb655ae7d4b)\r\n\r\n\r\nI don't think there is enough value in having the separate `Conda-Store` menu with just one item, and it might be confusing for users.\n\n### Expected behavior\n\n Should we replace action associated with `Services > Environment Management` to open conda-store within JupyterLab? Or should we have both in a single menu (in that case how should we distinguish them?)\n\n### OS and architecture in which you are running Nebari\n\n2024.3.2\n\n### How to Reproduce the problem?\n\nOpen JupyterLab\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\nThis is a follow-up to\r\n- https://github.com/nebari-dev/nebari/issues/2182\r\n- https://github.com/nebari-dev/nebari/issues/2216\r\n- https://github.com/nebari-dev/nebari/issues/2302",
    "actual_labels": [
      "bug",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 8,
    "title": "[BUG] - Conda store shows up in two places in menu bar",
    "body": "### Describe the bug\n\nIn 2024.3.2 we have `jupyterlab-conda-store` extension back. However, now we have two (three) menu items allowing to open conda store:\r\n- \"Services > Environment Management\" coming from nebari and opening conda-store in a new tab:\r\n  ![image](https://github.com/nebari-dev/nebari/assets/5832902/6cb1722c-05b0-4051-90ed-0df7a48c0f84)\r\n- \"Conda-Store > Conda Store Package Manager\" coming from `jupyterlab-conda-store`:\r\n   ![image](https://github.com/nebari-dev/nebari/assets/5832902/fc67933d-010a-46e0-bedf-cfb655ae7d4b)\r\n\r\n\r\nI don't think there is enough value in having the separate `Conda-Store` menu with just one item, and it might be confusing for users.\n\n### Expected behavior\n\n Should we replace action associated with `Services > Environment Management` to open conda-store within JupyterLab? Or should we have both in a single menu (in that case how should we distinguish them?)\n\n### OS and architecture in which you are running Nebari\n\n2024.3.2\n\n### How to Reproduce the problem?\n\nOpen JupyterLab\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\nThis is a follow-up to\r\n- https://github.com/nebari-dev/nebari/issues/2182\r\n- https://github.com/nebari-dev/nebari/issues/2216\r\n- https://github.com/nebari-dev/nebari/issues/2302",
    "actual_labels": [
      "bug",
      "ux/ui"
    ],
    "predicted_labels": "bug, configuration, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 9,
    "title": "ENH - Update node pool usage for ClearML",
    "body": "This was first spotted by @leej3 in #984, where he identified that some of ClearML inner resources didn't have a specific node selector key assigned to them, leading to some of those having weird behaviors when scaling for node usage, e.g. they could be deployed at wherever node available in the referred namespace disregarding the correct resource tags (in this case ClearML).\r\n\r\n- There was a PR to fix the mentioned issue, but due to the work made on #1003, we weren't able to apply those changes.\r\n\r\n- I am opening this issue to create a report of that PR and track any new changes made after `v0.4.0+` to address this issue as part of the #1217 work.",
    "actual_labels": [
      "enhancement",
      "configuration",
      "performance"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 9,
    "title": "ENH - Update node pool usage for ClearML",
    "body": "This was first spotted by @leej3 in #984, where he identified that some of ClearML inner resources didn't have a specific node selector key assigned to them, leading to some of those having weird behaviors when scaling for node usage, e.g. they could be deployed at wherever node available in the referred namespace disregarding the correct resource tags (in this case ClearML).\r\n\r\n- There was a PR to fix the mentioned issue, but due to the work made on #1003, we weren't able to apply those changes.\r\n\r\n- I am opening this issue to create a report of that PR and track any new changes made after `v0.4.0+` to address this issue as part of the #1217 work.",
    "actual_labels": [
      "enhancement",
      "configuration",
      "performance"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 10,
    "title": "Protections on PVs to prevent data loss",
    "body": "### Context\n\nWe recently had the situation that a [storage PR](https://github.com/nebari-dev/nebari/pull/2541) caused users upgrading an existing deployment to the `develop` branch to lose the data associated with jupyterhub users.  This issue was subsequently resolved by https://github.com/nebari-dev/nebari/pull/2639 and https://github.com/nebari-dev/nebari/pull/2673 prior to the next release of Nebari.  \n\n### Value and/or benefit\n\nAs usual, [we recommend a backup](https://www.nebari.dev/docs/how-tos/nebari-upgrade#backup-existing-data) be done prior to every upgrade.  However, given the critical nature of the info on those drives, there is also great benefit in preventing the user home directory and shared storage and conda envs PVs from being deleted.  More investigation is needed, but some ideas on how best to accomplish this are as follows:\r\n- Use a similar approach to what is done https://github.com/nebari-dev/nebari/pull/2660 to look at the terraform plan prior to deployment on a particular terraform stage.  If the plan includes destruction of a critical PVC, then raise an error.  This would only be applicable for terraform stages (as opposed to other types of Nebari stages), but so far the plan is to keep provisioning infrastructure with terraform so this seems like it would be fairly robust once set up.\r\n- Set prevent deploy on the critical PVCs in terraform ideally by adding a new class in https://github.com/nebari-dev/nebari/blob/develop/src/_nebari/stages/tf_objects.py.  Not sure this would work, but seems like a promising solution if we can find a way to patch resources defined in the .tf files.  We'd then remove prevent deploy when nebari destroy is run.\r\n- Change the PVs [reclaim policy](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming) to retain.  When we go to run `nebari destroy ...` the policy may need to be changed, but only then.  I'm not sure this would cover every scenario with terraform so more testing would be needed and/or potentially this and other measures would need to be taken.\r\n- Other options yet to be suggested.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 10,
    "title": "Protections on PVs to prevent data loss",
    "body": "### Context\n\nWe recently had the situation that a [storage PR](https://github.com/nebari-dev/nebari/pull/2541) caused users upgrading an existing deployment to the `develop` branch to lose the data associated with jupyterhub users.  This issue was subsequently resolved by https://github.com/nebari-dev/nebari/pull/2639 and https://github.com/nebari-dev/nebari/pull/2673 prior to the next release of Nebari.  \n\n### Value and/or benefit\n\nAs usual, [we recommend a backup](https://www.nebari.dev/docs/how-tos/nebari-upgrade#backup-existing-data) be done prior to every upgrade.  However, given the critical nature of the info on those drives, there is also great benefit in preventing the user home directory and shared storage and conda envs PVs from being deleted.  More investigation is needed, but some ideas on how best to accomplish this are as follows:\r\n- Use a similar approach to what is done https://github.com/nebari-dev/nebari/pull/2660 to look at the terraform plan prior to deployment on a particular terraform stage.  If the plan includes destruction of a critical PVC, then raise an error.  This would only be applicable for terraform stages (as opposed to other types of Nebari stages), but so far the plan is to keep provisioning infrastructure with terraform so this seems like it would be fairly robust once set up.\r\n- Set prevent deploy on the critical PVCs in terraform ideally by adding a new class in https://github.com/nebari-dev/nebari/blob/develop/src/_nebari/stages/tf_objects.py.  Not sure this would work, but seems like a promising solution if we can find a way to patch resources defined in the .tf files.  We'd then remove prevent deploy when nebari destroy is run.\r\n- Change the PVs [reclaim policy](https://kubernetes.io/docs/concepts/storage/persistent-volumes/#reclaiming) to retain.  When we go to run `nebari destroy ...` the policy may need to be changed, but only then.  I'm not sure this would cover every scenario with terraform so more testing would be needed and/or potentially this and other measures would need to be taken.\r\n- Other options yet to be suggested.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, aws, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 11,
    "title": "[ENH] - Relax Ingress Check When Using DNS Challenge",
    "body": "### Feature description\n\n- Relax the ingress check when `acme_challenge_type=dns` is used.  \n- Optionally, provide a warning instead of a hard failure if the ingress DN is not resolvable.\n\nThis is relevant after #2957 is merged.\n\n### Value and/or benefit\n\nCurrently, Nebari enforces an ingress check that requires a valid DNS resolution (to ingress IP) before proceeding with deploying anything after the `kubernetes_ingress` stage\n\nhttps://github.com/nebari-dev/nebari/blob/3c228407c084d26bab79c58718cb39c34dcd8f36/src/_nebari/stages/kubernetes_ingress/__init__.py#L64\n\nHowever, when using the **DNS Challenge** for Let's Encrypt, this check is not required since the certificate issuance does not rely on the ingress IP pointing to DNS record, but rather on just the DNS records.\n\nThis can be overly restrictive because when deploying Nebari in a private network, the DNS may not always point to the ingress IP address, like say there is a custom proxy in between:\n\n        +-------------+\n        |  DNS Record |\n        +-------------+\n               |\n               v\n        +---------------+\n        |  Custom Proxy |\n        +---------------+\n               |\n               v\n        +-------------+\n        |  Ingress IP |\n        +-------------+\n               |\n               v\n    +-------------------+\n    |  Private Network  |\n    +-------------------+\n\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 11,
    "title": "[ENH] - Relax Ingress Check When Using DNS Challenge",
    "body": "### Feature description\n\n- Relax the ingress check when `acme_challenge_type=dns` is used.  \n- Optionally, provide a warning instead of a hard failure if the ingress DN is not resolvable.\n\nThis is relevant after #2957 is merged.\n\n### Value and/or benefit\n\nCurrently, Nebari enforces an ingress check that requires a valid DNS resolution (to ingress IP) before proceeding with deploying anything after the `kubernetes_ingress` stage\n\nhttps://github.com/nebari-dev/nebari/blob/3c228407c084d26bab79c58718cb39c34dcd8f36/src/_nebari/stages/kubernetes_ingress/__init__.py#L64\n\nHowever, when using the **DNS Challenge** for Let's Encrypt, this check is not required since the certificate issuance does not rely on the ingress IP pointing to DNS record, but rather on just the DNS records.\n\nThis can be overly restrictive because when deploying Nebari in a private network, the DNS may not always point to the ingress IP address, like say there is a custom proxy in between:\n\n        +-------------+\n        |  DNS Record |\n        +-------------+\n               |\n               v\n        +---------------+\n        |  Custom Proxy |\n        +---------------+\n               |\n               v\n        +-------------+\n        |  Ingress IP |\n        +-------------+\n               |\n               v\n    +-------------------+\n    |  Private Network  |\n    +-------------------+\n\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 12,
    "title": "Openstack Provider for QHub",
    "body": "Opening this issue to track progress with Openstack support. Right now this would be in the research stage just trying to find out how feasible this is.\r\n\r\nThere is an [openstack provider](https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs) along with some terraform modules:\r\n - https://github.com/rancher/terraform-provider-rke\r\n - https://github.com/remche/terraform-openstack-rke\r\n\r\nSo this looks feasible. Once we have a kubernetes engine provisioned it should be easy to connect to the rest of Kubernetes. The hardest part on our side is how to test this easily since at Quansight we don't have an easy way to spin up openstack clusters. Advice here would be welcome @ChrisLayton.",
    "actual_labels": [
      "enhancement",
      "configuration",
      "testing"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, cost,",
    "model_name": "base"
  },
  {
    "index": 12,
    "title": "Openstack Provider for QHub",
    "body": "Opening this issue to track progress with Openstack support. Right now this would be in the research stage just trying to find out how feasible this is.\r\n\r\nThere is an [openstack provider](https://registry.terraform.io/providers/terraform-provider-openstack/openstack/latest/docs) along with some terraform modules:\r\n - https://github.com/rancher/terraform-provider-rke\r\n - https://github.com/remche/terraform-openstack-rke\r\n\r\nSo this looks feasible. Once we have a kubernetes engine provisioned it should be easy to connect to the rest of Kubernetes. The hardest part on our side is how to test this easily since at Quansight we don't have an easy way to spin up openstack clusters. Advice here would be welcome @ChrisLayton.",
    "actual_labels": [
      "enhancement",
      "configuration",
      "testing"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 13,
    "title": "GCP architecture diagram",
    "body": "@iameskild  is working on this\r\n\r\nShould be added to https://lucid.app/documents#/documents?folder_id=278399173",
    "actual_labels": [
      "documentation",
      "gcp"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 13,
    "title": "GCP architecture diagram",
    "body": "@iameskild  is working on this\r\n\r\nShould be added to https://lucid.app/documents#/documents?folder_id=278399173",
    "actual_labels": [
      "documentation",
      "gcp"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 14,
    "title": "Terraform state file should depend on environment",
    "body": "This issue blocks environment creation https://github.com/hashicorp/terraform/issues/13022 looking for workarounds.\r\n\r\nhttps://github.com/Quansight/qhub/blob/master/qhub/template/%7B%7B%20cookiecutter.repo_directory%20%7D%7D/infrastructure/state.tf\r\n\r\nDoes not allow environment variables",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 14,
    "title": "Terraform state file should depend on environment",
    "body": "This issue blocks environment creation https://github.com/hashicorp/terraform/issues/13022 looking for workarounds.\r\n\r\nhttps://github.com/Quansight/qhub/blob/master/qhub/template/%7B%7B%20cookiecutter.repo_directory%20%7D%7D/infrastructure/state.tf\r\n\r\nDoes not allow environment variables",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 15,
    "title": "Fix docker image build workflows",
    "body": "## Describe the bug\r\n\r\nThe JupyterLab docker image builds have been consistently failing for the past week or two. \r\n\r\nhttps://github.com/Quansight/qhub/runs/4845380327?check_suite_focus=true#step:8:3189",
    "actual_labels": [
      "bug",
      "ci/cd"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 15,
    "title": "Fix docker image build workflows",
    "body": "## Describe the bug\r\n\r\nThe JupyterLab docker image builds have been consistently failing for the past week or two. \r\n\r\nhttps://github.com/Quansight/qhub/runs/4845380327?check_suite_focus=true#step:8:3189",
    "actual_labels": [
      "bug",
      "ci/cd"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 16,
    "title": "Compare Various Workflow Automation Solutions for potential integration into Nebari",
    "body": "### Feature description\n\nCurrently long running computations require a browser window to be kept open for the duration of the computation. Some prototype work has been done to enable \u2018background\u2019 and \u2018batch\u2019 processes to be run on QHub. \n\n### Value and/or benefit\n\nThis feature would enhance this and make it easily accessible to scientists and engineers using the platform.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 16,
    "title": "Compare Various Workflow Automation Solutions for potential integration into Nebari",
    "body": "### Feature description\n\nCurrently long running computations require a browser window to be kept open for the duration of the computation. Some prototype work has been done to enable \u2018background\u2019 and \u2018batch\u2019 processes to be run on QHub. \n\n### Value and/or benefit\n\nThis feature would enhance this and make it easily accessible to scientists and engineers using the platform.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 17,
    "title": "[enhancement] - add DEvSecOps to QHub CI",
    "body": "## Description\r\n\r\nWe want to ensure that the QHub deployments are secure and that we can identify any potential vulnerabilities ASAP.\r\n\r\n\r\n## Suggestion\r\n- Use a tool like [kubescape](https://github.com/armosec/kubescape) within the CI \r\n- Add documentation for folks deploying their own QHub instances on how to use / add kubescape\r\n\r\n## Context\r\nDevSecOps benefits everyone - from the team developing QHub to the end-users and folks deploying QHub for their own use\r\n\r\n",
    "actual_labels": [
      "enhancement",
      "documentation",
      "ci/cd",
      "security"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, cost,",
    "model_name": "base"
  },
  {
    "index": 17,
    "title": "[enhancement] - add DEvSecOps to QHub CI",
    "body": "## Description\r\n\r\nWe want to ensure that the QHub deployments are secure and that we can identify any potential vulnerabilities ASAP.\r\n\r\n\r\n## Suggestion\r\n- Use a tool like [kubescape](https://github.com/armosec/kubescape) within the CI \r\n- Add documentation for folks deploying their own QHub instances on how to use / add kubescape\r\n\r\n## Context\r\nDevSecOps benefits everyone - from the team developing QHub to the end-users and folks deploying QHub for their own use\r\n\r\n",
    "actual_labels": [
      "enhancement",
      "documentation",
      "ci/cd",
      "security"
    ],
    "predicted_labels": "enhancement, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 18,
    "title": "[UX] - SSHing into a server without a server spun starts a server... but which?",
    "body": "### Describe the bug\r\n\r\nIf you don't have a server on started and you ssh into the domain (I followed [these instructions](https://github.com/nebari-dev/nebari-demo/blob/main/01_interface.ipynb)), when doing that, it starts a server.\r\n\r\nNow, I don't know which server it chooses for me out of the 4 configurations that I have available.\r\n\r\n### Expected behavior\r\n\r\nIt'd be better to error out with a nice error message, and ask the user to start the server via the browser. The user is going to need to have the browser open to close the server once they finish work anyway...",
    "actual_labels": [
      "bug",
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 18,
    "title": "[UX] - SSHing into a server without a server spun starts a server... but which?",
    "body": "### Describe the bug\r\n\r\nIf you don't have a server on started and you ssh into the domain (I followed [these instructions](https://github.com/nebari-dev/nebari-demo/blob/main/01_interface.ipynb)), when doing that, it starts a server.\r\n\r\nNow, I don't know which server it chooses for me out of the 4 configurations that I have available.\r\n\r\n### Expected behavior\r\n\r\nIt'd be better to error out with a nice error message, and ask the user to start the server via the browser. The user is going to need to have the browser open to close the server once they finish work anyway...",
    "actual_labels": [
      "bug",
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 19,
    "title": "[bug] Error when redeploying qhub GCP",
    "body": "## Describe the bug\r\n\r\nWhen testing qhub on GCP, after enabling DNS auto-provision and succeded deploying qhub, an error occurs when attempting to deploy again with superfluous changes to the qhub-config.\r\n\r\n## Expected behaviour\r\n\r\nDeployment succeeds without any error.\r\n\r\n## Actual behaviour\r\n\r\nIt seemed as the state of deployment is not updated since Qhub seems to try to redeploy and recreate the resources as if they do not exist. The following errors are presented once a redeploy is attempted.\r\n\r\n```\r\n[terraform]: \u2502 Error: googleapi: Error 409: Already exists: projects/.../locations/us-central1/clusters/projectname., alreadyExists\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.kubernetes.google_container_cluster.main,\r\n[terraform]: \u2502   on modules/gcp/kubernetes/main.tf line 4, in resource \"google_container_cluster\" \"main\":\r\n[terraform]: \u2502    4: resource \"google_container_cluster\" \"main\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Error creating service account: googleapi: Error 409: Service account projectname-dev already exists within project projects/....\r\n[terraform]: \u2502 Details:\r\n[terraform]: \u2502 [\r\n[terraform]: \u2502   {\r\n[terraform]: \u2502     \"@type\": \"type.googleapis.com/google.rpc.ResourceInfo\",\r\n[terraform]: \u2502     \"resourceName\": \"projects/.../serviceAccounts/projectname-dev@q....iam.gserviceaccount.com\"\r\n[terraform]: \u2502   }\r\n[terraform]: \u2502 ]\r\n[terraform]: \u2502 , alreadyExists\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.kubernetes.google_service_account.main,\r\n[terraform]: \u2502   on modules/gcp/kubernetes/service_account.tf line 1, in resource \"google_service_account\" \"main\":\r\n[terraform]: \u2502    1: resource \"google_service_account\" \"main\" {\r\n[terraform]: \u2502 \r\n```\r\n## How to reproduce\r\n\r\nFresh instant qhub from `main`, initialize a GCP deployment, then deploy qhub using the DNS Cloudflare auto-provision setting.\r\nOnce the deployment finishes, make a superfluous change to the qhub-config and attempt a second deployment\r\n",
    "actual_labels": [
      "bug",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "```\n[terraform]: \u2502 Error: googleapi: Error 409: Already exists: projects/.../locations/us-central1/clusters/projectname., alreadyExists\n[terraform]: \u2502 \n[terraform]: \u2502   with",
    "model_name": "base"
  },
  {
    "index": 19,
    "title": "[bug] Error when redeploying qhub GCP",
    "body": "## Describe the bug\r\n\r\nWhen testing qhub on GCP, after enabling DNS auto-provision and succeded deploying qhub, an error occurs when attempting to deploy again with superfluous changes to the qhub-config.\r\n\r\n## Expected behaviour\r\n\r\nDeployment succeeds without any error.\r\n\r\n## Actual behaviour\r\n\r\nIt seemed as the state of deployment is not updated since Qhub seems to try to redeploy and recreate the resources as if they do not exist. The following errors are presented once a redeploy is attempted.\r\n\r\n```\r\n[terraform]: \u2502 Error: googleapi: Error 409: Already exists: projects/.../locations/us-central1/clusters/projectname., alreadyExists\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.kubernetes.google_container_cluster.main,\r\n[terraform]: \u2502   on modules/gcp/kubernetes/main.tf line 4, in resource \"google_container_cluster\" \"main\":\r\n[terraform]: \u2502    4: resource \"google_container_cluster\" \"main\" {\r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Error creating service account: googleapi: Error 409: Service account projectname-dev already exists within project projects/....\r\n[terraform]: \u2502 Details:\r\n[terraform]: \u2502 [\r\n[terraform]: \u2502   {\r\n[terraform]: \u2502     \"@type\": \"type.googleapis.com/google.rpc.ResourceInfo\",\r\n[terraform]: \u2502     \"resourceName\": \"projects/.../serviceAccounts/projectname-dev@q....iam.gserviceaccount.com\"\r\n[terraform]: \u2502   }\r\n[terraform]: \u2502 ]\r\n[terraform]: \u2502 , alreadyExists\r\n[terraform]: \u2502 \r\n[terraform]: \u2502   with module.kubernetes.google_service_account.main,\r\n[terraform]: \u2502   on modules/gcp/kubernetes/service_account.tf line 1, in resource \"google_service_account\" \"main\":\r\n[terraform]: \u2502    1: resource \"google_service_account\" \"main\" {\r\n[terraform]: \u2502 \r\n```\r\n## How to reproduce\r\n\r\nFresh instant qhub from `main`, initialize a GCP deployment, then deploy qhub using the DNS Cloudflare auto-provision setting.\r\nOnce the deployment finishes, make a superfluous change to the qhub-config and attempt a second deployment\r\n",
    "actual_labels": [
      "bug",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 20,
    "title": "Make jupyterlab default environment part of managed nebari-config.yaml environments",
    "body": "## Summary\r\n\r\nCurrently the environment that jupyterlab launches with is built into the docker container that qhub launches with. This becomes an issue when users would like to modify the jupyterlab environment e.g. [like PR #730](https://github.com/Quansight/qhub/pull/730). This would also reduce the size of the docker image speeding up the jupyterlab image launching.\r\n\r\n## Proposed implementation\r\n\r\nThe `default` jupyterlab environment will now be specified in [DEFAULT_ENVIRONMENTS](https://github.com/Quansight/qhub/blob/1fe386d412ca154b933696b8e43a6e30e16cca01/qhub/initialize.py#L209) and the environment.yaml will be removed from the jupyterlab qhub image. There additionally needs to be a validation check to ensure that the default environment exists in the default environments. \r\n\r\n## Acceptance Criteria\r\n\r\n - [ ] jupyterlab profile should launch normally\r\n\r\n## Tasks to complete\r\n\r\n## Related to\r\n\r\nCloses https://github.com/Quansight/qhub/issues/715 ",
    "actual_labels": [
      "enhancement",
      "performance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 20,
    "title": "Make jupyterlab default environment part of managed nebari-config.yaml environments",
    "body": "## Summary\r\n\r\nCurrently the environment that jupyterlab launches with is built into the docker container that qhub launches with. This becomes an issue when users would like to modify the jupyterlab environment e.g. [like PR #730](https://github.com/Quansight/qhub/pull/730). This would also reduce the size of the docker image speeding up the jupyterlab image launching.\r\n\r\n## Proposed implementation\r\n\r\nThe `default` jupyterlab environment will now be specified in [DEFAULT_ENVIRONMENTS](https://github.com/Quansight/qhub/blob/1fe386d412ca154b933696b8e43a6e30e16cca01/qhub/initialize.py#L209) and the environment.yaml will be removed from the jupyterlab qhub image. There additionally needs to be a validation check to ensure that the default environment exists in the default environments. \r\n\r\n## Acceptance Criteria\r\n\r\n - [ ] jupyterlab profile should launch normally\r\n\r\n## Tasks to complete\r\n\r\n## Related to\r\n\r\nCloses https://github.com/Quansight/qhub/issues/715 ",
    "actual_labels": [
      "enhancement",
      "performance",
      "configuration"
    ],
    "predicted_labels": "enhancement, maintenance, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 21,
    "title": "[BUG] - Pip Local Editable Packages not Manifested in JupyterLab",
    "body": "### Describe the bug\n\nI followed the steps outlined in the documentation to install local editable packages via pip in  a Nebari conda environment. The package works as expected when used in the terminal. However, they do not seem to be accessible from the JupyterLab interface after using the approach specified in the documentation's section \"Usage from Jupyter\".\nAnd I get module not found error.\n\nHere is the link to the specific documentation: https://www.nebari.dev/docs/how-tos/develop-local-packages/\n\n### Expected behavior\n\nThe installed packages should be available for use in JupyterLab notebooks\n\n### OS and architecture in which you are running Nebari\n\nOS: macOS Ventura 13.5.2, and Architecture: ARM64 (Apple Silicon)\n\n### How to Reproduce the problem?\n\nI created a new user of the role group Analyst, and created a non-shared environment in conda store.\nOpen a terminal in JupyterLab or VS Code\nActivate the environment: conda activate <the non-shared environment>\nCreate a virtual environment (venv) in a folder of your choice: python -m venv .venv_myenv\nActivate the venv: source .venv_myenv/bin/activate\nInstall your package via pip: pip install -e <the local editable package folder>\n\nAnd open a notebook using the kernel of <the non-shared environment>, append the package installation folder into system path as instructed in the step of the documentation\n```python\nimport sys\nsys.path.append('/home/<email of the user>/.venv_myenv/lib/<python version the environment uses>/site-packages/')\n```\n\n### Command output\n\n```bash session\nI'm using minipy cloned from their github page to test pip install -e in Nebari environment\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[19], line 1\n----> 1 import minipy\n\nModuleNotFoundError: No module named 'minipy'\n```\n\n### Versions and dependencies used.\n\n- Conda 24.5.0\n- Kubernetes\n   - Client Version: v1.30.5\n   - Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\n   - Server Version: v1.29.2\n- Nebari 2024.12.1\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "documentation",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\n```",
    "model_name": "base"
  },
  {
    "index": 21,
    "title": "[BUG] - Pip Local Editable Packages not Manifested in JupyterLab",
    "body": "### Describe the bug\n\nI followed the steps outlined in the documentation to install local editable packages via pip in  a Nebari conda environment. The package works as expected when used in the terminal. However, they do not seem to be accessible from the JupyterLab interface after using the approach specified in the documentation's section \"Usage from Jupyter\".\nAnd I get module not found error.\n\nHere is the link to the specific documentation: https://www.nebari.dev/docs/how-tos/develop-local-packages/\n\n### Expected behavior\n\nThe installed packages should be available for use in JupyterLab notebooks\n\n### OS and architecture in which you are running Nebari\n\nOS: macOS Ventura 13.5.2, and Architecture: ARM64 (Apple Silicon)\n\n### How to Reproduce the problem?\n\nI created a new user of the role group Analyst, and created a non-shared environment in conda store.\nOpen a terminal in JupyterLab or VS Code\nActivate the environment: conda activate <the non-shared environment>\nCreate a virtual environment (venv) in a folder of your choice: python -m venv .venv_myenv\nActivate the venv: source .venv_myenv/bin/activate\nInstall your package via pip: pip install -e <the local editable package folder>\n\nAnd open a notebook using the kernel of <the non-shared environment>, append the package installation folder into system path as instructed in the step of the documentation\n```python\nimport sys\nsys.path.append('/home/<email of the user>/.venv_myenv/lib/<python version the environment uses>/site-packages/')\n```\n\n### Command output\n\n```bash session\nI'm using minipy cloned from their github page to test pip install -e in Nebari environment\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\nCell In[19], line 1\n----> 1 import minipy\n\nModuleNotFoundError: No module named 'minipy'\n```\n\n### Versions and dependencies used.\n\n- Conda 24.5.0\n- Kubernetes\n   - Client Version: v1.30.5\n   - Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\n   - Server Version: v1.29.2\n- Nebari 2024.12.1\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "documentation",
      "configuration"
    ],
    "predicted_labels": "bug, configuration, dependency-update",
    "model_name": "fine_tuned"
  },
  {
    "index": 22,
    "title": "Determine how to allow arbitrary customization to files",
    "body": "",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 22,
    "title": "Determine how to allow arbitrary customization to files",
    "body": "",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 23,
    "title": "GitHub Actions CI not running on main",
    "body": "It seems that the CI is not running on the main branch.",
    "actual_labels": [
      "bug",
      "ci/cd",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 23,
    "title": "GitHub Actions CI not running on main",
    "body": "It seems that the CI is not running on the main branch.",
    "actual_labels": [
      "bug",
      "ci/cd",
      "configuration"
    ],
    "predicted_labels": "bug, ci/cd, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 24,
    "title": "Explore upgrading Keycloak to version 21.x",
    "body": "We are currently running on keycloak version 15.0.2 and this might be an issue for local Nebari deployments on M1 Macs, likely due to the change in architecture and the lack of keycloak support for this new architecture at that time. \r\n\r\nAs someone running developing on an M1 Mac, I have performed simple tests with both versions (15.0.2 and 21.1.1) and only the later version worked for me. \r\n\r\nAlso see #1405 ",
    "actual_labels": [
      "bug",
      "maintenance",
      "dependency-update"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, keycloak, version 21.x",
    "model_name": "base"
  },
  {
    "index": 24,
    "title": "Explore upgrading Keycloak to version 21.x",
    "body": "We are currently running on keycloak version 15.0.2 and this might be an issue for local Nebari deployments on M1 Macs, likely due to the change in architecture and the lack of keycloak support for this new architecture at that time. \r\n\r\nAs someone running developing on an M1 Mac, I have performed simple tests with both versions (15.0.2 and 21.1.1) and only the later version worked for me. \r\n\r\nAlso see #1405 ",
    "actual_labels": [
      "bug",
      "maintenance",
      "dependency-update"
    ],
    "predicted_labels": "enhancement, maintenance, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 25,
    "title": "[MAINT] - Integrate latest conda-store versions",
    "body": "### Feature description\n\nOnce we have new conda-store releases, we need to update their versions on Nebari, test it, and include it in an upcomming Nebari release.\n\n### Value and/or benefit\n\nUsers get newest conda-store features.\n\n### Anything else?\n\nNote: This is currently blocked by conda-store releases. This issue is to keep track of the work.",
    "actual_labels": [
      "maintenance",
      "dependency-update",
      "release"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 25,
    "title": "[MAINT] - Integrate latest conda-store versions",
    "body": "### Feature description\n\nOnce we have new conda-store releases, we need to update their versions on Nebari, test it, and include it in an upcomming Nebari release.\n\n### Value and/or benefit\n\nUsers get newest conda-store features.\n\n### Anything else?\n\nNote: This is currently blocked by conda-store releases. This issue is to keep track of the work.",
    "actual_labels": [
      "maintenance",
      "dependency-update",
      "release"
    ],
    "predicted_labels": "maintenance, ci/cd, testing, release",
    "model_name": "fine_tuned"
  },
  {
    "index": 26,
    "title": "Improve CDS Dashboard integration [enhancement]",
    "body": "Initial CDS Dashboard integration is done by https://github.com/Quansight/qhub-cloud/pull/386.  This issue documents the limitations of the current integration.\n\n1. Can't specify which conda environment you want to run the dashboard (always uses default, I believe, but it might be base)\n2. User sees \"Page not found\" when dashboard is being created.  After a few seconds, refreshing the dashboard url will load the dashboard (or sometimes a blank page, but that is documented in the next line).  \n3. Refreshing the dashboard link quickly will mostly load a blank page, but sometimes will load the actual app, and it's not clear why that is.\n4. Need to inherit Qhub theme.\n\nI talked with @danlester this morning about these issues, and he is going to work on issues 2 & 3 above.",
    "actual_labels": [
      "bug",
      "enhancement",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 26,
    "title": "Improve CDS Dashboard integration [enhancement]",
    "body": "Initial CDS Dashboard integration is done by https://github.com/Quansight/qhub-cloud/pull/386.  This issue documents the limitations of the current integration.\n\n1. Can't specify which conda environment you want to run the dashboard (always uses default, I believe, but it might be base)\n2. User sees \"Page not found\" when dashboard is being created.  After a few seconds, refreshing the dashboard url will load the dashboard (or sometimes a blank page, but that is documented in the next line).  \n3. Refreshing the dashboard link quickly will mostly load a blank page, but sometimes will load the actual app, and it's not clear why that is.\n4. Need to inherit Qhub theme.\n\nI talked with @danlester this morning about these issues, and he is going to work on issues 2 & 3 above.",
    "actual_labels": [
      "bug",
      "enhancement",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 27,
    "title": "[BUG] - Validation error while reading generated config file for Digital Ocean",
    "body": "### Describe the bug\n\nThe generated config file for Digital Ocean causes a model validation error.\r\n\r\n```bash\r\nnebari init do --project projectname \\\r\n      --domain domain \\\r\n      --auth-provider password\r\n```\r\n\r\nThe generated config file looks like this:\r\n```yaml\r\nprovider: do\r\nnamespace: dev\r\nnebari_version: 2023.7.2.dev28+gf4ab2ac5\r\nproject_name: projectname\r\ndomain: domain\r\nci_cd:\r\n  type: none\r\nterraform_state:\r\n  type: remote\r\nsecurity:\r\n  keycloak:\r\n    initial_root_password: yelnf3345fiu51srhu17ytb3a8bh6bx7\r\n  authentication:\r\n    type: password\r\ntheme:\r\n  jupyterhub:\r\n    hub_title: Nebari - projectname\r\n    welcome: Welcome! Learn about Nebari's features and configurations in <a href=\"https://www.nebari.dev/docs\">the\r\n      documentation</a>. If you have any questions or feedback, reach the team on\r\n      <a href=\"https://www.nebari.dev/docs/community#getting-support\">Nebari's support\r\n      forums</a>.\r\n    hub_subtitle: Your open source data science platform, hosted on Digital Ocean\r\n\r\n```\n\n### Expected behavior\n\nThere should be no validation error.\n\n### OS and architecture in which you are running Nebari\n\nm1 mac\n\n### How to Reproduce the problem?\n\n```python\r\nfrom ruamel.yaml import YAML\r\n\r\nfrom nebari.plugins import nebari_plugin_manager\r\n\r\nyaml = YAML()\r\n\r\nconfig_schema = nebari_plugin_manager.config_schema\r\n\r\n\r\nwith open(\"nebari-config.yaml\") as fp:\r\n    data = yaml.load(fp)\r\n    config = config_schema(**data)\r\n```\n\n### Command output\n\n```bash session\n(nebari-dev) fangchenli@Fangchens-MacBook-Pro-2 nebari-do % python test.py\r\nTraceback (most recent call last):\r\n  File \"/Users/fangchenli/Workspace/nebari-do/test.py\", line 23, in <module>\r\n    config = config_schema(**data)\r\n  File \"pydantic/main.py\", line 342, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError: 1 validation error for ConfigSchema\r\n__root__ -> __root__\r\n  'latin-1' codec can't encode character '\\u201c' in position 7: ordinal not in range(256) (type=value_error.unicodeencode)\n```\n\n\n### Versions and dependencies used.\n\n```\r\nconda 23.7.3\r\n\r\nClient Version: v1.28.1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\n\r\n2023.7.2.dev28+gf4ab2ac5\r\n```\n\n### Compute environment\n\nDigital Ocean\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n\nAllowed labels with descriptions",
    "model_name": "base"
  },
  {
    "index": 27,
    "title": "[BUG] - Validation error while reading generated config file for Digital Ocean",
    "body": "### Describe the bug\n\nThe generated config file for Digital Ocean causes a model validation error.\r\n\r\n```bash\r\nnebari init do --project projectname \\\r\n      --domain domain \\\r\n      --auth-provider password\r\n```\r\n\r\nThe generated config file looks like this:\r\n```yaml\r\nprovider: do\r\nnamespace: dev\r\nnebari_version: 2023.7.2.dev28+gf4ab2ac5\r\nproject_name: projectname\r\ndomain: domain\r\nci_cd:\r\n  type: none\r\nterraform_state:\r\n  type: remote\r\nsecurity:\r\n  keycloak:\r\n    initial_root_password: yelnf3345fiu51srhu17ytb3a8bh6bx7\r\n  authentication:\r\n    type: password\r\ntheme:\r\n  jupyterhub:\r\n    hub_title: Nebari - projectname\r\n    welcome: Welcome! Learn about Nebari's features and configurations in <a href=\"https://www.nebari.dev/docs\">the\r\n      documentation</a>. If you have any questions or feedback, reach the team on\r\n      <a href=\"https://www.nebari.dev/docs/community#getting-support\">Nebari's support\r\n      forums</a>.\r\n    hub_subtitle: Your open source data science platform, hosted on Digital Ocean\r\n\r\n```\n\n### Expected behavior\n\nThere should be no validation error.\n\n### OS and architecture in which you are running Nebari\n\nm1 mac\n\n### How to Reproduce the problem?\n\n```python\r\nfrom ruamel.yaml import YAML\r\n\r\nfrom nebari.plugins import nebari_plugin_manager\r\n\r\nyaml = YAML()\r\n\r\nconfig_schema = nebari_plugin_manager.config_schema\r\n\r\n\r\nwith open(\"nebari-config.yaml\") as fp:\r\n    data = yaml.load(fp)\r\n    config = config_schema(**data)\r\n```\n\n### Command output\n\n```bash session\n(nebari-dev) fangchenli@Fangchens-MacBook-Pro-2 nebari-do % python test.py\r\nTraceback (most recent call last):\r\n  File \"/Users/fangchenli/Workspace/nebari-do/test.py\", line 23, in <module>\r\n    config = config_schema(**data)\r\n  File \"pydantic/main.py\", line 342, in pydantic.main.BaseModel.__init__\r\npydantic.error_wrappers.ValidationError: 1 validation error for ConfigSchema\r\n__root__ -> __root__\r\n  'latin-1' codec can't encode character '\\u201c' in position 7: ordinal not in range(256) (type=value_error.unicodeencode)\n```\n\n\n### Versions and dependencies used.\n\n```\r\nconda 23.7.3\r\n\r\nClient Version: v1.28.1\r\nKustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3\r\n\r\n2023.7.2.dev28+gf4ab2ac5\r\n```\n\n### Compute environment\n\nDigital Ocean\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 28,
    "title": "[ENH] - Update Nebari to the 2023.10.1 conda-store release",
    "body": "### Feature description\n\nWe should update the nebari docker images to include the latest conda-store release, 2023.10.1. \r\n\r\n\n\n### Value and/or benefit\n\nStaying up to date with conda-store!\n\n### Anything else?\n\nThis is the line where this should be updated: https://github.com/nebari-dev/nebari-docker-images/blob/a1e02f905f3f175d34c43b83430ad51710b49b76/jupyterlab/environment.yaml#L58",
    "actual_labels": [
      "enhancement",
      "dependency-update"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 28,
    "title": "[ENH] - Update Nebari to the 2023.10.1 conda-store release",
    "body": "### Feature description\n\nWe should update the nebari docker images to include the latest conda-store release, 2023.10.1. \r\n\r\n\n\n### Value and/or benefit\n\nStaying up to date with conda-store!\n\n### Anything else?\n\nThis is the line where this should be updated: https://github.com/nebari-dev/nebari-docker-images/blob/a1e02f905f3f175d34c43b83430ad51710b49b76/jupyterlab/environment.yaml#L58",
    "actual_labels": [
      "enhancement",
      "dependency-update"
    ],
    "predicted_labels": "enhancement, maintenance, ci/cd, testing, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 29,
    "title": "Create a milestone for a last release within `qhub` `v.0.4.x`",
    "body": "",
    "actual_labels": [
      "release"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 29,
    "title": "Create a milestone for a last release within `qhub` `v.0.4.x`",
    "body": "",
    "actual_labels": [
      "release"
    ],
    "predicted_labels": "release, maintenance, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 30,
    "title": "[BUG] - Azure redeployments fail if storage_account_postfix is not explicitly set ",
    "body": "### Describe the bug\n\nInitial Azure deployments are successful but the subsequent deployments fail because the Nebari schema generates a new postfix if the storage_account_postfix field is not set in the nebari-config.yaml; this was how it was prior to the extension-mechanism. \r\n\r\nThe second deployment fails because a new random postfix is generated and this causes the Terraform state to delete the storage account and the state and then the deployment grinds to a halt.\n\n### Expected behavior\n\nTo successfully redeploy Azure.\n\n### OS and architecture in which you are running Nebari\n\nMacOS\n\n### How to Reproduce the problem?\n\nDeploy Azure twice from the `@develop` branch.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nAzure\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "azure",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 30,
    "title": "[BUG] - Azure redeployments fail if storage_account_postfix is not explicitly set ",
    "body": "### Describe the bug\n\nInitial Azure deployments are successful but the subsequent deployments fail because the Nebari schema generates a new postfix if the storage_account_postfix field is not set in the nebari-config.yaml; this was how it was prior to the extension-mechanism. \r\n\r\nThe second deployment fails because a new random postfix is generated and this causes the Terraform state to delete the storage account and the state and then the deployment grinds to a halt.\n\n### Expected behavior\n\nTo successfully redeploy Azure.\n\n### OS and architecture in which you are running Nebari\n\nMacOS\n\n### How to Reproduce the problem?\n\nDeploy Azure twice from the `@develop` branch.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nAzure\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "azure",
      "configuration"
    ],
    "predicted_labels": "bug, azure, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 31,
    "title": "[BUG] - Nebari validate typo causes `nebari-linter.yaml` action to fail",
    "body": "### OS system and architecture in which you are running Nebari\n\nubuntu \n\n### Expected behavior\n\nWhen rendering the CICD files, the `nebari-linter.yaml` should run the following command:\r\n\r\n```\r\nnebari validate -c nebari-config.yaml --enable-commenting\r\n```\n\n### Actual behavior\n\nUnfortunately there is a minor typo that causes this causes to fail:\r\n\r\n```\r\nnebari validate -c nebari-config.yaml --enable_commenting\r\n```\r\n\n\n### How to Reproduce the problem?\n\nSetup CICD for your deployment, make a change to the `nebari-config.yaml` and open a PR to make this change. This will run the `nebari-linter.yaml` action but it will fail because of the typo mentioned above.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "ci/cd"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 31,
    "title": "[BUG] - Nebari validate typo causes `nebari-linter.yaml` action to fail",
    "body": "### OS system and architecture in which you are running Nebari\n\nubuntu \n\n### Expected behavior\n\nWhen rendering the CICD files, the `nebari-linter.yaml` should run the following command:\r\n\r\n```\r\nnebari validate -c nebari-config.yaml --enable-commenting\r\n```\n\n### Actual behavior\n\nUnfortunately there is a minor typo that causes this causes to fail:\r\n\r\n```\r\nnebari validate -c nebari-config.yaml --enable_commenting\r\n```\r\n\n\n### How to Reproduce the problem?\n\nSetup CICD for your deployment, make a change to the `nebari-config.yaml` and open a PR to make this change. This will run the `nebari-linter.yaml` action but it will fail because of the typo mentioned above.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "ci/cd"
    ],
    "predicted_labels": "bug, ci/cd, testing, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 32,
    "title": "[ENH] - Update azure terraform provider, API retiring in Feb, 2025",
    "body": "### Feature description\n\nWe're using =3.97.1 of the terraform azure provider [1] in Nebari. It looks like it's using the API version **2023-06-02-preview** [2], which is retiring on 24 February 2025.\n\nReference:\n- [1] https://github.com/nebari-dev/nebari/blob/43e63fe7f21c68ec6b4ecf7a117b62f8937691fe/src/_nebari/stages/terraform_state/template/azure/main.tf#L50C18-L50C25\n- [2] https://github.com/hashicorp/terraform-provider-azurerm/blob/v3.97.1/internal/services/containers/kubernetes_nodepool.go#L21\n\n\nOfficial mail from Microsoft:\n\n>Migrate to a newer AKS API to avoid service disruptions\nYou\u2019re receiving this notice because you're currently using an Azure Kubernetes Service API that is about to be retired.\n\n>The following AKS APIs will be retired soon:\n\n>2023-03-02-preview will be retired on 3 February 2025\n2023-04-02-preview will be retired on 10 February 2025\n2023-05-02-preview will be retired on 17 February 2025\n2023-06-02-preview will be retired on 24 February 2025\n2023-07-02-preview will be retired on 3 March 2025\n2023-08-02 -preview will be retired on 10 March 2025\nAfter these dates, these APIs will no longer work.\n\n### Value and/or benefit\n\nBeing able to deploy Nebari on Azure.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "maintenance",
      "azure",
      "dependency-update"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 32,
    "title": "[ENH] - Update azure terraform provider, API retiring in Feb, 2025",
    "body": "### Feature description\n\nWe're using =3.97.1 of the terraform azure provider [1] in Nebari. It looks like it's using the API version **2023-06-02-preview** [2], which is retiring on 24 February 2025.\n\nReference:\n- [1] https://github.com/nebari-dev/nebari/blob/43e63fe7f21c68ec6b4ecf7a117b62f8937691fe/src/_nebari/stages/terraform_state/template/azure/main.tf#L50C18-L50C25\n- [2] https://github.com/hashicorp/terraform-provider-azurerm/blob/v3.97.1/internal/services/containers/kubernetes_nodepool.go#L21\n\n\nOfficial mail from Microsoft:\n\n>Migrate to a newer AKS API to avoid service disruptions\nYou\u2019re receiving this notice because you're currently using an Azure Kubernetes Service API that is about to be retired.\n\n>The following AKS APIs will be retired soon:\n\n>2023-03-02-preview will be retired on 3 February 2025\n2023-04-02-preview will be retired on 10 February 2025\n2023-05-02-preview will be retired on 17 February 2025\n2023-06-02-preview will be retired on 24 February 2025\n2023-07-02-preview will be retired on 3 March 2025\n2023-08-02 -preview will be retired on 10 March 2025\nAfter these dates, these APIs will no longer work.\n\n### Value and/or benefit\n\nBeing able to deploy Nebari on Azure.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "maintenance",
      "azure",
      "dependency-update"
    ],
    "predicted_labels": "enhancement, maintenance, azure, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 33,
    "title": "QHub logic app",
    "body": "",
    "actual_labels": [
      "question",
      "azure"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 33,
    "title": "QHub logic app",
    "body": "",
    "actual_labels": [
      "question",
      "azure"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 34,
    "title": "Refactor profile code of jupyterhub.yaml",
    "body": "",
    "actual_labels": [
      "maintenance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 34,
    "title": "Refactor profile code of jupyterhub.yaml",
    "body": "",
    "actual_labels": [
      "maintenance",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 35,
    "title": "Automate oauth provisioning within qhub init for auth0",
    "body": "",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 35,
    "title": "Automate oauth provisioning within qhub init for auth0",
    "body": "",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 36,
    "title": "[BUG] - azure project name length requirements aren't exactly right",
    "body": "### Describe the bug\n\nI did a guided init with nebari 2024.3.3 for azure and this was apparently the command it corresponds to \r\n```bash\r\n$ nebari init ProviderEnum.azure --project-name xxxxxxxxx-nebari --namespace dev --auth-provider \r\nAuthenticationEnum.password --terraform-state TerraformStateEnum.remote --region southcentralus\r\n```\r\n\r\nI ended up with this error\r\n```\r\n\u2502 Error: Failed to get existing workspaces: Error retrieving keys for Storage Account \"xxxx61xxx61xxa2Dneba61ridevlnwl\": storage.AccountsClient#ListKeys: Invalid input: autorest/validation: validation failed: parameter=accountName constraint=MaxLength value=\"broa61dva61ila2Dneba61ridevlnwl\" details: value length must be less than or equal to 24\r\n```\n\n### Expected behavior\n\nNo error if the name said it was passing in the guided init\n\n### OS and architecture in which you are running Nebari\n\nLinux x86-64\n\n### How to Reproduce the problem?\n\nsee above\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "azure",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nrules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n```",
    "model_name": "base"
  },
  {
    "index": 36,
    "title": "[BUG] - azure project name length requirements aren't exactly right",
    "body": "### Describe the bug\n\nI did a guided init with nebari 2024.3.3 for azure and this was apparently the command it corresponds to \r\n```bash\r\n$ nebari init ProviderEnum.azure --project-name xxxxxxxxx-nebari --namespace dev --auth-provider \r\nAuthenticationEnum.password --terraform-state TerraformStateEnum.remote --region southcentralus\r\n```\r\n\r\nI ended up with this error\r\n```\r\n\u2502 Error: Failed to get existing workspaces: Error retrieving keys for Storage Account \"xxxx61xxx61xxa2Dneba61ridevlnwl\": storage.AccountsClient#ListKeys: Invalid input: autorest/validation: validation failed: parameter=accountName constraint=MaxLength value=\"broa61dva61ila2Dneba61ridevlnwl\" details: value length must be less than or equal to 24\r\n```\n\n### Expected behavior\n\nNo error if the name said it was passing in the guided init\n\n### OS and architecture in which you are running Nebari\n\nLinux x86-64\n\n### How to Reproduce the problem?\n\nsee above\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "azure",
      "configuration"
    ],
    "predicted_labels": "bug, azure, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 37,
    "title": "[ENH] - Allow AWS deployments to specify IAM permission boundaries ",
    "body": "### Feature description\n\nDeploying in particular environments may require that an IAM Permissions Boundary is applied to all IAM Roles created during deployment.\n\n### Value and/or benefit\n\nAllows for additional customization during the Nebari deployment.\n\n### Anything else?\n\nIt appears we need to provide the policy ARN to the role (see [Terraform AWS provider docs](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role#permissions_boundary))",
    "actual_labels": [
      "enhancement",
      "aws",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 37,
    "title": "[ENH] - Allow AWS deployments to specify IAM permission boundaries ",
    "body": "### Feature description\n\nDeploying in particular environments may require that an IAM Permissions Boundary is applied to all IAM Roles created during deployment.\n\n### Value and/or benefit\n\nAllows for additional customization during the Nebari deployment.\n\n### Anything else?\n\nIt appears we need to provide the policy ARN to the role (see [Terraform AWS provider docs](https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/iam_role#permissions_boundary))",
    "actual_labels": [
      "enhancement",
      "aws",
      "security",
      "configuration"
    ],
    "predicted_labels": "enhancement, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 38,
    "title": "[DOC] - Add docs for shared group mounting based on keycloak permissions",
    "body": "",
    "actual_labels": [
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 38,
    "title": "[DOC] - Add docs for shared group mounting based on keycloak permissions",
    "body": "",
    "actual_labels": [
      "documentation"
    ],
    "predicted_labels": "documentation, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 39,
    "title": "[BUG] - Shared conda-store environment namespaces are no longer created automatically for groups (e.g. analyst, admin, etc.)",
    "body": "### Describe the bug\n\nI see that on 2024.3.3 Nebari with 2024.1.1 conda store that namespaces for the groups are not created by default (e.g. analyst, admin, etc) anymore.  \r\n\r\nI was able to create a shared namespace from the `<domain>/admin/namespace/` page, but not from the non admin UI.  It seems like a useful feature.  If it is desired, we should consider if we want to make that behavior configurable.\n\n### Expected behavior\n\n-\n\n### OS and architecture in which you are running Nebari\n\nLinux x86-64\n\n### How to Reproduce the problem?\n\nSee above\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 39,
    "title": "[BUG] - Shared conda-store environment namespaces are no longer created automatically for groups (e.g. analyst, admin, etc.)",
    "body": "### Describe the bug\n\nI see that on 2024.3.3 Nebari with 2024.1.1 conda store that namespaces for the groups are not created by default (e.g. analyst, admin, etc) anymore.  \r\n\r\nI was able to create a shared namespace from the `<domain>/admin/namespace/` page, but not from the non admin UI.  It seems like a useful feature.  If it is desired, we should consider if we want to make that behavior configurable.\n\n### Expected behavior\n\n-\n\n### OS and architecture in which you are running Nebari\n\nLinux x86-64\n\n### How to Reproduce the problem?\n\nSee above\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 40,
    "title": "[DOC] - Deployment on AWS suggestions for improvement",
    "body": "### Preliminary Checks\n\n- [X] This issue is not a question, feature request, RFC, or anything other than a bug report. Please post those things in GitHub Discussions: https://github.com/nebari-dev/nebari/discussions\n\n### Summary\n\nThe first time we ran the guided init for AWS, we set only the two AWS credential variables as directed.\r\nWe got all the way through the questions and then received the error that GITHUB_USERNAME was not defined:\r\n ![2022-12-06_13-57-33](https://user-images.githubusercontent.com/1872600/206005203-1c9eeb84-a4ff-4381-8243-6aa55d669f01.png)\r\n\r\nWe then looked at the code where that error was coming from and it looked like we would need to specify GITHUB_TOKEN as well, using a personal access token.  We figured we might as well set the `AWS_REGION=us-east-1` for good measure.\r\n\r\nThe 2nd time we ran the guided init, it worked, though the config file generated specified `us-west-2` for some reason.  So we edited the config and changed it to `us-east-1` before proceeding to the `nebari deploy` step.    The nebari deploy then went find.\r\n\r\nNotes on running the guided init:\r\n* it was confusing to be asked for the `client_id` and `client_secret` twice during the guided init.  We were thinking we did something wrong. \r\n* it should be mentioned that the github user running the terraform deploy must have the ability to create repositories. \r\n\r\n\r\n\r\n\n\n### Steps to Resolve this Issue\n\nWe set the additional env vars as described above.\r\n\r\n@jkellndorfer \r\n",
    "actual_labels": [
      "bug",
      "documentation",
      "aws",
      "ux/ui",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 40,
    "title": "[DOC] - Deployment on AWS suggestions for improvement",
    "body": "### Preliminary Checks\n\n- [X] This issue is not a question, feature request, RFC, or anything other than a bug report. Please post those things in GitHub Discussions: https://github.com/nebari-dev/nebari/discussions\n\n### Summary\n\nThe first time we ran the guided init for AWS, we set only the two AWS credential variables as directed.\r\nWe got all the way through the questions and then received the error that GITHUB_USERNAME was not defined:\r\n ![2022-12-06_13-57-33](https://user-images.githubusercontent.com/1872600/206005203-1c9eeb84-a4ff-4381-8243-6aa55d669f01.png)\r\n\r\nWe then looked at the code where that error was coming from and it looked like we would need to specify GITHUB_TOKEN as well, using a personal access token.  We figured we might as well set the `AWS_REGION=us-east-1` for good measure.\r\n\r\nThe 2nd time we ran the guided init, it worked, though the config file generated specified `us-west-2` for some reason.  So we edited the config and changed it to `us-east-1` before proceeding to the `nebari deploy` step.    The nebari deploy then went find.\r\n\r\nNotes on running the guided init:\r\n* it was confusing to be asked for the `client_id` and `client_secret` twice during the guided init.  We were thinking we did something wrong. \r\n* it should be mentioned that the github user running the terraform deploy must have the ability to create repositories. \r\n\r\n\r\n\r\n\n\n### Steps to Resolve this Issue\n\nWe set the additional env vars as described above.\r\n\r\n@jkellndorfer \r\n",
    "actual_labels": [
      "bug",
      "documentation",
      "aws",
      "ux/ui",
      "configuration"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 41,
    "title": "Allow for arbitrary customization of terraform files",
    "body": "Currently, files need to be manually committed after running `qhub-ops` this is due to features that are not covered in the qhub configuration. We need an escape hook. This issue will require discussion on how we can achieve this.\n\n - [ ] Within quansight we will need to apply this solution to internal qhub clusters",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 41,
    "title": "Allow for arbitrary customization of terraform files",
    "body": "Currently, files need to be manually committed after running `qhub-ops` this is due to features that are not covered in the qhub configuration. We need an escape hook. This issue will require discussion on how we can achieve this.\n\n - [ ] Within quansight we will need to apply this solution to internal qhub clusters",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 42,
    "title": "[bug] Qhub fails to destroy on AWS instance",
    "body": "## Describe the bug\r\n\r\nFresh install of qhub (version `0.4.0.dev86+g9fb62c8` based on `main:9fb62c812e`), failed to destroy instance: \r\n```\r\n[terraform]: module.network.aws_subnet.main[1]: Still destroying... [id=subnet-0e67818d0783e1d0c, 20m1s elapsed]\r\n[terraform]: module.network.aws_subnet.main[0]: Still destroying... [id=subnet-057575eebaf199725, 20m1s elapsed]\r\n[terraform]: 2022-02-18T20:24:16.347-0300 [INFO]  provider.terraform-provider-aws_v3.73.0_x5: 2022/02/18 20:24:16 [WARN] WaitForState timeout after 20m0s: timestamp=2022-02-18T20:24:16.347-0300\r\n[terraform]: 2022-02-18T20:24:16.347-0300 [INFO]  provider.terraform-provider-aws_v3.73.0_x5: 2022/02/18 20:24:16 [WARN] WaitForState starting 30s refresh grace period: timestamp=2022-02-18T20:24:16.347-0300\r\n[terraform]: 2022-02-18T20:24:16.371-0300 [INFO]  provider.terraform-provider-aws_v3.73.0_x5: 2022/02/18 20:24:16 [WARN] WaitForState timeout after 20m0s: timestamp=2022-02-18T20:24:16.371-0300\r\n[terraform]: 2022-02-18T20:24:16.371-0300 [INFO]  provider.terraform-provider-aws_v3.73.0_x5: 2022/02/18 20:24:16 [WARN] WaitForState starting 30s refresh grace period: timestamp=2022-02-18T20:24:16.371-0300\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: error deleting EC2 Subnet (subnet-0e67818d0783e1d0c): DependencyViolation: The subnet 'subnet-0e67818d0783e1d0c' has dependencies and cannot be deleted.\r\n[terraform]: \u2502 \tstatus code: 400, request id: a936bad1-4605-4b70-9fc4-25c0dae06131\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: error detaching EC2 Internet Gateway (igw-00f5928d9219a02fa) from VPC (vpc-027cf7b4d3f134cc1): DependencyViolation: Network vpc-027cf7b4d3f134cc1 has some mapped public address(es). Please unmap those public address(es) before detaching the gateway.\r\n[terraform]: \u2502 \tstatus code: 400, request id: ae39e552-b365-45e6-a900-0304447bd733\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: error deleting EC2 Subnet (subnet-057575eebaf199725): DependencyViolation: The subnet 'subnet-057575eebaf199725' has dependencies and cannot be deleted.\r\n[terraform]: \u2502 \tstatus code: 400, request id: 88945487-86a3-4f29-8ebd-8ab98ed4d638\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n```\r\nI spotted this error before, and I was able to fix it by manually deleting the ELB (Elastic Load Balancer) assigned to the vpn, then deleting the vpn (as qhub has no condition to delete the resource for some weird reason).\r\n\r\n## How to reproduce\r\nSteps to recreate:\r\n- `qhub init aws --project=qhubstages  --domain awsqhubstages.qhub.dev --auth-provider=password --terraform-state=local  --ci-provider=github-actions`\r\n- `qhub deploy -c qhub-config.yaml --disable-prompt --dns-provider cloudflare --dns-auto-provision ` (do not execute the dns auto-provision bit if you need to do a redeployment)\r\n- then qhub destroy\r\n\r\n## Expected behavior\r\n- Successful execution of the destroy command, no traces of qhub resources left in the aws portal\r\n\r\n## Observations\r\n\r\nI am not sure why this is happening, maybe the ELB is created from the provider, and terraform does not have control over it? or we just need to change the order of deletion (simple thoughts here)",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n```",
    "model_name": "base"
  },
  {
    "index": 42,
    "title": "[bug] Qhub fails to destroy on AWS instance",
    "body": "## Describe the bug\r\n\r\nFresh install of qhub (version `0.4.0.dev86+g9fb62c8` based on `main:9fb62c812e`), failed to destroy instance: \r\n```\r\n[terraform]: module.network.aws_subnet.main[1]: Still destroying... [id=subnet-0e67818d0783e1d0c, 20m1s elapsed]\r\n[terraform]: module.network.aws_subnet.main[0]: Still destroying... [id=subnet-057575eebaf199725, 20m1s elapsed]\r\n[terraform]: 2022-02-18T20:24:16.347-0300 [INFO]  provider.terraform-provider-aws_v3.73.0_x5: 2022/02/18 20:24:16 [WARN] WaitForState timeout after 20m0s: timestamp=2022-02-18T20:24:16.347-0300\r\n[terraform]: 2022-02-18T20:24:16.347-0300 [INFO]  provider.terraform-provider-aws_v3.73.0_x5: 2022/02/18 20:24:16 [WARN] WaitForState starting 30s refresh grace period: timestamp=2022-02-18T20:24:16.347-0300\r\n[terraform]: 2022-02-18T20:24:16.371-0300 [INFO]  provider.terraform-provider-aws_v3.73.0_x5: 2022/02/18 20:24:16 [WARN] WaitForState timeout after 20m0s: timestamp=2022-02-18T20:24:16.371-0300\r\n[terraform]: 2022-02-18T20:24:16.371-0300 [INFO]  provider.terraform-provider-aws_v3.73.0_x5: 2022/02/18 20:24:16 [WARN] WaitForState starting 30s refresh grace period: timestamp=2022-02-18T20:24:16.371-0300\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: error deleting EC2 Subnet (subnet-0e67818d0783e1d0c): DependencyViolation: The subnet 'subnet-0e67818d0783e1d0c' has dependencies and cannot be deleted.\r\n[terraform]: \u2502 \tstatus code: 400, request id: a936bad1-4605-4b70-9fc4-25c0dae06131\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: error detaching EC2 Internet Gateway (igw-00f5928d9219a02fa) from VPC (vpc-027cf7b4d3f134cc1): DependencyViolation: Network vpc-027cf7b4d3f134cc1 has some mapped public address(es). Please unmap those public address(es) before detaching the gateway.\r\n[terraform]: \u2502 \tstatus code: 400, request id: ae39e552-b365-45e6-a900-0304447bd733\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: error deleting EC2 Subnet (subnet-057575eebaf199725): DependencyViolation: The subnet 'subnet-057575eebaf199725' has dependencies and cannot be deleted.\r\n[terraform]: \u2502 \tstatus code: 400, request id: 88945487-86a3-4f29-8ebd-8ab98ed4d638\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 \r\n[terraform]: \u2575\r\n```\r\nI spotted this error before, and I was able to fix it by manually deleting the ELB (Elastic Load Balancer) assigned to the vpn, then deleting the vpn (as qhub has no condition to delete the resource for some weird reason).\r\n\r\n## How to reproduce\r\nSteps to recreate:\r\n- `qhub init aws --project=qhubstages  --domain awsqhubstages.qhub.dev --auth-provider=password --terraform-state=local  --ci-provider=github-actions`\r\n- `qhub deploy -c qhub-config.yaml --disable-prompt --dns-provider cloudflare --dns-auto-provision ` (do not execute the dns auto-provision bit if you need to do a redeployment)\r\n- then qhub destroy\r\n\r\n## Expected behavior\r\n- Successful execution of the destroy command, no traces of qhub resources left in the aws portal\r\n\r\n## Observations\r\n\r\nI am not sure why this is happening, maybe the ELB is created from the provider, and terraform does not have control over it? or we just need to change the order of deletion (simple thoughts here)",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 43,
    "title": "Rendering vs not rendering terraform files in github",
    "body": "Pro:\nHide secrets\nCleaner repo\nDevelopers wouldn't see the render when we move to terraform CDK anywho\n\nCon:\nDiagnosing an existing problem becomes difficult to debug\nDevops would like to see deployed resources\nQHub in its current state is not reliable enough to not have the rendered files\n\nConsensus:\nWe wait until qhub is more mature and re-evaluate",
    "actual_labels": [
      "configuration",
      "question",
      "security"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 43,
    "title": "Rendering vs not rendering terraform files in github",
    "body": "Pro:\nHide secrets\nCleaner repo\nDevelopers wouldn't see the render when we move to terraform CDK anywho\n\nCon:\nDiagnosing an existing problem becomes difficult to debug\nDevops would like to see deployed resources\nQHub in its current state is not reliable enough to not have the rendered files\n\nConsensus:\nWe wait until qhub is more mature and re-evaluate",
    "actual_labels": [
      "configuration",
      "question",
      "security"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 44,
    "title": "Isolate .local (i.e. pip) installations for each environment",
    "body": "### Feature description\r\n\r\nCurrently, if I pip install a package into a conda env from the command line, the packages get added to my private `.local` space. However, if I have a second environment which does the same thing, THOSE get added to the `.local` space as well. This creates a situation where the pip packages from BOTH environments are now available in BOTH environments. \r\n\r\nThe real kicker is that if the second environment required different versions of the pip packages, it goes ahead and updates the `.local`, leaving the first environment in a broken state. \r\n\r\nThis happens very much behind the scenes and users have no idea what happened to their first environment since they \"didn't touch it\". \r\n\r\nWe should isolate the pip installed packages so that there is no cross-contamination between environments. \r\n\r\n### Value and/or benefit\r\n\r\nClean, isolated environments that pip can't break and happy users :) \r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug",
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 44,
    "title": "Isolate .local (i.e. pip) installations for each environment",
    "body": "### Feature description\r\n\r\nCurrently, if I pip install a package into a conda env from the command line, the packages get added to my private `.local` space. However, if I have a second environment which does the same thing, THOSE get added to the `.local` space as well. This creates a situation where the pip packages from BOTH environments are now available in BOTH environments. \r\n\r\nThe real kicker is that if the second environment required different versions of the pip packages, it goes ahead and updates the `.local`, leaving the first environment in a broken state. \r\n\r\nThis happens very much behind the scenes and users have no idea what happened to their first environment since they \"didn't touch it\". \r\n\r\nWe should isolate the pip installed packages so that there is no cross-contamination between environments. \r\n\r\n### Value and/or benefit\r\n\r\nClean, isolated environments that pip can't break and happy users :) \r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug",
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 45,
    "title": "Investigate Cheaper Azure Instances",
    "body": "### Context\r\n\r\nI think the Standard_E4_v3 (4 vCPU, 32 GB RAM, \\~$184/month) would be a good candidate to replace the Standard_D8_v3 instance (\\~$280/month) for the general node. \r\n\r\n### Value and/or benefit\r\n\r\nCheaper Nebari deployment with no loss in functionality\r\n\r\n### Anything else?\r\n\r\nNA",
    "actual_labels": [
      "enhancement",
      "azure",
      "performance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 45,
    "title": "Investigate Cheaper Azure Instances",
    "body": "### Context\r\n\r\nI think the Standard_E4_v3 (4 vCPU, 32 GB RAM, \\~$184/month) would be a good candidate to replace the Standard_D8_v3 instance (\\~$280/month) for the general node. \r\n\r\n### Value and/or benefit\r\n\r\nCheaper Nebari deployment with no loss in functionality\r\n\r\n### Anything else?\r\n\r\nNA",
    "actual_labels": [
      "enhancement",
      "azure",
      "performance",
      "configuration"
    ],
    "predicted_labels": "enhancement, maintenance, aws, performance",
    "model_name": "fine_tuned"
  },
  {
    "index": 46,
    "title": "Fix code scanning alert - Clusters should be set to private",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/72\r\n",
    "actual_labels": [
      "bug",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 46,
    "title": "Fix code scanning alert - Clusters should be set to private",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/72\r\n",
    "actual_labels": [
      "bug",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 47,
    "title": "Fix code scanning alert - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to ...",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/42\r\n",
    "actual_labels": [
      "bug",
      "aws",
      "security"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 47,
    "title": "Fix code scanning alert - VPC Flow Logs is a feature that enables you to capture information about the IP traffic going to ...",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/42\r\n",
    "actual_labels": [
      "bug",
      "aws",
      "security"
    ],
    "predicted_labels": "bug, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 48,
    "title": "[ENH] - Add new node group for conda store workers",
    "body": "### Feature description\r\n\r\nWe are looking forward to the conda worker scaling in https://github.com/nebari-dev/nebari/pull/2384.  After it's merged, we want to move the conda store workers to it's own node group so the pods on the general node group don't exceed the general node group memory.\r\n\r\nA few specifics:\r\n- We want to add a new node group for most conda store workers.\r\n- We want to keep 1 conda store worker on the general node group so that conda store builds are speedy (may require increasing size of general node group nodes).  This should still scale down to 0 since conda store worker seems to have a memory leak issue [see here](https://github.com/nebari-dev/nebari/issues/2418).\r\n- We want to set the threading to 1 (currently 4) on each of the conda store worker pods to keep things easy to reason about\r\n- We need to get an idea of memory usage for an average conda store build so we can size the conda store worker nodes appropriately.  Update: In very loose testing, memory usage doesn't seem to be more than a few GiB, it seems like it's more the memory leak issue mentioned above.\r\n\r\n### Value and/or benefit\r\n\r\n-\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "enhancement",
      "performance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, azure, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance,",
    "model_name": "base"
  },
  {
    "index": 48,
    "title": "[ENH] - Add new node group for conda store workers",
    "body": "### Feature description\r\n\r\nWe are looking forward to the conda worker scaling in https://github.com/nebari-dev/nebari/pull/2384.  After it's merged, we want to move the conda store workers to it's own node group so the pods on the general node group don't exceed the general node group memory.\r\n\r\nA few specifics:\r\n- We want to add a new node group for most conda store workers.\r\n- We want to keep 1 conda store worker on the general node group so that conda store builds are speedy (may require increasing size of general node group nodes).  This should still scale down to 0 since conda store worker seems to have a memory leak issue [see here](https://github.com/nebari-dev/nebari/issues/2418).\r\n- We want to set the threading to 1 (currently 4) on each of the conda store worker pods to keep things easy to reason about\r\n- We need to get an idea of memory usage for an average conda store build so we can size the conda store worker nodes appropriately.  Update: In very loose testing, memory usage doesn't seem to be more than a few GiB, it seems like it's more the memory leak issue mentioned above.\r\n\r\n### Value and/or benefit\r\n\r\n-\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "enhancement",
      "performance",
      "configuration"
    ],
    "predicted_labels": "enhancement, ux/ui, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 49,
    "title": "[ENH] - Keycloak Group attributes to control JupyterLab profile access",
    "body": "### Feature description\n\nThis functionality has been dropped in the recent refactor.\r\n\r\nIf a Keycloak group had a `profile` attribute containing a list of JupyterLab profiles then members of that group would be able to access the profiles. This check only came into effect for profiles defined with `groups:` (which could be empty) in the `qhub-config.yaml` file. That choice was because profiles defined with no `groups:` (or `users:`) would be available to all users - i.e. no access control - so this ability to override dynamically by setting Keycloak group attributes is not wanted for those groups.\r\n\r\nI would like to reinstate this functionality. It is possible that some usage of roles in keycloak would be better, but as a short term solution at least, this functionality can be easily reinstated by adding a mapper to the jupyterhub client as follows:\r\n\r\n![Screenshot 2022-03-24 at 12 33 13](https://user-images.githubusercontent.com/786238/159918038-fc3367c8-905f-46f4-ba01-758994970a0b.png)\r\n\r\nWe would also need to reinstate the code that checks for this in `jupyterhub_config.py` or similar.\r\n\r\nI propose we change the attribute name from `profiles` to `jupyterlabprofiles` though.\r\n\r\n\n\n### Value and/or benefit\n\nThis allows profiles to be assigned to groups dynamically using Keycloak without a full redeploy.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 49,
    "title": "[ENH] - Keycloak Group attributes to control JupyterLab profile access",
    "body": "### Feature description\n\nThis functionality has been dropped in the recent refactor.\r\n\r\nIf a Keycloak group had a `profile` attribute containing a list of JupyterLab profiles then members of that group would be able to access the profiles. This check only came into effect for profiles defined with `groups:` (which could be empty) in the `qhub-config.yaml` file. That choice was because profiles defined with no `groups:` (or `users:`) would be available to all users - i.e. no access control - so this ability to override dynamically by setting Keycloak group attributes is not wanted for those groups.\r\n\r\nI would like to reinstate this functionality. It is possible that some usage of roles in keycloak would be better, but as a short term solution at least, this functionality can be easily reinstated by adding a mapper to the jupyterhub client as follows:\r\n\r\n![Screenshot 2022-03-24 at 12 33 13](https://user-images.githubusercontent.com/786238/159918038-fc3367c8-905f-46f4-ba01-758994970a0b.png)\r\n\r\nWe would also need to reinstate the code that checks for this in `jupyterhub_config.py` or similar.\r\n\r\nI propose we change the attribute name from `profiles` to `jupyterlabprofiles` though.\r\n\r\n\n\n### Value and/or benefit\n\nThis allows profiles to be assigned to groups dynamically using Keycloak without a full redeploy.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 50,
    "title": "Warning: Interpolation-only expressions are deprecated",
    "body": "```\r\nWarning: Interpolation-only expressions are deprecated\r\n\r\n  on .terraform/modules/qhub/modules/kubernetes/services/meta/qhub/main.tf line 16, in module \"kubernetes-jupyterhub\":\r\n  16:           \"${var.general-node-group.key}\" = var.general-node-group.value\r\n\r\nTerraform 0.11 and earlier required all non-constant expressions to be\r\nprovided via interpolation syntax, but this pattern is now deprecated. To\r\nsilence this warning, remove the \"${ sequence from the start and the }\"\r\nsequence from the end of this expression, leaving just the inner expression.\r\n\r\nTemplate interpolation syntax is still used to construct strings from\r\nexpressions when the template includes multiple interpolation sequences or a\r\nmixture of literal strings and interpolations. This deprecation applies only\r\nto templates that consist entirely of a single interpolation sequence.\r\n\r\n(and 3 more similar warnings elsewhere)\r\n```",
    "actual_labels": [
      "maintenance"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 50,
    "title": "Warning: Interpolation-only expressions are deprecated",
    "body": "```\r\nWarning: Interpolation-only expressions are deprecated\r\n\r\n  on .terraform/modules/qhub/modules/kubernetes/services/meta/qhub/main.tf line 16, in module \"kubernetes-jupyterhub\":\r\n  16:           \"${var.general-node-group.key}\" = var.general-node-group.value\r\n\r\nTerraform 0.11 and earlier required all non-constant expressions to be\r\nprovided via interpolation syntax, but this pattern is now deprecated. To\r\nsilence this warning, remove the \"${ sequence from the start and the }\"\r\nsequence from the end of this expression, leaving just the inner expression.\r\n\r\nTemplate interpolation syntax is still used to construct strings from\r\nexpressions when the template includes multiple interpolation sequences or a\r\nmixture of literal strings and interpolations. This deprecation applies only\r\nto templates that consist entirely of a single interpolation sequence.\r\n\r\n(and 3 more similar warnings elsewhere)\r\n```",
    "actual_labels": [
      "maintenance"
    ],
    "predicted_labels": "bug, maintenance, ci/cd, performance, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 51,
    "title": "[BUG] - Do not show oauth screen for users accessing the main jhub-apps service",
    "body": "### Describe the bug\n\nIn nebari (and often in JupyterHub in general) the auth sequence involves two or three OAuth providers:\r\n- JupyterHub OAuth provider, which calls\r\n- Keycloak OAuth provider, which (sometimes) calls\r\n- External third-party OAuth provider (e.g. Google or GitHub).\r\n\r\nIn such setups the JupyterHub OAuth provider is meant to be transparent to the users unless user grants additional permissions to third-party services. Typically JupyterHub OAuth checks the user identity when spawning servers, and verifies that user wants to share their identity (or specific permissions) with services.\r\n\r\nJhub-apps is a service which requires users to confirm that they agree to share the JupyterHub OAuth identity with the service. Currently, a confusing screen is shown to users when they first log-in into JupyterHub, after they already authenticated with Keycloak:\r\n\r\n![Screenshot from 2024-07-22 09-57-57](https://github.com/user-attachments/assets/7c694ad1-d85e-4ef8-b465-c2f2334e2f20)\r\n\r\nIt is confusing because users do not know what \"japps\" (sic!) service is, nor what the default scope description (\"Identify the owner of the requesting entity\") mean, since a very technical language is used.\r\n\r\nThis screen is not always necessary, user servers are by default confirmed without user action, and services \"blessed\" by administrator of the JupyterHub are also handled transparently. This is configured by setting `oauth_no_confirm` in the service configuration.\n\n### Expected behavior\n\nSince jhub-apps is tightly integrated with nebari it makes sense to exempt it from user making the explicit confirmation that they do want to share their identity with jhub-apps.\r\n\r\nIf we do want to keep this screen however, we may want to customize it to better describes which actions the service will perform on behalf of the user (like creating named servers, sharing servers, etc) as access to this actions is currently permitted via a separate token rather than through oauth scopes.\r\n\r\nWe previously had `oauth_no_confirm` set to `True` on some services used when jhub-apps is enabled. This got removed during JupyterHub 5 upgrade (https://github.com/nebari-dev/nebari/pull/2427) due to JupyterHub now requiring additional URL configuration for it (https://github.com/jupyterhub/jupyterhub/issues/4801). The removal was based on a conversation in which these services were described as applications that do not oauth with JupyterHub, but uses keycloak directly. This is correct, these services:\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/4f8fc54189df896fdb2c74b0fb15516b34c660b0/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/jupyterhub/files/jupyterhub/02-spawner.py#L77-L84\r\n\r\ndo not use JupyterHub oauth but they use Keycloak directly.\r\n\r\nHowever, `jhub-apps` itself as of today:\r\n- does not have any references to keycloak (and is designed to work in nebari-agnostic environments)\r\n- uses JupyterHub as an oauth provider, see [jhub_apps/service/security.py](https://github.com/nebari-dev/jhub-apps/blob/3df80a3f2a3aea7faadae2beb92f691fc475eacd/jhub_apps/service/security.py#L13-L86)\r\n\r\nThe code change would need to be done in:\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/4f8fc54189df896fdb2c74b0fb15516b34c660b0/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/jupyterhub/files/jupyterhub/02-spawner.py#L87\r\n\r\nBy passing a new `oauth_no_confirm` argument down to where the main jhub-app service is appended to `c.JupyterHub.services` in jhub-apps.\r\n\r\nWe should probably also rethink the name of the service, it is currently set to `japps` ([here](https://github.com/nebari-dev/jhub-apps/blob/3df80a3f2a3aea7faadae2beb92f691fc475eacd/jhub_apps/configuration.py#L42)) which does not correspond to `jhub-apps` used in the documentation.\n\n### OS and architecture in which you are running Nebari\n\nIrrelevant\n\n### How to Reproduce the problem?\n\n1. Deploy nebari with `jhub_apps.enabled: false`\r\n2. Create a new test user in keycloak\r\n3. Login to nebari as that test user\r\n3. See that everything is fine, logout\r\n4. Re-deploy nebari with `jhub_apps.enabled: true`\r\n5. Login to nebari as the test user again\r\n6. See the OAuth screen prompting for confirmation\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "ux/ui",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n\nAllowed labels with descriptions",
    "model_name": "base"
  },
  {
    "index": 51,
    "title": "[BUG] - Do not show oauth screen for users accessing the main jhub-apps service",
    "body": "### Describe the bug\n\nIn nebari (and often in JupyterHub in general) the auth sequence involves two or three OAuth providers:\r\n- JupyterHub OAuth provider, which calls\r\n- Keycloak OAuth provider, which (sometimes) calls\r\n- External third-party OAuth provider (e.g. Google or GitHub).\r\n\r\nIn such setups the JupyterHub OAuth provider is meant to be transparent to the users unless user grants additional permissions to third-party services. Typically JupyterHub OAuth checks the user identity when spawning servers, and verifies that user wants to share their identity (or specific permissions) with services.\r\n\r\nJhub-apps is a service which requires users to confirm that they agree to share the JupyterHub OAuth identity with the service. Currently, a confusing screen is shown to users when they first log-in into JupyterHub, after they already authenticated with Keycloak:\r\n\r\n![Screenshot from 2024-07-22 09-57-57](https://github.com/user-attachments/assets/7c694ad1-d85e-4ef8-b465-c2f2334e2f20)\r\n\r\nIt is confusing because users do not know what \"japps\" (sic!) service is, nor what the default scope description (\"Identify the owner of the requesting entity\") mean, since a very technical language is used.\r\n\r\nThis screen is not always necessary, user servers are by default confirmed without user action, and services \"blessed\" by administrator of the JupyterHub are also handled transparently. This is configured by setting `oauth_no_confirm` in the service configuration.\n\n### Expected behavior\n\nSince jhub-apps is tightly integrated with nebari it makes sense to exempt it from user making the explicit confirmation that they do want to share their identity with jhub-apps.\r\n\r\nIf we do want to keep this screen however, we may want to customize it to better describes which actions the service will perform on behalf of the user (like creating named servers, sharing servers, etc) as access to this actions is currently permitted via a separate token rather than through oauth scopes.\r\n\r\nWe previously had `oauth_no_confirm` set to `True` on some services used when jhub-apps is enabled. This got removed during JupyterHub 5 upgrade (https://github.com/nebari-dev/nebari/pull/2427) due to JupyterHub now requiring additional URL configuration for it (https://github.com/jupyterhub/jupyterhub/issues/4801). The removal was based on a conversation in which these services were described as applications that do not oauth with JupyterHub, but uses keycloak directly. This is correct, these services:\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/4f8fc54189df896fdb2c74b0fb15516b34c660b0/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/jupyterhub/files/jupyterhub/02-spawner.py#L77-L84\r\n\r\ndo not use JupyterHub oauth but they use Keycloak directly.\r\n\r\nHowever, `jhub-apps` itself as of today:\r\n- does not have any references to keycloak (and is designed to work in nebari-agnostic environments)\r\n- uses JupyterHub as an oauth provider, see [jhub_apps/service/security.py](https://github.com/nebari-dev/jhub-apps/blob/3df80a3f2a3aea7faadae2beb92f691fc475eacd/jhub_apps/service/security.py#L13-L86)\r\n\r\nThe code change would need to be done in:\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/4f8fc54189df896fdb2c74b0fb15516b34c660b0/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/jupyterhub/files/jupyterhub/02-spawner.py#L87\r\n\r\nBy passing a new `oauth_no_confirm` argument down to where the main jhub-app service is appended to `c.JupyterHub.services` in jhub-apps.\r\n\r\nWe should probably also rethink the name of the service, it is currently set to `japps` ([here](https://github.com/nebari-dev/jhub-apps/blob/3df80a3f2a3aea7faadae2beb92f691fc475eacd/jhub_apps/configuration.py#L42)) which does not correspond to `jhub-apps` used in the documentation.\n\n### OS and architecture in which you are running Nebari\n\nIrrelevant\n\n### How to Reproduce the problem?\n\n1. Deploy nebari with `jhub_apps.enabled: false`\r\n2. Create a new test user in keycloak\r\n3. Login to nebari as that test user\r\n3. See that everything is fine, logout\r\n4. Re-deploy nebari with `jhub_apps.enabled: true`\r\n5. Login to nebari as the test user again\r\n6. See the OAuth screen prompting for confirmation\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "ux/ui",
      "configuration"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 52,
    "title": "[BUG] - Error when using non \"Gi\" units in storage section of Nebari config",
    "body": "### Describe the bug\r\n\r\nIf you use any unit not accepted by Kubernetes as listed [here](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory) in the storage section then an error is thrown.  We should restrict the units used to those supported by Kubernetes.  The part of the config I'm talking about is below.\r\n\r\n```\r\nstorage:\r\n  conda_store: 2048Gi\r\n  shared_filesystem: 1024Gi\r\n```",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 52,
    "title": "[BUG] - Error when using non \"Gi\" units in storage section of Nebari config",
    "body": "### Describe the bug\r\n\r\nIf you use any unit not accepted by Kubernetes as listed [here](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#meaning-of-memory) in the storage section then an error is thrown.  We should restrict the units used to those supported by Kubernetes.  The part of the config I'm talking about is below.\r\n\r\n```\r\nstorage:\r\n  conda_store: 2048Gi\r\n  shared_filesystem: 1024Gi\r\n```",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 53,
    "title": "[ENH] - Make Nebari admin group also have keycloak realm admin permissions",
    "body": "### Feature description\n\nCurrently users have to login with the root keycloak credentials to manage users/groups etc. It would be nice to make the admins of a particular realm in the \"admin\" group also have permissions to edit keycloak without requiring the root username and password.\r\n\r\nRelated docs https://github.com/keycloak/keycloak-documentation/blob/main/server_admin/topics/admin-console-permissions/master-realm.adoc#master-realm-access-control\n\n### Value and/or benefit\n\nSecurity and easy ability to add users to self manage the cluster.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, case,",
    "model_name": "base"
  },
  {
    "index": 53,
    "title": "[ENH] - Make Nebari admin group also have keycloak realm admin permissions",
    "body": "### Feature description\n\nCurrently users have to login with the root keycloak credentials to manage users/groups etc. It would be nice to make the admins of a particular realm in the \"admin\" group also have permissions to edit keycloak without requiring the root username and password.\r\n\r\nRelated docs https://github.com/keycloak/keycloak-documentation/blob/main/server_admin/topics/admin-console-permissions/master-realm.adoc#master-realm-access-control\n\n### Value and/or benefit\n\nSecurity and easy ability to add users to self manage the cluster.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "security",
      "configuration"
    ],
    "predicted_labels": "enhancement, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 54,
    "title": "Use jupyterhub as a single sign-on authentication provider",
    "body": "This would allow easy securing of domains served within kubernetes cluster",
    "actual_labels": [
      "enhancement",
      "security"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 54,
    "title": "Use jupyterhub as a single sign-on authentication provider",
    "body": "This would allow easy securing of domains served within kubernetes cluster",
    "actual_labels": [
      "enhancement",
      "security"
    ],
    "predicted_labels": "enhancement, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 55,
    "title": "[BUG] - Upgrading from 2023.10.1 to 2023.11.1+ causes `nebari deploy` to hang",
    "body": "### Describe the bug\r\n\r\nIn version 2023.11.1 kbatch, clearml, and prefect were removed from Nebari.  If you upgrade from a prior version, `clearml.tf`, `kbatch.tf`, and `prefect.tf` persist in `./stages/07-kubernetes-services/` and are not deleted when running `nebari render` or `nebari deploy` commands.\r\n\r\n### Expected behavior\r\n\r\nI would expect the removed files to be deleted during the upgrade process\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nNixOS - x86-64\r\n\r\n### How to Reproduce the problem?\r\n\r\nInstall nebari version 2023.10.1, then run `nebari upgrade` to version 2024.1.1.  Then run `nebari deploy -c nebari-config.yaml` and deployment will hang asking for an unset, required terraform variable `clearml-enable-forwardauth`.\r\n\r\n### Other\r\nAt a minimum, I think we should put a note telling people to manually delete those files when upgrading to version 2023.11.1.",
    "actual_labels": [
      "bug",
      "configuration",
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, azure, security, performance, ux/ui, configuration, dependency-update, third-party-update,  aws, gcp",
    "model_name": "base"
  },
  {
    "index": 55,
    "title": "[BUG] - Upgrading from 2023.10.1 to 2023.11.1+ causes `nebari deploy` to hang",
    "body": "### Describe the bug\r\n\r\nIn version 2023.11.1 kbatch, clearml, and prefect were removed from Nebari.  If you upgrade from a prior version, `clearml.tf`, `kbatch.tf`, and `prefect.tf` persist in `./stages/07-kubernetes-services/` and are not deleted when running `nebari render` or `nebari deploy` commands.\r\n\r\n### Expected behavior\r\n\r\nI would expect the removed files to be deleted during the upgrade process\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nNixOS - x86-64\r\n\r\n### How to Reproduce the problem?\r\n\r\nInstall nebari version 2023.10.1, then run `nebari upgrade` to version 2024.1.1.  Then run `nebari deploy -c nebari-config.yaml` and deployment will hang asking for an unset, required terraform variable `clearml-enable-forwardauth`.\r\n\r\n### Other\r\nAt a minimum, I think we should put a note telling people to manually delete those files when upgrading to version 2023.11.1.",
    "actual_labels": [
      "bug",
      "configuration",
      "documentation"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 56,
    "title": "Launching spark clusters in QHub",
    "body": "per Chris:\r\n\r\nCreate a general launcher for launching various clusters. IE Spark, MPI, Dask\r\n\r\nSimilar to kubernetes launcher. Have a CRD (https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) that launches higher level objects. Could be leveraged for CDS dashboards and the above. Does lock us into using Kubernetes. There already exists a pyspark CRD.",
    "actual_labels": [
      "enhancement"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, case study, launching spark clusters in QHub",
    "model_name": "base"
  },
  {
    "index": 56,
    "title": "Launching spark clusters in QHub",
    "body": "per Chris:\r\n\r\nCreate a general launcher for launching various clusters. IE Spark, MPI, Dask\r\n\r\nSimilar to kubernetes launcher. Have a CRD (https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) that launches higher level objects. Could be leveraged for CDS dashboards and the above. Does lock us into using Kubernetes. There already exists a pyspark CRD.",
    "actual_labels": [
      "enhancement"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 57,
    "title": "Document that 0.13.5 is required and how to download",
    "body": "The docs should detail how to install terraform and a note that a certain version is required.\r\n\r\n1. Download the binary with url\r\n2. Add to `~/.local/bin`\r\n3. Add to your path `export PATH=$HOME/.local/bin:$PATH`if not already in your path\r\n",
    "actual_labels": [
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 57,
    "title": "Document that 0.13.5 is required and how to download",
    "body": "The docs should detail how to install terraform and a note that a certain version is required.\r\n\r\n1. Download the binary with url\r\n2. Add to `~/.local/bin`\r\n3. Add to your path `export PATH=$HOME/.local/bin:$PATH`if not already in your path\r\n",
    "actual_labels": [
      "documentation"
    ],
    "predicted_labels": "documentation, maintenance, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 58,
    "title": ".pem integration [enhancement]",
    "body": "## Description\r\n\r\nThis is a client request. We would like to support the following usage:\r\n\r\n```\r\nimport swat\r\nimport os\r\n\r\nos.environ['CAS_CLIENT_SSL_CA_LIST'] = \"/opt/sas/viya/config/etc/SASSecurityCertificateFramework/cacerts/trustedcerts.pem\"\r\n\r\nconn = swat.CAS('https://sas.url.com/SASDrive', 5570,'username','password')\r\n```\r\n\r\nFor this to work we think that the cert has to be installed in the jupyterlab instance. Is this correct @costrouc? \r\n",
    "actual_labels": [
      "enhancement",
      "question",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 58,
    "title": ".pem integration [enhancement]",
    "body": "## Description\r\n\r\nThis is a client request. We would like to support the following usage:\r\n\r\n```\r\nimport swat\r\nimport os\r\n\r\nos.environ['CAS_CLIENT_SSL_CA_LIST'] = \"/opt/sas/viya/config/etc/SASSecurityCertificateFramework/cacerts/trustedcerts.pem\"\r\n\r\nconn = swat.CAS('https://sas.url.com/SASDrive', 5570,'username','password')\r\n```\r\n\r\nFor this to work we think that the cert has to be installed in the jupyterlab instance. Is this correct @costrouc? \r\n",
    "actual_labels": [
      "enhancement",
      "question",
      "security",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 59,
    "title": "[ENH] - Order Nebari commands in CLI more consistent with the order of their use",
    "body": "### Feature description\r\n\r\nAs you can see commands are ordered `destroy`, `deploy`, `init`, `info`.  That is roughly opposite of what I'd like since the first command teh user does is init or info, then deploy, then destroy.  \r\n![image](https://github.com/user-attachments/assets/c065e877-966c-4a49-9145-ad14408f4a0e)\r\n\r\n\r\n### Value and/or benefit\r\n\r\nMore intuitive UX\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 59,
    "title": "[ENH] - Order Nebari commands in CLI more consistent with the order of their use",
    "body": "### Feature description\r\n\r\nAs you can see commands are ordered `destroy`, `deploy`, `init`, `info`.  That is roughly opposite of what I'd like since the first command teh user does is init or info, then deploy, then destroy.  \r\n![image](https://github.com/user-attachments/assets/c065e877-966c-4a49-9145-ad14408f4a0e)\r\n\r\n\r\n### Value and/or benefit\r\n\r\nMore intuitive UX\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 60,
    "title": "[ENH] - Store user Jupyter settings on NFS ",
    "body": "### Feature description\n\nWould it be possible to store the user's jupyter settings on NFS rather than the JHub server?\n\n### Value and/or benefit\n\nWhenever an upgrade (and maybe a redeploy?) happens, I lose my jupyter settings. Its not a huge deal, just frustrating when my keybindings suddenly go missing and I have to set them up again. \r\n\r\nI'm guessing this is happening because the user settings are currently kept on the JHub server which is destroyed. If they were kept on NFS, we could have persistent settings... right??\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 60,
    "title": "[ENH] - Store user Jupyter settings on NFS ",
    "body": "### Feature description\n\nWould it be possible to store the user's jupyter settings on NFS rather than the JHub server?\n\n### Value and/or benefit\n\nWhenever an upgrade (and maybe a redeploy?) happens, I lose my jupyter settings. Its not a huge deal, just frustrating when my keybindings suddenly go missing and I have to set them up again. \r\n\r\nI'm guessing this is happening because the user settings are currently kept on the JHub server which is destroyed. If they were kept on NFS, we could have persistent settings... right??\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 61,
    "title": "Make github action auto-update optional for configuration",
    "body": "",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 61,
    "title": "Make github action auto-update optional for configuration",
    "body": "",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 62,
    "title": "[BUG] - local deploy cannot guaranteed be done without having access to a domain",
    "body": "### Describe the bug\r\n\r\nRelated to #1703. TL;DR: our guide tells users to fix `/etc/hosts`, but the deployed cluster doesn't see this change. There are three ways to achieve that:\r\n\r\n1. Have access to a domain and create a DNS entry for `--domain=...` to point to `172.18.1.100`. This is what we do for `github-actions.nebari.dev` to get our CI going:\r\n   https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/.github/workflows/kubernetes_test.yaml#L95\r\n2. Use a service like [nip.io](https://nip.io/) and set something like `--domain=172.18.1.100.nip.io` during `nebari init`. With this users no longer need to have a domain available, but still need to have access to the internet. Plus, we add an external dependency, since our guide then depends on nip.io being available.\r\n3. Instead of using an URL, users could also use the IP directly, i.e. `--domain=172.18.1.100`. It is not as pretty as having an URL, but probably good enough for local deploy. The major advantage is that this can be done even without internet access making it the most reliable and easiest to use option of the three. Doing this, the user also doesn't need to fix `/etc/hosts`.\r\n\r\nRegardless of what we decide we want to use, 2. and 3. (as well as our current docs) make an assumption that doesn't hold in 100% of the cases: the cluster IP is fixed at `172.18.1.100`. \r\n\r\nHowever, we don't know that. It should work most of the time, but I know for example that it doesn't work for @costrouc. Maybe he can fill in some details on when this won't be true?\r\n\r\nInternally, we only know the correct IP after stage 4:\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/deploy.py#L102-L104\r\n\r\nMeaning, setting it upfront during `nebari init --domain` is not guaranteed to work.\r\n\r\n### Expected behavior\r\n\r\nI think we should allow setting no domain upfront and instead of asking the user to set up their DNS (when `--disallow-prompt` is not set)\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/deploy.py#L128-L131\r\n\r\njust use the IP we get from the load balancer.\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nLinux\r\n\r\n### How to Reproduce the problem?\r\n\r\nFollow the [local deployment how-to](https://www.nebari.dev/docs/how-tos/nebari-local) in an environment where the load balancer doesn't use `172.18.1.100` as IP.\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug",
      "enhancement",
      "documentation",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 62,
    "title": "[BUG] - local deploy cannot guaranteed be done without having access to a domain",
    "body": "### Describe the bug\r\n\r\nRelated to #1703. TL;DR: our guide tells users to fix `/etc/hosts`, but the deployed cluster doesn't see this change. There are three ways to achieve that:\r\n\r\n1. Have access to a domain and create a DNS entry for `--domain=...` to point to `172.18.1.100`. This is what we do for `github-actions.nebari.dev` to get our CI going:\r\n   https://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/.github/workflows/kubernetes_test.yaml#L95\r\n2. Use a service like [nip.io](https://nip.io/) and set something like `--domain=172.18.1.100.nip.io` during `nebari init`. With this users no longer need to have a domain available, but still need to have access to the internet. Plus, we add an external dependency, since our guide then depends on nip.io being available.\r\n3. Instead of using an URL, users could also use the IP directly, i.e. `--domain=172.18.1.100`. It is not as pretty as having an URL, but probably good enough for local deploy. The major advantage is that this can be done even without internet access making it the most reliable and easiest to use option of the three. Doing this, the user also doesn't need to fix `/etc/hosts`.\r\n\r\nRegardless of what we decide we want to use, 2. and 3. (as well as our current docs) make an assumption that doesn't hold in 100% of the cases: the cluster IP is fixed at `172.18.1.100`. \r\n\r\nHowever, we don't know that. It should work most of the time, but I know for example that it doesn't work for @costrouc. Maybe he can fill in some details on when this won't be true?\r\n\r\nInternally, we only know the correct IP after stage 4:\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/deploy.py#L102-L104\r\n\r\nMeaning, setting it upfront during `nebari init --domain` is not guaranteed to work.\r\n\r\n### Expected behavior\r\n\r\nI think we should allow setting no domain upfront and instead of asking the user to set up their DNS (when `--disallow-prompt` is not set)\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/daecbcf37497b1b8b4e3f0f52667ababf3c023ad/nebari/deploy.py#L128-L131\r\n\r\njust use the IP we get from the load balancer.\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nLinux\r\n\r\n### How to Reproduce the problem?\r\n\r\nFollow the [local deployment how-to](https://www.nebari.dev/docs/how-tos/nebari-local) in an environment where the load balancer doesn't use `172.18.1.100` as IP.\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug",
      "enhancement",
      "documentation",
      "configuration"
    ],
    "predicted_labels": "bug, configuration, dependency-update",
    "model_name": "fine_tuned"
  },
  {
    "index": 63,
    "title": "[ENH] - Deploying within AWS VPC",
    "body": "### Discussed in https://github.com/orgs/nebari-dev/discussions/1518\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **martinfleis** October 27, 2022</sup>\r\nWe have a RDS Postgres database running within a VPC and would need to be able to access it from within the QHub/nebari infrastructure. I see the [documentation on how to do that within Azure and GC ](https://docs.qhub.dev/en/stable/source/installation/configuration.html?highlight=virtual%20private#deployment-inside-virtual-private-network)but am not sure if the same is possible within AWS. Is that possible? Any idea how can I achieve that?</div>\r\n\r\nAs [@viniciusdc mentioned, the actions are](https://github.com/orgs/nebari-dev/discussions/1518#discussioncomment-4353286):\r\n\r\n> If I understood everything correctly, there are two requests here:\r\n> \r\n> 1. deploying nebari on an existing AWS VPC\r\n> 2. deploying a third-party app into Nebari, allowing access to another VPC\r\n> \r\n> (1) should automatically solve 2., as all apps would then be in the same VPC. The other way around would need more work.\r\n> \r\n> So, for the first item, currently, Nebari accepts overwriting of the VPC/Networking for both Azure, and GCP ([see old docs](https://docs.qhub.dev/en/stable/source/installation/configuration.html?highlight=vpc#deployment-inside-virtual-private-network)). The network configuration of Nebari VPC on AWS are defined [here](https://github.com/nebari-dev/nebari/blob/release/2022.11.1/nebari/template/stages/02-infrastructure/aws/modules/network/main.tf), so it is possible to implement an overriding mechanism to import an existing VPC resource to terraform. However, this would need to be scoped as an ENH issue/task.\r\n> \r\n> (2) For this particular case of a DB access over VPCs... based on [AWS docs](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html#USER_VPC.Scenario3), using a [VPC peering](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html) seems to be the solution. I think Nebari might not be able to offer it as is during deployment due to its specificness, but we could add some toggle switch in the yaml to deploy such a Link. For a general third-party tool, we could deploy a standalone app using the nebari `helm-extension` mechanism and allow it IP to use the VPC tunnel.\r\n> \r\n> All of the above would be wonderful enchantments to the user customization capabilities in the nebari config, and require each a separate ENH issue to track \ud83d\ude04\r\n\r\n\r\n",
    "actual_labels": [
      "enhancement",
      "documentation",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 63,
    "title": "[ENH] - Deploying within AWS VPC",
    "body": "### Discussed in https://github.com/orgs/nebari-dev/discussions/1518\r\n\r\n<div type='discussions-op-text'>\r\n\r\n<sup>Originally posted by **martinfleis** October 27, 2022</sup>\r\nWe have a RDS Postgres database running within a VPC and would need to be able to access it from within the QHub/nebari infrastructure. I see the [documentation on how to do that within Azure and GC ](https://docs.qhub.dev/en/stable/source/installation/configuration.html?highlight=virtual%20private#deployment-inside-virtual-private-network)but am not sure if the same is possible within AWS. Is that possible? Any idea how can I achieve that?</div>\r\n\r\nAs [@viniciusdc mentioned, the actions are](https://github.com/orgs/nebari-dev/discussions/1518#discussioncomment-4353286):\r\n\r\n> If I understood everything correctly, there are two requests here:\r\n> \r\n> 1. deploying nebari on an existing AWS VPC\r\n> 2. deploying a third-party app into Nebari, allowing access to another VPC\r\n> \r\n> (1) should automatically solve 2., as all apps would then be in the same VPC. The other way around would need more work.\r\n> \r\n> So, for the first item, currently, Nebari accepts overwriting of the VPC/Networking for both Azure, and GCP ([see old docs](https://docs.qhub.dev/en/stable/source/installation/configuration.html?highlight=vpc#deployment-inside-virtual-private-network)). The network configuration of Nebari VPC on AWS are defined [here](https://github.com/nebari-dev/nebari/blob/release/2022.11.1/nebari/template/stages/02-infrastructure/aws/modules/network/main.tf), so it is possible to implement an overriding mechanism to import an existing VPC resource to terraform. However, this would need to be scoped as an ENH issue/task.\r\n> \r\n> (2) For this particular case of a DB access over VPCs... based on [AWS docs](https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_VPC.Scenarios.html#USER_VPC.Scenario3), using a [VPC peering](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html) seems to be the solution. I think Nebari might not be able to offer it as is during deployment due to its specificness, but we could add some toggle switch in the yaml to deploy such a Link. For a general third-party tool, we could deploy a standalone app using the nebari `helm-extension` mechanism and allow it IP to use the VPC tunnel.\r\n> \r\n> All of the above would be wonderful enchantments to the user customization capabilities in the nebari config, and require each a separate ENH issue to track \ud83d\ude04\r\n\r\n\r\n",
    "actual_labels": [
      "enhancement",
      "documentation",
      "aws",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 64,
    "title": "[BUG] - JupyterHub navbar items not redirecting correctly after visiting Admin Dashboard",
    "body": "### Describe the bug\r\n\r\nNebari borrows some navbar items from JupyterHub, such as the Home, Token and Admin tabs. These work fine until one goes to the Admin tab. After visiting the Admin Dashboard, trying to go back to any of the Home, Token, and even the Admin tabs result in a `404 page not found error`. Here is an example of the Token tab, before and after visiting the Admin Dashboard:\r\n\r\nhttps://github.com/nebari-dev/nebari/assets/36754005/df30e9cb-0f35-457f-864e-580aa49c15cd\r\n\r\nAs you can see, the Token tab URL is `/hub/token` before visiting the Admin Dashboard, but becomes `/token` after that.\r\n\r\nI can reproduce this behavior (i.e. URLs changing after visiting the Admin Dashboard; see lower left corner in the video) locally using vanilla JupyterHub, with the difference that redirection works fine:\r\n\r\nhttps://github.com/nebari-dev/nebari/assets/36754005/f5039655-ef57-4205-9c96-a257d43002ed\r\n\r\nI guess we can have Nebari redirect `/token` to `/hub/token` but we would need to make sure that's not a general rule, as other services don't have the `/hub` prefix. For example: `/conda-store`.\r\n\r\n### Expected behavior\r\n\r\nI would expect for users to be able to go to the Admin Dashboard and then navigate back successfully to other sections of JupyterHub (e.g., the Home or Token tabs).\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nAWS\r\n\r\n### How to Reproduce the problem?\r\n\r\n1. Visit the Admin Dashboard at `/hub/admin`\r\n2. Try to navigate to one of the Home, Token or Admin tabs.\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\nNebari `2024.3.2`\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\nThis issue was originally identified by @kalpanachinnappan.",
    "actual_labels": [
      "bug",
      "aws",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 64,
    "title": "[BUG] - JupyterHub navbar items not redirecting correctly after visiting Admin Dashboard",
    "body": "### Describe the bug\r\n\r\nNebari borrows some navbar items from JupyterHub, such as the Home, Token and Admin tabs. These work fine until one goes to the Admin tab. After visiting the Admin Dashboard, trying to go back to any of the Home, Token, and even the Admin tabs result in a `404 page not found error`. Here is an example of the Token tab, before and after visiting the Admin Dashboard:\r\n\r\nhttps://github.com/nebari-dev/nebari/assets/36754005/df30e9cb-0f35-457f-864e-580aa49c15cd\r\n\r\nAs you can see, the Token tab URL is `/hub/token` before visiting the Admin Dashboard, but becomes `/token` after that.\r\n\r\nI can reproduce this behavior (i.e. URLs changing after visiting the Admin Dashboard; see lower left corner in the video) locally using vanilla JupyterHub, with the difference that redirection works fine:\r\n\r\nhttps://github.com/nebari-dev/nebari/assets/36754005/f5039655-ef57-4205-9c96-a257d43002ed\r\n\r\nI guess we can have Nebari redirect `/token` to `/hub/token` but we would need to make sure that's not a general rule, as other services don't have the `/hub` prefix. For example: `/conda-store`.\r\n\r\n### Expected behavior\r\n\r\nI would expect for users to be able to go to the Admin Dashboard and then navigate back successfully to other sections of JupyterHub (e.g., the Home or Token tabs).\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nAWS\r\n\r\n### How to Reproduce the problem?\r\n\r\n1. Visit the Admin Dashboard at `/hub/admin`\r\n2. Try to navigate to one of the Home, Token or Admin tabs.\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\nNebari `2024.3.2`\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\nThis issue was originally identified by @kalpanachinnappan.",
    "actual_labels": [
      "bug",
      "aws",
      "ux/ui"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 65,
    "title": "ExternalDNS",
    "body": "[ExternalDNS](https://github.com/kubernetes-sigs/external-dns) looks like a possible way to add support for lots of DNS providers.  Helm chart is [here](https://github.com/bitnami/charts/tree/master/bitnami/external-dns/).",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 65,
    "title": "ExternalDNS",
    "body": "[ExternalDNS](https://github.com/kubernetes-sigs/external-dns) looks like a possible way to add support for lots of DNS providers.  Helm chart is [here](https://github.com/bitnami/charts/tree/master/bitnami/external-dns/).",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 66,
    "title": "[BUG] - JHubApps Create App Page Selects only First ~100 Conda Environments",
    "body": "### Context\r\n\r\nThe Software Environment dropdown on the create page truncates after the first (apparently 98) environments in alpha order\r\n\r\nThis first came up when a user reported to that he couldn't see his conda store envs when he tried to create apps on\u00a0a nebari deployment.  Further testing verified this (username beginning with 'z' can not see their apps in the same env.  Creating another env with a username starting with 'k' bumped a 't...' environment off the list in the dropdown)The Software Environment dropdown on the create page truncates after the first (apparently 98) environments in alpha order\r\n\r\n### Value and/or benefit\r\n\r\nThis is a bug - we can't assume that there will only be <100 valid options in the environment selector even after https://github.com/nebari-dev/nebari/issues/2193 is addressed\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 66,
    "title": "[BUG] - JHubApps Create App Page Selects only First ~100 Conda Environments",
    "body": "### Context\r\n\r\nThe Software Environment dropdown on the create page truncates after the first (apparently 98) environments in alpha order\r\n\r\nThis first came up when a user reported to that he couldn't see his conda store envs when he tried to create apps on\u00a0a nebari deployment.  Further testing verified this (username beginning with 'z' can not see their apps in the same env.  Creating another env with a username starting with 'k' bumped a 't...' environment off the list in the dropdown)The Software Environment dropdown on the create page truncates after the first (apparently 98) environments in alpha order\r\n\r\n### Value and/or benefit\r\n\r\nThis is a bug - we can't assume that there will only be <100 valid options in the environment selector even after https://github.com/nebari-dev/nebari/issues/2193 is addressed\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug",
      "ux/ui"
    ],
    "predicted_labels": "bug, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 67,
    "title": "[RELEASE] `2022.11.1`",
    "body": "# Release Checklist\r\n\r\n## Release details\r\n\r\nScheduled release date - 2022-11-30\r\n\r\nRelease captain responsible - @iameskild\r\n\r\n\r\n## Starting point - a new release is out\r\n\r\n- [x] Create *this* issue to track and discuss the upcoming release.\r\n- [x] Use the previous release issue for any final release-specific discussions, then close.\r\n  - This can be a good time to debrief and discuss improvements to the release process.\r\n\r\n\r\n## Looking forward - planning\r\n\r\n- [x] [Create milestone for next release](https://github.com/nebari-dev/nebari/milestones) (if it doesn't already exist) and link it back here.\r\n- [x] Triage `bugs` to determine what be should included in the release and add it to the milestone.\r\n- [x] What new features, if any, will be included in the release and add it to the milestone.\r\n  - This will be, in large part, determined by the roadmap.\r\n  - Is there a focus for this release (i.e. UX/UI, stabilitation, etc.)?\r\n\r\n\r\n## Pre-release process\r\n\r\n- [x] Decide on a date for the release.\r\n  - What outstanding issues need to be addressed?\r\n  - Has documentation been updated appropriately?\r\n  - Are there any breaking changes that should be highlighted?\r\n  - Are there any upstream releases we are waiting on?\r\n  - [Do we need to update the `dask` versions in the `nebari-dask`?](https://github.com/conda-forge/nebari-dask-feedstock/blob/main/recipe/meta.yaml#L13-L16)\r\n  - Will there be an accompanying blog post?\r\n- [x] Prepare for the release.\r\n  - [x] Announce build freeze.\r\n  - [x] Release Candidate (RC) cycle.\r\n    - Is this a hotfix?\r\n      - [x] Create a new branch off of the last version tag.\r\n        - Use this branch to cut the pre-release and the \"official\" release.\r\n      - [x] `git cherry-pick` the commits that should be included.\r\n    - [x] Perform end-to-end testing. [Use the Testing Checklist template.](https://github.com/nebari-dev/nebari/issues/new?assignees=&labels=type%3A+release+%F0%9F%8F%B7&template=testing-checklist.md&title=Testing+checklist+for+<version>)\r\n      - For minor releases, relying on the end-to-end integration tests might suffice.\r\n    - [x] [Cut RC via GHA release workflow (w/ \"This is a pre-release\" checked).](https://github.com/nebari-dev/nebari/releases/new)\r\n    - [x] End-user validation.\r\n      - If possible, pull in volunteers to help test.\r\n      - (Repeat steps if necessary)\r\n  - [x] [Update `RELEASE.md` notes.](https://github.com/nebari-dev/nebari/blob/main/RELEASE.md)\r\n\r\n\r\n## Cut the official release\r\n\r\n*These steps must be actioned in the order they appear in this checklist.*\r\n- [x] [Tag, build and push docker images](https://github.com/nebari-dev/nebari-docker-images/releases/new)\r\n- [x] [Update and cut release for `nebari-dask` meta package on Conda-Forge.](https://github.com/conda-forge/nebari-dask-feedstock)\r\n- [x] [Cut PyPI release via GHA release workflow.](https://github.com/nebari-dev/nebari/releases/new)\r\n  - Avoid appending `v` to tag.\r\n    - Copy release notes from `RELEASE.md`.\r\n- [x] Merge release branch into `main`\r\n- [ ] [Merge automated release PR for `nebari` on Conda-Forge.](https://github.com/conda-forge/nebari-feedstock)",
    "actual_labels": [
      "release",
      "ci/cd",
      "testing"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 67,
    "title": "[RELEASE] `2022.11.1`",
    "body": "# Release Checklist\r\n\r\n## Release details\r\n\r\nScheduled release date - 2022-11-30\r\n\r\nRelease captain responsible - @iameskild\r\n\r\n\r\n## Starting point - a new release is out\r\n\r\n- [x] Create *this* issue to track and discuss the upcoming release.\r\n- [x] Use the previous release issue for any final release-specific discussions, then close.\r\n  - This can be a good time to debrief and discuss improvements to the release process.\r\n\r\n\r\n## Looking forward - planning\r\n\r\n- [x] [Create milestone for next release](https://github.com/nebari-dev/nebari/milestones) (if it doesn't already exist) and link it back here.\r\n- [x] Triage `bugs` to determine what be should included in the release and add it to the milestone.\r\n- [x] What new features, if any, will be included in the release and add it to the milestone.\r\n  - This will be, in large part, determined by the roadmap.\r\n  - Is there a focus for this release (i.e. UX/UI, stabilitation, etc.)?\r\n\r\n\r\n## Pre-release process\r\n\r\n- [x] Decide on a date for the release.\r\n  - What outstanding issues need to be addressed?\r\n  - Has documentation been updated appropriately?\r\n  - Are there any breaking changes that should be highlighted?\r\n  - Are there any upstream releases we are waiting on?\r\n  - [Do we need to update the `dask` versions in the `nebari-dask`?](https://github.com/conda-forge/nebari-dask-feedstock/blob/main/recipe/meta.yaml#L13-L16)\r\n  - Will there be an accompanying blog post?\r\n- [x] Prepare for the release.\r\n  - [x] Announce build freeze.\r\n  - [x] Release Candidate (RC) cycle.\r\n    - Is this a hotfix?\r\n      - [x] Create a new branch off of the last version tag.\r\n        - Use this branch to cut the pre-release and the \"official\" release.\r\n      - [x] `git cherry-pick` the commits that should be included.\r\n    - [x] Perform end-to-end testing. [Use the Testing Checklist template.](https://github.com/nebari-dev/nebari/issues/new?assignees=&labels=type%3A+release+%F0%9F%8F%B7&template=testing-checklist.md&title=Testing+checklist+for+<version>)\r\n      - For minor releases, relying on the end-to-end integration tests might suffice.\r\n    - [x] [Cut RC via GHA release workflow (w/ \"This is a pre-release\" checked).](https://github.com/nebari-dev/nebari/releases/new)\r\n    - [x] End-user validation.\r\n      - If possible, pull in volunteers to help test.\r\n      - (Repeat steps if necessary)\r\n  - [x] [Update `RELEASE.md` notes.](https://github.com/nebari-dev/nebari/blob/main/RELEASE.md)\r\n\r\n\r\n## Cut the official release\r\n\r\n*These steps must be actioned in the order they appear in this checklist.*\r\n- [x] [Tag, build and push docker images](https://github.com/nebari-dev/nebari-docker-images/releases/new)\r\n- [x] [Update and cut release for `nebari-dask` meta package on Conda-Forge.](https://github.com/conda-forge/nebari-dask-feedstock)\r\n- [x] [Cut PyPI release via GHA release workflow.](https://github.com/nebari-dev/nebari/releases/new)\r\n  - Avoid appending `v` to tag.\r\n    - Copy release notes from `RELEASE.md`.\r\n- [x] Merge release branch into `main`\r\n- [ ] [Merge automated release PR for `nebari` on Conda-Forge.](https://github.com/conda-forge/nebari-feedstock)",
    "actual_labels": [
      "release",
      "ci/cd",
      "testing"
    ],
    "predicted_labels": "release, maintenance, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 68,
    "title": "END-GAME -  Transfer `quansight/qhub-jupyterhub-theme` \u2192 `nebari-dev/nebari-jupyterhub-theme`",
    "body": "",
    "actual_labels": [
      "maintenance"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 68,
    "title": "END-GAME -  Transfer `quansight/qhub-jupyterhub-theme` \u2192 `nebari-dev/nebari-jupyterhub-theme`",
    "body": "",
    "actual_labels": [
      "maintenance"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 69,
    "title": "[ENH] - Integrate conda-store 2024.1.1 release",
    "body": "### Feature description\n\nThis issue is to add the latest conda-store release (2024.1.1) to Nebari. There are significant changes in this conda-store release so it will need thorough testing. \n\n### Value and/or benefit\n\nUpdated conda-store.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "dependency-update",
      "testing"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 69,
    "title": "[ENH] - Integrate conda-store 2024.1.1 release",
    "body": "### Feature description\n\nThis issue is to add the latest conda-store release (2024.1.1) to Nebari. There are significant changes in this conda-store release so it will need thorough testing. \n\n### Value and/or benefit\n\nUpdated conda-store.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "dependency-update",
      "testing"
    ],
    "predicted_labels": "enhancement, ci/cd, testing",
    "model_name": "fine_tuned"
  },
  {
    "index": 70,
    "title": "[BUG] - nebari upgrade doesn't match -gpu images",
    "body": "### Describe the bug\r\n\r\nRunning `nebari upgrade` fails to find/upgrade included gpu images when searching for tags to upgrade to the current version. Ex: `quay.io/nebari/nebari-jupyterlab-gpu:2023.5.1` will get skipped, but `quay.io/nebari/nebari-jupyterlab:2023.5.1` will trigger an upgrade prompt.\r\n\r\n### Expected behavior\r\n\r\nAll [valid nebari image tags](https://quay.io/organization/nebari) should be matched and trigger the associated upgrade behavior.\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nUbuntu Linux, x64\r\n\r\n### How to Reproduce the problem?\r\n\r\nRun `nebari upgrade -c nebari-config.yaml` on a file with something like the following present:\r\n\r\n``` yaml\r\n\r\nproject_name: test\r\nprovider: local\r\ndomain: test.example.com\r\nnamespace: dev\r\nnebari_version: 2023.4.1\r\nprofiles:\r\n  jupyterlab:\r\n  - display_name: test\r\n    kubespawner_override:\r\n      image: quay.io/nebari/nebari-jupyterlab:2023.4.1\r\n  - display_name: test-gpu\r\n    kubespawner_override:\r\n      image: quay.io/nebari/nebari-jupyterlab-gpu:2023.4.1\r\n\r\n```\r\n\r\n### Command output\r\n\r\n``` yaml\r\n\r\nproject_name: test\r\nprovider: local\r\ndomain: test.example.com\r\nnamespace: dev\r\nnebari_version: 2023.7.2\r\nprofiles:\r\n  jupyterlab:\r\n  - display_name: test\r\n    kubespawner_override:\r\n      image: quay.io/nebari/nebari-jupyterlab:2023.7.2\r\n  - display_name: test-gpu\r\n    kubespawner_override:\r\n      image: quay.io/nebari/nebari-jupyterlab-gpu:2023.4.1\r\n\r\n```\r\n\r\n### Versions and dependencies used.\r\n\r\nNebari: 2023.7.2.dev\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 70,
    "title": "[BUG] - nebari upgrade doesn't match -gpu images",
    "body": "### Describe the bug\r\n\r\nRunning `nebari upgrade` fails to find/upgrade included gpu images when searching for tags to upgrade to the current version. Ex: `quay.io/nebari/nebari-jupyterlab-gpu:2023.5.1` will get skipped, but `quay.io/nebari/nebari-jupyterlab:2023.5.1` will trigger an upgrade prompt.\r\n\r\n### Expected behavior\r\n\r\nAll [valid nebari image tags](https://quay.io/organization/nebari) should be matched and trigger the associated upgrade behavior.\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nUbuntu Linux, x64\r\n\r\n### How to Reproduce the problem?\r\n\r\nRun `nebari upgrade -c nebari-config.yaml` on a file with something like the following present:\r\n\r\n``` yaml\r\n\r\nproject_name: test\r\nprovider: local\r\ndomain: test.example.com\r\nnamespace: dev\r\nnebari_version: 2023.4.1\r\nprofiles:\r\n  jupyterlab:\r\n  - display_name: test\r\n    kubespawner_override:\r\n      image: quay.io/nebari/nebari-jupyterlab:2023.4.1\r\n  - display_name: test-gpu\r\n    kubespawner_override:\r\n      image: quay.io/nebari/nebari-jupyterlab-gpu:2023.4.1\r\n\r\n```\r\n\r\n### Command output\r\n\r\n``` yaml\r\n\r\nproject_name: test\r\nprovider: local\r\ndomain: test.example.com\r\nnamespace: dev\r\nnebari_version: 2023.7.2\r\nprofiles:\r\n  jupyterlab:\r\n  - display_name: test\r\n    kubespawner_override:\r\n      image: quay.io/nebari/nebari-jupyterlab:2023.7.2\r\n  - display_name: test-gpu\r\n    kubespawner_override:\r\n      image: quay.io/nebari/nebari-jupyterlab-gpu:2023.4.1\r\n\r\n```\r\n\r\n### Versions and dependencies used.\r\n\r\nNebari: 2023.7.2.dev\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug"
    ],
    "predicted_labels": "bug, configuration, testing, dependency-update",
    "model_name": "fine_tuned"
  },
  {
    "index": 71,
    "title": "[EXP] Explore the current associations with the Keycloak's `Users` group",
    "body": "find why do we need it and if we should keep it",
    "actual_labels": [
      "question",
      "maintenance"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 71,
    "title": "[EXP] Explore the current associations with the Keycloak's `Users` group",
    "body": "find why do we need it and if we should keep it",
    "actual_labels": [
      "question",
      "maintenance"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 72,
    "title": "[ENH] - Add ability to navigate back to Nebari after visiting a feature",
    "body": "### Feature description\n\nWe have many features and extensions which allow for a wide variety of functionalities in Nebari. However, in most cases, when a user visits a feature (e.g. Argo, Grafana, or standalone codna-store), they have no way of navigating back to Nebari. \r\n\r\nUsage patterns of many of these features go to new urls within the feature as users are interacting. This means that user's can't effective \"just click back\" in the browser to return to Nebari. \r\n\r\nAs an end user, I usually click on my bookmark to navigate back to the nebari deployment or retype the deployment URL. It would be great to provide users with something like a Nebari logo in the top left corner to allow them to quickly return to the deployment home page. \n\n### Value and/or benefit\n\nThis would make Nebari feel like a single cohesive platform and less like a collection of semi disjointed tools from an end user perspective. \n\n### Anything else?\n\nI personally have felt this for a long time, but then came up as a pain point during the conda-store usability study, so I decided it was time to open an issue :) ",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, case study, new url, bookmark, Nebari deployment, user",
    "model_name": "base"
  },
  {
    "index": 72,
    "title": "[ENH] - Add ability to navigate back to Nebari after visiting a feature",
    "body": "### Feature description\n\nWe have many features and extensions which allow for a wide variety of functionalities in Nebari. However, in most cases, when a user visits a feature (e.g. Argo, Grafana, or standalone codna-store), they have no way of navigating back to Nebari. \r\n\r\nUsage patterns of many of these features go to new urls within the feature as users are interacting. This means that user's can't effective \"just click back\" in the browser to return to Nebari. \r\n\r\nAs an end user, I usually click on my bookmark to navigate back to the nebari deployment or retype the deployment URL. It would be great to provide users with something like a Nebari logo in the top left corner to allow them to quickly return to the deployment home page. \n\n### Value and/or benefit\n\nThis would make Nebari feel like a single cohesive platform and less like a collection of semi disjointed tools from an end user perspective. \n\n### Anything else?\n\nI personally have felt this for a long time, but then came up as a pain point during the conda-store usability study, so I decided it was time to open an issue :) ",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 73,
    "title": "[BUG] NFS not mounting to Dask workers",
    "body": "### Describe the bug\r\n\r\nI'm been sometime since I've tried reading/writing from the NFS while using Dask but this was at one point possible.  \r\n\r\n### Expected behavior\r\n\r\nThe dask-workers have access to the data in the user's home directory.\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nLinux\r\n\r\n### How to Reproduce the problem?\r\n\r\nTry reading a file that exists in home directory while running on a dask worker.\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\nI believe the solution is to convert the `emptyDir` to `persistentVolumeClaim` here:\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/270aad6e6de7c45542099ec6f0bfc5cd3c2e2957/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/dask-gateway/files/gateway_config.py#L211C35-L211C84",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, azure, security, performance, ux/ui, configuration, dependency-update, dask-workers, Nebari, Linux, D",
    "model_name": "base"
  },
  {
    "index": 73,
    "title": "[BUG] NFS not mounting to Dask workers",
    "body": "### Describe the bug\r\n\r\nI'm been sometime since I've tried reading/writing from the NFS while using Dask but this was at one point possible.  \r\n\r\n### Expected behavior\r\n\r\nThe dask-workers have access to the data in the user's home directory.\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nLinux\r\n\r\n### How to Reproduce the problem?\r\n\r\nTry reading a file that exists in home directory while running on a dask worker.\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\nI believe the solution is to convert the `emptyDir` to `persistentVolumeClaim` here:\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/270aad6e6de7c45542099ec6f0bfc5cd3c2e2957/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/dask-gateway/files/gateway_config.py#L211C35-L211C84",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, configuration, dependency-update",
    "model_name": "fine_tuned"
  },
  {
    "index": 74,
    "title": "Deploy dask-gateway using Helm chart",
    "body": "### Context\n\nCurrently, we're writing our [own k8s manifests to deploy dask-gateway](https://github.com/nebari-dev/nebari/tree/843453daf9d440be00ba74402a8607870adcf373/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/dask-gateway). However, there's a Helm chart available: https://github.com/dask/dask-gateway/tree/main/resources/helm/dask-gateway\n\n### Value and/or benefit\n\nThis would simplify the management of dask-gateway, reduce maintenance on our side, and be more consistent with how we're deploying other services withing Nebari\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "maintenance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 74,
    "title": "Deploy dask-gateway using Helm chart",
    "body": "### Context\n\nCurrently, we're writing our [own k8s manifests to deploy dask-gateway](https://github.com/nebari-dev/nebari/tree/843453daf9d440be00ba74402a8607870adcf373/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/dask-gateway). However, there's a Helm chart available: https://github.com/dask/dask-gateway/tree/main/resources/helm/dask-gateway\n\n### Value and/or benefit\n\nThis would simplify the management of dask-gateway, reduce maintenance on our side, and be more consistent with how we're deploying other services withing Nebari\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "maintenance",
      "configuration"
    ],
    "predicted_labels": "enhancement, maintenance, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 75,
    "title": "[ENH] - Update upgrade prompt regarding shared group mounting",
    "body": "### Context\r\n\r\nAfter merging PR [#2593](https://github.com/nebari-dev/nebari/pull/2593), any user groups besides `admin`, `analyst`, and `develop` have been unmounted from users' JupyterLab pods. While no data has been removed, this change breaks compatibility with existing deployments that rely on these custom groups.\r\n\r\n- Include a prompt during the upgrade process to inform users about this change and guide them on how to manage their custom groups.\r\n- Add detailed release notes about this change, allowing users to opt into the new role addition for existing groups.\r\n\r\n### Value and/or benefit\r\n\r\n- backwards compatibility\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "enhancement",
      "documentation",
      "ux/ui",
      "release"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 75,
    "title": "[ENH] - Update upgrade prompt regarding shared group mounting",
    "body": "### Context\r\n\r\nAfter merging PR [#2593](https://github.com/nebari-dev/nebari/pull/2593), any user groups besides `admin`, `analyst`, and `develop` have been unmounted from users' JupyterLab pods. While no data has been removed, this change breaks compatibility with existing deployments that rely on these custom groups.\r\n\r\n- Include a prompt during the upgrade process to inform users about this change and guide them on how to manage their custom groups.\r\n- Add detailed release notes about this change, allowing users to opt into the new role addition for existing groups.\r\n\r\n### Value and/or benefit\r\n\r\n- backwards compatibility\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "enhancement",
      "documentation",
      "ux/ui",
      "release"
    ],
    "predicted_labels": "enhancement, maintenance, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 76,
    "title": "Fix code scanning alert - EKS Clusters should have cluster control plane logging turned on",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/34\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/33\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/32\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/31\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/30\r\n",
    "actual_labels": [
      "bug",
      "security",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, cost,",
    "model_name": "base"
  },
  {
    "index": 76,
    "title": "Fix code scanning alert - EKS Clusters should have cluster control plane logging turned on",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/34\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/33\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/32\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/31\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/30\r\n",
    "actual_labels": [
      "bug",
      "security",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 77,
    "title": "[BUG] - AWS CI/CD deployment may fail due to S3 ACL changes",
    "body": "### Describe the bug\n\nDue to [changes made by AWS](https://github.com/hashicorp/terraform-provider-aws/issues/28353) several months ago regarding S3 ACL's, I've observed deployment pipelines in both Github Actions and Gitlab failing regularly on the initial terraform state bucket stage. I've tried bypassing this with the `--skip-remote-state-provision` switch but this appears to not be working in the develop/extensions baseline.\r\n\r\n\r\n![image](https://github.com/nebari-dev/nebari/assets/70233904/4662cd5b-5140-4979-a461-b232a1fece6a)\r\n\n\n### Expected behavior\n\n1. The 01-terraform-state stage should work in AWS\r\n2. The `--skip-remote-state-provision` should allow bypassing of this stage\n\n### OS and architecture in which you are running Nebari\n\nUbuntu Linux, x64\n\n### How to Reproduce the problem?\n\n1. Run `nebari deploy...` in Github Actions or Gitlab targeting an AWS environment\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\nKubernetes: 1.24\r\nNebari: 2023.7.2.dev\n\n### Compute environment\n\nAWS\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "aws",
      "ci/cd"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 77,
    "title": "[BUG] - AWS CI/CD deployment may fail due to S3 ACL changes",
    "body": "### Describe the bug\n\nDue to [changes made by AWS](https://github.com/hashicorp/terraform-provider-aws/issues/28353) several months ago regarding S3 ACL's, I've observed deployment pipelines in both Github Actions and Gitlab failing regularly on the initial terraform state bucket stage. I've tried bypassing this with the `--skip-remote-state-provision` switch but this appears to not be working in the develop/extensions baseline.\r\n\r\n\r\n![image](https://github.com/nebari-dev/nebari/assets/70233904/4662cd5b-5140-4979-a461-b232a1fece6a)\r\n\n\n### Expected behavior\n\n1. The 01-terraform-state stage should work in AWS\r\n2. The `--skip-remote-state-provision` should allow bypassing of this stage\n\n### OS and architecture in which you are running Nebari\n\nUbuntu Linux, x64\n\n### How to Reproduce the problem?\n\n1. Run `nebari deploy...` in Github Actions or Gitlab targeting an AWS environment\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\nKubernetes: 1.24\r\nNebari: 2023.7.2.dev\n\n### Compute environment\n\nAWS\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "aws",
      "ci/cd"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 78,
    "title": "[BUG] - conda-store UI is broken after upgrading conda-store to 2024.6.1",
    "body": "### Describe the bug\r\n\r\nWhen deploying locally from f2f5a5b67a7e8eccacf1a8b2d77f03a9441ea26b and visiting the conda-store UI, I get the following error:\r\n![image (5)](https://github.com/user-attachments/assets/f2be8207-45ee-4b7c-a540-1ab645452df6)\r\n\r\n\r\n### Expected behavior\r\n\r\nI'd hope to see the conda-store UI\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nmacOS arm64\r\n\r\n### How to Reproduce the problem?\r\n\r\nDeploy Nebari from f2f5a5b67a7e8eccacf1a8b2d77f03a9441ea26b and navigate to /conda-store\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\nThis seems to be a bug with conda-store itself rather than Nebari",
    "actual_labels": [
      "bug",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 78,
    "title": "[BUG] - conda-store UI is broken after upgrading conda-store to 2024.6.1",
    "body": "### Describe the bug\r\n\r\nWhen deploying locally from f2f5a5b67a7e8eccacf1a8b2d77f03a9441ea26b and visiting the conda-store UI, I get the following error:\r\n![image (5)](https://github.com/user-attachments/assets/f2be8207-45ee-4b7c-a540-1ab645452df6)\r\n\r\n\r\n### Expected behavior\r\n\r\nI'd hope to see the conda-store UI\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nmacOS arm64\r\n\r\n### How to Reproduce the problem?\r\n\r\nDeploy Nebari from f2f5a5b67a7e8eccacf1a8b2d77f03a9441ea26b and navigate to /conda-store\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\nThis seems to be a bug with conda-store itself rather than Nebari",
    "actual_labels": [
      "bug",
      "ux/ui"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 79,
    "title": "150cpu/3.4TB RAM user instance: Add user option to change jupyterhub timeout greater than 300 seconds",
    "body": "## Description\r\nWhen spinning up massive user instances on GCP (IE 150 vcpu, 3.4TB memory) it takes longer than 300 seconds for the virtual machine to spin up. This causes the spawner to fail with a time out message. Immediately attempting to spawn again is successful, because the virtual machine has already begun to spin up.\r\n\r\n## Suggestion\r\nAdd an option in qhub-config to increase the time out. Ideally at the nodegroup level.\r\n\r\n## Context\r\nNice to have, not critical.\r\n\r\n## Additional context\r\nHere's how to configure the option for a gigantic [preemptible instance](https://cloud.google.com/compute/docs/instances/preemptible) on GCP\r\n\r\n```\r\ngoogle_cloud_platform:\r\n  project: **private**\r\n  region: us-central1\r\n  zone: us-central1-c\r\n  availability_zones:\r\n    - us-central1-c\r\n  kubernetes_version: \"1.14.10-gke.31\"\r\n  node_groups:\r\n    user-gigantic:\r\n      instance: m1-ultramem-160\r\n      min_nodes: 0\r\n      max_nodes: 5\r\n      preemptible: true\r\nprofiles:\r\n  jupyterlab:\r\n    - display_name: Gigantor Large Instance (~$5/hr)\r\n      description: Stable environment with 158 cpu / 3500 GB ram\r\n      kubespawner_override:\r\n        cpu_limit: 150\r\n        cpu_guarantee: 150\r\n        mem_limit: 3400G\r\n        mem_guarantee: 3400G\r\n        image: \"gcr.io/geoencoding-datu-1571679200174/datum-prod/qhub-jupyterlab:9f85f6ca480b28b451c3702d733318938798b8c2\"\r\n        node_selector:\r\n          \"cloud.google.com/gke-nodepool\": \"user-gigantic\"\r\n```",
    "actual_labels": [
      "enhancement",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 79,
    "title": "150cpu/3.4TB RAM user instance: Add user option to change jupyterhub timeout greater than 300 seconds",
    "body": "## Description\r\nWhen spinning up massive user instances on GCP (IE 150 vcpu, 3.4TB memory) it takes longer than 300 seconds for the virtual machine to spin up. This causes the spawner to fail with a time out message. Immediately attempting to spawn again is successful, because the virtual machine has already begun to spin up.\r\n\r\n## Suggestion\r\nAdd an option in qhub-config to increase the time out. Ideally at the nodegroup level.\r\n\r\n## Context\r\nNice to have, not critical.\r\n\r\n## Additional context\r\nHere's how to configure the option for a gigantic [preemptible instance](https://cloud.google.com/compute/docs/instances/preemptible) on GCP\r\n\r\n```\r\ngoogle_cloud_platform:\r\n  project: **private**\r\n  region: us-central1\r\n  zone: us-central1-c\r\n  availability_zones:\r\n    - us-central1-c\r\n  kubernetes_version: \"1.14.10-gke.31\"\r\n  node_groups:\r\n    user-gigantic:\r\n      instance: m1-ultramem-160\r\n      min_nodes: 0\r\n      max_nodes: 5\r\n      preemptible: true\r\nprofiles:\r\n  jupyterlab:\r\n    - display_name: Gigantor Large Instance (~$5/hr)\r\n      description: Stable environment with 158 cpu / 3500 GB ram\r\n      kubespawner_override:\r\n        cpu_limit: 150\r\n        cpu_guarantee: 150\r\n        mem_limit: 3400G\r\n        mem_guarantee: 3400G\r\n        image: \"gcr.io/geoencoding-datu-1571679200174/datum-prod/qhub-jupyterlab:9f85f6ca480b28b451c3702d733318938798b8c2\"\r\n        node_selector:\r\n          \"cloud.google.com/gke-nodepool\": \"user-gigantic\"\r\n```",
    "actual_labels": [
      "enhancement",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 80,
    "title": "More jupyterlab extensions (nbgitpuller)",
    "body": "## Description\r\n\r\nFor a class I give, I'd like to be able to use [nbgitpuller](https://jupyterhub.github.io/nbgitpuller/link.html) with my qhub deployment on GCP.\r\n\r\nTo do so, I need to modify files under the jupyterlab folder:\r\n- Add ``nbgitpuller`` to ``jupyterlab/labextensions.txt``\r\n- Add pip install of ``nbgitpuller`` to ``jupyterlab/environment.yaml``\r\n\r\n## Suggestion\r\nWould it be possible to implement in the ``config-hub.yml`` file a new section for more jupyterlab extensions ?\r\n\r\nAlternatively, it may be faster to simply add the ``nbgitpuller`` extension to the default qhub set-up ? (I could send a PR for this)\r\n",
    "actual_labels": [
      "enhancement",
      "configuration",
      "gcp"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 80,
    "title": "More jupyterlab extensions (nbgitpuller)",
    "body": "## Description\r\n\r\nFor a class I give, I'd like to be able to use [nbgitpuller](https://jupyterhub.github.io/nbgitpuller/link.html) with my qhub deployment on GCP.\r\n\r\nTo do so, I need to modify files under the jupyterlab folder:\r\n- Add ``nbgitpuller`` to ``jupyterlab/labextensions.txt``\r\n- Add pip install of ``nbgitpuller`` to ``jupyterlab/environment.yaml``\r\n\r\n## Suggestion\r\nWould it be possible to implement in the ``config-hub.yml`` file a new section for more jupyterlab extensions ?\r\n\r\nAlternatively, it may be faster to simply add the ``nbgitpuller`` extension to the default qhub set-up ? (I could send a PR for this)\r\n",
    "actual_labels": [
      "enhancement",
      "configuration",
      "gcp"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 81,
    "title": "Docs not built properly, not showing all pages?",
    "body": "It seems all the pages are not visible on qhub.dev for example the following:\r\n\r\n- https://github.com/Quansight/qhub/blob/main/docs/source/08_integrations/\r\n- https://github.com/Quansight/qhub/blob/main/docs/source/04_how_to_guides/7_qhub_gpu.md",
    "actual_labels": [
      "bug",
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 81,
    "title": "Docs not built properly, not showing all pages?",
    "body": "It seems all the pages are not visible on qhub.dev for example the following:\r\n\r\n- https://github.com/Quansight/qhub/blob/main/docs/source/08_integrations/\r\n- https://github.com/Quansight/qhub/blob/main/docs/source/04_how_to_guides/7_qhub_gpu.md",
    "actual_labels": [
      "bug",
      "documentation"
    ],
    "predicted_labels": "documentation, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 82,
    "title": "[DOC] - Update `CONTRIBUTING.md` to point to docs",
    "body": "It'll be nice to have one source of truth for community/contributing docs that we can keep up-to-date.\r\n\r\nWe can update the `CONTRIBUTING.md` file to point to https://www.nebari.dev/docs/community",
    "actual_labels": [
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 82,
    "title": "[DOC] - Update `CONTRIBUTING.md` to point to docs",
    "body": "It'll be nice to have one source of truth for community/contributing docs that we can keep up-to-date.\r\n\r\nWe can update the `CONTRIBUTING.md` file to point to https://www.nebari.dev/docs/community",
    "actual_labels": [
      "documentation"
    ],
    "predicted_labels": "documentation, maintenance, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 83,
    "title": "[BUG] - Local install fails to install kind and kubectl providers",
    "body": "### Describe the bug\n\nKind and docker providers are not available through the opentofu registry so a local install fails.\n\n### Expected behavior\n\nlocal install succeeds\n\n### OS and architecture in which you are running Nebari\n\nLinux x86_64\n\n### How to Reproduce the problem?\n\ncreate a new local deploy.\n\n### Command output\n\n```bash session\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Failed to install provider\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 Error while installing gavinbunney/kubectl v1.14.0: github.com: Get\r\n[terraform]: \u2502 \"https://objects.githubusercontent.com/github-production-release-asset-2e65be/204515734/b6085ad2-7e79-41d3-90f0-2a0786d99e34?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241113%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241113T123909Z&X-Amz-Expires=300&X-Amz-Signature=d7dce317083892577639aa02cc2e830dd46fc08ba20212598684313623f81cc3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dterraform-provider-kubectl_1.14.0_linux_amd64.zip&response-content-type=application%2Foctet-stream\":\r\n[terraform]: \u2502 net/http: TLS handshake timeout\r\n[terraform]: \u2575\r\n[terraform]: \r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Failed to install provider\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 Error while installing tehcyx/kind v0.4.0: github.com: Get\r\n[terraform]: \u2502 \"https://objects.githubusercontent.com/github-production-release-asset-2e65be/214505084/e512d818-3efd-4261-b68c-0b329f39a60e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241113%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241113T123912Z&X-Amz-Expires=300&X-Amz-Signature=f76a497b4fe8357a879f2ae5182fc7a855a1bf7dac2b1b5853b3ec0cbfef3c2e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dterraform-provider-kind_0.4.0_linux_amd64.zip&response-content-type=application%2Foctet-stream\":\r\n[terraform]: \u2502 net/http: TLS handshake timeout\r\n[terraform]: \u2575\n```\n\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\n```",
    "model_name": "base"
  },
  {
    "index": 83,
    "title": "[BUG] - Local install fails to install kind and kubectl providers",
    "body": "### Describe the bug\n\nKind and docker providers are not available through the opentofu registry so a local install fails.\n\n### Expected behavior\n\nlocal install succeeds\n\n### OS and architecture in which you are running Nebari\n\nLinux x86_64\n\n### How to Reproduce the problem?\n\ncreate a new local deploy.\n\n### Command output\n\n```bash session\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Failed to install provider\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 Error while installing gavinbunney/kubectl v1.14.0: github.com: Get\r\n[terraform]: \u2502 \"https://objects.githubusercontent.com/github-production-release-asset-2e65be/204515734/b6085ad2-7e79-41d3-90f0-2a0786d99e34?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241113%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241113T123909Z&X-Amz-Expires=300&X-Amz-Signature=d7dce317083892577639aa02cc2e830dd46fc08ba20212598684313623f81cc3&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dterraform-provider-kubectl_1.14.0_linux_amd64.zip&response-content-type=application%2Foctet-stream\":\r\n[terraform]: \u2502 net/http: TLS handshake timeout\r\n[terraform]: \u2575\r\n[terraform]: \r\n[terraform]: \u2577\r\n[terraform]: \u2502 Error: Failed to install provider\r\n[terraform]: \u2502 \r\n[terraform]: \u2502 Error while installing tehcyx/kind v0.4.0: github.com: Get\r\n[terraform]: \u2502 \"https://objects.githubusercontent.com/github-production-release-asset-2e65be/214505084/e512d818-3efd-4261-b68c-0b329f39a60e?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20241113%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20241113T123912Z&X-Amz-Expires=300&X-Amz-Signature=f76a497b4fe8357a879f2ae5182fc7a855a1bf7dac2b1b5853b3ec0cbfef3c2e&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dterraform-provider-kind_0.4.0_linux_amd64.zip&response-content-type=application%2Foctet-stream\":\r\n[terraform]: \u2502 net/http: TLS handshake timeout\r\n[terraform]: \u2575\n```\n\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 84,
    "title": "[ENH] - Add a static type checker?",
    "body": "### Feature description\n\nAdd a static type checker like [`mypy`](https://mypy-lang.org/) to our lint workflows. Note that this doesn't mean that we have to declare support for [PEP561](https://peps.python.org/pep-0561/). I mostly want to make sure that the type hints that we have in our source are correct.\n\n### Value and/or benefit\n\nFor me personally the following holds:\r\n\r\ncorrect type hints > no type hints > wrong type hints\r\n\r\nI'm aware that type hints are not everyone's cup of tea and they prefer no type hints, but I don't think anyone can argue that wrong type hints aren't detrimental for everyone. \r\n\r\nWe currently have type hints in a few places, but no way to enforce their correctness. One example of wrong type hints is\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/8bd56f90b84c7686f45459ae8f7cf690721bd60a/nebari/render.py#L229-L231\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/8bd56f90b84c7686f45459ae8f7cf690721bd60a/nebari/render.py#L77-L81\r\n\r\nThe type hint has to be `List[str]` in both cases.\r\n\r\nThus, if we want to use type hints, and I'm all for that, we should also have a checker that enforces their correctness.\n\n### Anything else?\n\nIf we decide that we don't want to have a type checker, we should also purge all type hints from the repository and enforce that no new ones are added. Sounds harsh, but I don't think there is another option to avoid wrong ones.",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "maintenance"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 84,
    "title": "[ENH] - Add a static type checker?",
    "body": "### Feature description\n\nAdd a static type checker like [`mypy`](https://mypy-lang.org/) to our lint workflows. Note that this doesn't mean that we have to declare support for [PEP561](https://peps.python.org/pep-0561/). I mostly want to make sure that the type hints that we have in our source are correct.\n\n### Value and/or benefit\n\nFor me personally the following holds:\r\n\r\ncorrect type hints > no type hints > wrong type hints\r\n\r\nI'm aware that type hints are not everyone's cup of tea and they prefer no type hints, but I don't think anyone can argue that wrong type hints aren't detrimental for everyone. \r\n\r\nWe currently have type hints in a few places, but no way to enforce their correctness. One example of wrong type hints is\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/8bd56f90b84c7686f45459ae8f7cf690721bd60a/nebari/render.py#L229-L231\r\n\r\nhttps://github.com/nebari-dev/nebari/blob/8bd56f90b84c7686f45459ae8f7cf690721bd60a/nebari/render.py#L77-L81\r\n\r\nThe type hint has to be `List[str]` in both cases.\r\n\r\nThus, if we want to use type hints, and I'm all for that, we should also have a checker that enforces their correctness.\n\n### Anything else?\n\nIf we decide that we don't want to have a type checker, we should also purge all type hints from the repository and enforce that no new ones are added. Sounds harsh, but I don't think there is another option to avoid wrong ones.",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "maintenance"
    ],
    "predicted_labels": "enhancement, ux/ui, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 85,
    "title": "[bug] Unable to Deploy to AWS",
    "body": "## Describe the bug\r\nI'm getting the following error: \r\n\r\n`error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.`\r\n\r\n## Reproducible Example:\r\n\r\n```\r\nexport GITHUB_USERNAME=\u201c\u201d\r\nexport GITHUB_TOKEN=\u201c\u201d\r\nexport AWS_ACCESS_KEY_ID=\u201c\u201d\r\nexport AWS_SECRET_ACCESS_KEY=\u201c\u201d\r\nexport CLOUDFLARE_TOKEN=\u201c\u201d\r\n\r\nqhub init aws\r\n\r\nqhub deploy -c qhub-config.yaml --dns-provider cloudflare --dns-auto-provision\r\n```\r\n\r\n## Full Error:\r\n\r\n```\r\nINFO:qhub.provider.terraform:terraform init took 21.004 [s]\r\nINFO:qhub.provider.terraform:rm local state file terraform.tfstate directory=terraform-state\r\nINFO:qhub.provider.terraform:terraform import directory=terraform-state addr=module.terraform-state.aws_s3_bucket.terraform-state id=daskhub-dev-terraform-state\r\nINFO:qhub.provider.terraform: terraform at /var/folders/wf/_b6z5_fx7gb71f96vy026vrr0000gn/T/terraform/0.14.9/terraform\r\n[terraform]: \r\n[terraform]: Error: error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\r\n[terraform]: \tstatus code: 403, request id: a28c0f32-1283-4e05-a0a1-0da3df0fa985\r\n[terraform]: \r\n[terraform]:   on /Users/rhun/PycharmProjects/qhub-test/terraform-state/main.tf line 1, in provider \"aws\":\r\n[terraform]:    1: provider \"aws\" {\r\n[terraform]: \r\n[terraform]: \r\nINFO:qhub.provider.terraform:terraform apply directory=terraform-state targets=[]\r\nINFO:qhub.provider.terraform: terraform at /var/folders/wf/_b6z5_fx7gb71f96vy026vrr0000gn/T/terraform/0.14.9/terraform\r\n[terraform]: \r\n[terraform]: Error: error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\r\n[terraform]: \tstatus code: 403, request id: 29ddaf82-5d11-42dc-b967-574896af8b7b\r\n[terraform]: \r\n[terraform]:   on main.tf line 1, in provider \"aws\":\r\n[terraform]:    1: provider \"aws\" {\r\n[terraform]: \r\n[terraform]: \r\n\r\nProblem encountered: Terraform error\r\n```\r\n",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\n```",
    "model_name": "base"
  },
  {
    "index": 85,
    "title": "[bug] Unable to Deploy to AWS",
    "body": "## Describe the bug\r\nI'm getting the following error: \r\n\r\n`error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.`\r\n\r\n## Reproducible Example:\r\n\r\n```\r\nexport GITHUB_USERNAME=\u201c\u201d\r\nexport GITHUB_TOKEN=\u201c\u201d\r\nexport AWS_ACCESS_KEY_ID=\u201c\u201d\r\nexport AWS_SECRET_ACCESS_KEY=\u201c\u201d\r\nexport CLOUDFLARE_TOKEN=\u201c\u201d\r\n\r\nqhub init aws\r\n\r\nqhub deploy -c qhub-config.yaml --dns-provider cloudflare --dns-auto-provision\r\n```\r\n\r\n## Full Error:\r\n\r\n```\r\nINFO:qhub.provider.terraform:terraform init took 21.004 [s]\r\nINFO:qhub.provider.terraform:rm local state file terraform.tfstate directory=terraform-state\r\nINFO:qhub.provider.terraform:terraform import directory=terraform-state addr=module.terraform-state.aws_s3_bucket.terraform-state id=daskhub-dev-terraform-state\r\nINFO:qhub.provider.terraform: terraform at /var/folders/wf/_b6z5_fx7gb71f96vy026vrr0000gn/T/terraform/0.14.9/terraform\r\n[terraform]: \r\n[terraform]: Error: error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\r\n[terraform]: \tstatus code: 403, request id: a28c0f32-1283-4e05-a0a1-0da3df0fa985\r\n[terraform]: \r\n[terraform]:   on /Users/rhun/PycharmProjects/qhub-test/terraform-state/main.tf line 1, in provider \"aws\":\r\n[terraform]:    1: provider \"aws\" {\r\n[terraform]: \r\n[terraform]: \r\nINFO:qhub.provider.terraform:terraform apply directory=terraform-state targets=[]\r\nINFO:qhub.provider.terraform: terraform at /var/folders/wf/_b6z5_fx7gb71f96vy026vrr0000gn/T/terraform/0.14.9/terraform\r\n[terraform]: \r\n[terraform]: Error: error configuring Terraform AWS Provider: error validating provider credentials: error calling sts:GetCallerIdentity: InvalidClientTokenId: The security token included in the request is invalid.\r\n[terraform]: \tstatus code: 403, request id: 29ddaf82-5d11-42dc-b967-574896af8b7b\r\n[terraform]: \r\n[terraform]:   on main.tf line 1, in provider \"aws\":\r\n[terraform]:    1: provider \"aws\" {\r\n[terraform]: \r\n[terraform]: \r\n\r\nProblem encountered: Terraform error\r\n```\r\n",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 86,
    "title": "[ENH] - Investigate cert-manager",
    "body": "### Feature description\n\nWe can look into adding cert-manager: https://cert-manager.io\n\n### Value and/or benefit\n\nIt'll allow users to get certs from various providers (besides, let's encrypt).\n\n### Anything else?\n\n(Originally proposed by @costrouc)",
    "actual_labels": [
      "enhancement",
      "security"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 86,
    "title": "[ENH] - Investigate cert-manager",
    "body": "### Feature description\n\nWe can look into adding cert-manager: https://cert-manager.io\n\n### Value and/or benefit\n\nIt'll allow users to get certs from various providers (besides, let's encrypt).\n\n### Anything else?\n\n(Originally proposed by @costrouc)",
    "actual_labels": [
      "enhancement",
      "security"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 87,
    "title": "META - Nebari integrations stability",
    "body": "## Description\r\n\r\nMeta issue used to track integrations work or issues within Nebari\r\n\r\n## Issues related to integrations\r\n\r\n```[tasklist]\r\n### Clearml\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1217\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1281\r\n- [x] https://github.com/nebari-dev/nebari/issues/932\r\n```\r\n\r\n```[tasklist]\r\n### Grafana\r\n- [x] https://github.com/nebari-dev/nebari/issues/1481\r\n- [x] https://github.com/nebari-dev/nebari/issues/1127\r\n- [x] https://github.com/nebari-dev/nebari/issues/757\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1120\r\n```\r\n\r\n```[tasklist]\r\n### Dask\r\n- [x] https://github.com/nebari-dev/nebari/issues/1388\r\n- [ ] https://github.com/nebari-dev/nebari/issues/979\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1294\r\n```\r\n\r\n```[tasklist]\r\n### Conda store\r\n- [x] https://github.com/nebari-dev/nebari/issues/1594\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1507\r\n- [x] https://github.com/nebari-dev/nebari/issues/1412\r\n- [x] https://github.com/nebari-dev/nebari/issues/1121\r\n- [x] https://github.com/nebari-dev/nebari/issues/1158\r\n```\r\n\r\n```[tasklist]\r\n### Other\r\n\r\n```\r\n\r\n",
    "actual_labels": [
      "enhancement"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 87,
    "title": "META - Nebari integrations stability",
    "body": "## Description\r\n\r\nMeta issue used to track integrations work or issues within Nebari\r\n\r\n## Issues related to integrations\r\n\r\n```[tasklist]\r\n### Clearml\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1217\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1281\r\n- [x] https://github.com/nebari-dev/nebari/issues/932\r\n```\r\n\r\n```[tasklist]\r\n### Grafana\r\n- [x] https://github.com/nebari-dev/nebari/issues/1481\r\n- [x] https://github.com/nebari-dev/nebari/issues/1127\r\n- [x] https://github.com/nebari-dev/nebari/issues/757\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1120\r\n```\r\n\r\n```[tasklist]\r\n### Dask\r\n- [x] https://github.com/nebari-dev/nebari/issues/1388\r\n- [ ] https://github.com/nebari-dev/nebari/issues/979\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1294\r\n```\r\n\r\n```[tasklist]\r\n### Conda store\r\n- [x] https://github.com/nebari-dev/nebari/issues/1594\r\n- [ ] https://github.com/nebari-dev/nebari/issues/1507\r\n- [x] https://github.com/nebari-dev/nebari/issues/1412\r\n- [x] https://github.com/nebari-dev/nebari/issues/1121\r\n- [x] https://github.com/nebari-dev/nebari/issues/1158\r\n```\r\n\r\n```[tasklist]\r\n### Other\r\n\r\n```\r\n\r\n",
    "actual_labels": [
      "enhancement"
    ],
    "predicted_labels": "bug, configuration, gcp, dependency-update, dask, gcp, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 88,
    "title": "Unable to change user InitContainers configured when launching JupyterLab",
    "body": "### Context\n\nI am unable to customize the InitContainers run when a user launches their JupyterLab instance. This is a problem because we are running into the docker rate limit in our network, and cannot change the source of the busybox:1.31 image being used by said InitContainers.\n\nThe InitContainers are currently hardcoded within Nebari, specifically in the [Jupyterhub configuration here](https://github.com/nebari-dev/nebari/blob/main/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/jupyterhub/files/jupyterhub/03-profiles.py#L64). We cannot update the image source through the Kubernetes API, as the InitContainers are set by the linked python script when a user launches their Jupyterlab instance. We cannot pre-stage the image on our nodes because the launch_template configuration is unavailable. We cannot override the InitContainer images using kubespawner_overrides configuration, as it doesn't merge specified init_containers with the python-defined ones, resulting in duplicate InitContainer names that break the launch.\n\nAny advice on how to update the image source of the JupyterLab InitContainers would be greatly appreciated. We run Nebari on a shared internal network, so we hit the docker rate limit very commonly with our limited number of NAT IP addresses. Thanks!\n\n### Value and/or benefit\n\nHaving the ability to customize where these containers are pulled from would allow us to source images from trusted, and non rate-limited, sources.\n\n### Anything else?\n\nThe ability to use Nebari's [launch_templates](https://www.nebari.dev/docs/explanations/provider-configuration/#launch-templates-optional) configuration within AWS would allow us to mitigate the issue, as we could stage AMIs with images already installed, or download the images from other locations using the pre_bootstrap_script. However, it appears the launch_template configuration is not supported, as including it in the config results in the following error:\n\n```bash\n$ nebari validate --config nebari-dev-config.yml\nERROR validating configuration \nnebari-dev-config.yml\n1 validation error for ConfigSchema\namazon_web_services.node_groups.user\n  Value error, The 'launch_template' field is currently unavailable and has been\nremoved from the configuration schema.\nPlease omit this field until it is reintroduced in a future update. \n    For further information visit https://errors.pydantic.dev/2.11/v/value_error\nAborted.\n```",
    "actual_labels": [
      "bug",
      "enhancement",
      "question",
      "configuration",
      "aws"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 88,
    "title": "Unable to change user InitContainers configured when launching JupyterLab",
    "body": "### Context\n\nI am unable to customize the InitContainers run when a user launches their JupyterLab instance. This is a problem because we are running into the docker rate limit in our network, and cannot change the source of the busybox:1.31 image being used by said InitContainers.\n\nThe InitContainers are currently hardcoded within Nebari, specifically in the [Jupyterhub configuration here](https://github.com/nebari-dev/nebari/blob/main/src/_nebari/stages/kubernetes_services/template/modules/kubernetes/services/jupyterhub/files/jupyterhub/03-profiles.py#L64). We cannot update the image source through the Kubernetes API, as the InitContainers are set by the linked python script when a user launches their Jupyterlab instance. We cannot pre-stage the image on our nodes because the launch_template configuration is unavailable. We cannot override the InitContainer images using kubespawner_overrides configuration, as it doesn't merge specified init_containers with the python-defined ones, resulting in duplicate InitContainer names that break the launch.\n\nAny advice on how to update the image source of the JupyterLab InitContainers would be greatly appreciated. We run Nebari on a shared internal network, so we hit the docker rate limit very commonly with our limited number of NAT IP addresses. Thanks!\n\n### Value and/or benefit\n\nHaving the ability to customize where these containers are pulled from would allow us to source images from trusted, and non rate-limited, sources.\n\n### Anything else?\n\nThe ability to use Nebari's [launch_templates](https://www.nebari.dev/docs/explanations/provider-configuration/#launch-templates-optional) configuration within AWS would allow us to mitigate the issue, as we could stage AMIs with images already installed, or download the images from other locations using the pre_bootstrap_script. However, it appears the launch_template configuration is not supported, as including it in the config results in the following error:\n\n```bash\n$ nebari validate --config nebari-dev-config.yml\nERROR validating configuration \nnebari-dev-config.yml\n1 validation error for ConfigSchema\namazon_web_services.node_groups.user\n  Value error, The 'launch_template' field is currently unavailable and has been\nremoved from the configuration schema.\nPlease omit this field until it is reintroduced in a future update. \n    For further information visit https://errors.pydantic.dev/2.11/v/value_error\nAborted.\n```",
    "actual_labels": [
      "bug",
      "enhancement",
      "question",
      "configuration",
      "aws"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 89,
    "title": "[BUG] - Azure NoRegisteredProviderFound Error",
    "body": "### Describe the bug\n\nAzurerm was throwing the following error on deployment in eastus2 region when updating a cluster from 2024.1.1 to 2024.3.3.\r\n\r\n```\r\nError: retrieving Cluster: (Managed Cluster Name \"nebari-dev\" / Resource Group \"my-resourcegroup\"): \r\ncontainerservice.ManagedClustersClient#Get: Failure respond ing to request: StatusCode=400 -- \r\nOriginal Error: autorest/azure: Service returned an error. Status=400 Code=\"NoRegisteredProviderFound\" \r\nMessage=\"No registered resource provider found for location 'eastus2' and API version '2022-03-02-preview' for type 'managedClusters'.\r\n```\r\n\r\nUpdating azurerm provider fixed the issue.  Fixed by https://github.com/nebari-dev/nebari/pull/2370\r\n\n\n### Expected behavior\n\nno error\n\n### OS and architecture in which you are running Nebari\n\nLinux 2023.3.3\n\n### How to Reproduce the problem?\n\nSee above\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "azure",
      "configuration",
      "dependency-update"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 89,
    "title": "[BUG] - Azure NoRegisteredProviderFound Error",
    "body": "### Describe the bug\n\nAzurerm was throwing the following error on deployment in eastus2 region when updating a cluster from 2024.1.1 to 2024.3.3.\r\n\r\n```\r\nError: retrieving Cluster: (Managed Cluster Name \"nebari-dev\" / Resource Group \"my-resourcegroup\"): \r\ncontainerservice.ManagedClustersClient#Get: Failure respond ing to request: StatusCode=400 -- \r\nOriginal Error: autorest/azure: Service returned an error. Status=400 Code=\"NoRegisteredProviderFound\" \r\nMessage=\"No registered resource provider found for location 'eastus2' and API version '2022-03-02-preview' for type 'managedClusters'.\r\n```\r\n\r\nUpdating azurerm provider fixed the issue.  Fixed by https://github.com/nebari-dev/nebari/pull/2370\r\n\n\n### Expected behavior\n\nno error\n\n### OS and architecture in which you are running Nebari\n\nLinux 2023.3.3\n\n### How to Reproduce the problem?\n\nSee above\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "azure",
      "configuration",
      "dependency-update"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 90,
    "title": "Add static code analysis to CI",
    "body": "### Context\n\nScan code for vulnerabilities\n\n### Value and/or benefit\n\nSecurity.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "security"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 90,
    "title": "Add static code analysis to CI",
    "body": "### Context\n\nScan code for vulnerabilities\n\n### Value and/or benefit\n\nSecurity.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "security"
    ],
    "predicted_labels": "enhancement, security, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 91,
    "title": "Switch to git-flow",
    "body": "https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "release"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 91,
    "title": "Switch to git-flow",
    "body": "https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "release"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 92,
    "title": "[ENH] - SBOM Generation ",
    "body": "### Feature description\n\nGenerate a software-bill-of-materials (SBOM) supply-chain security artifact in CI.\n\n### Value and/or benefit\n\nImprove supply-chain security by generating a SBOM artifact in CI.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "security"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 92,
    "title": "[ENH] - SBOM Generation ",
    "body": "### Feature description\n\nGenerate a software-bill-of-materials (SBOM) supply-chain security artifact in CI.\n\n### Value and/or benefit\n\nImprove supply-chain security by generating a SBOM artifact in CI.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "security"
    ],
    "predicted_labels": "enhancement, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 93,
    "title": "Conda-store pods get evicted on AWS",
    "body": "## Describe the bug\r\nSeems like conda-store pods get evicted on AWS on a fresh deployment. This is tested with latest main commit: https://github.com/Quansight/qhub/commit/e7992115abfe65fd429999d5a4241e4863b2a85d\r\n\r\nDescribe on the pod:\r\n\r\n```\r\n\u2502 Events:                                                                                                                                                                                                                                    \u2502\r\n\u2502   Type     Reason            Age                From                Message                                                                                                                                                                \u2502\r\n\u2502   ----     ------            ----               ----                -------                                                                                                                                                                \u2502\r\n\u2502   Warning  FailedScheduling  51s (x2 over 52s)  default-scheduler   0/3 nodes are available: 1 node(s) had taint {node.kubernetes.io/disk-pressure: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.           \u2502\r\n\u2502   Normal   TriggeredScaleUp  41s                cluster-autoscaler  pod triggered scale-up: [{eks-20bd5579-b270-ddc9-c256-f021f1d7978b 1->2 (max: 5)}]                                                                                     \u2502\r\n\u2502   Warning  FailedScheduling  6s (x2 over 6s)    default-scheduler   0/4 nodes are available: 1 node(s) had taint {node.kubernetes.io/disk-pressure: }, that the pod didn't tolerate, 1 node(s) had taint {node.kubernetes.io/not-ready: }, \u2502\r\n\u2502  that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.                                                                                                                                                                 \u2502\r\n\u2502\r\n```\r\n",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 93,
    "title": "Conda-store pods get evicted on AWS",
    "body": "## Describe the bug\r\nSeems like conda-store pods get evicted on AWS on a fresh deployment. This is tested with latest main commit: https://github.com/Quansight/qhub/commit/e7992115abfe65fd429999d5a4241e4863b2a85d\r\n\r\nDescribe on the pod:\r\n\r\n```\r\n\u2502 Events:                                                                                                                                                                                                                                    \u2502\r\n\u2502   Type     Reason            Age                From                Message                                                                                                                                                                \u2502\r\n\u2502   ----     ------            ----               ----                -------                                                                                                                                                                \u2502\r\n\u2502   Warning  FailedScheduling  51s (x2 over 52s)  default-scheduler   0/3 nodes are available: 1 node(s) had taint {node.kubernetes.io/disk-pressure: }, that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.           \u2502\r\n\u2502   Normal   TriggeredScaleUp  41s                cluster-autoscaler  pod triggered scale-up: [{eks-20bd5579-b270-ddc9-c256-f021f1d7978b 1->2 (max: 5)}]                                                                                     \u2502\r\n\u2502   Warning  FailedScheduling  6s (x2 over 6s)    default-scheduler   0/4 nodes are available: 1 node(s) had taint {node.kubernetes.io/disk-pressure: }, that the pod didn't tolerate, 1 node(s) had taint {node.kubernetes.io/not-ready: }, \u2502\r\n\u2502  that the pod didn't tolerate, 2 node(s) didn't match Pod's node affinity.                                                                                                                                                                 \u2502\r\n\u2502\r\n```\r\n",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 94,
    "title": "[BUG] - keycloak root password not working",
    "body": "### OS system and architecture in which you are running Nebari\n\nunbuntu on aws\n\n### Expected behavior\n\ni see the root login and password, and went to https://<root-dns>/auth/admin and type in root + the 1-time password and get rejected.... it took me a couple of time to get the install script to work = is there a chance i have the wrong password and if so how to i reset it?\n\n### Actual behavior\n\nnot logging in\n\n### How to Reproduce the problem?\n\nvanilla aws ubuntu install\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "question",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 94,
    "title": "[BUG] - keycloak root password not working",
    "body": "### OS system and architecture in which you are running Nebari\n\nunbuntu on aws\n\n### Expected behavior\n\ni see the root login and password, and went to https://<root-dns>/auth/admin and type in root + the 1-time password and get rejected.... it took me a couple of time to get the install script to work = is there a chance i have the wrong password and if so how to i reset it?\n\n### Actual behavior\n\nnot logging in\n\n### How to Reproduce the problem?\n\nvanilla aws ubuntu install\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "question",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 95,
    "title": "Rename `qhub-jupyterhub-theme` -> `nebari-jupyterhub-theme` and update colors+logo",
    "body": "Source repo - https://github.com/Quansight/qhub-jupyterhub-theme\r\n\r\nShould align with nebari/design",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 95,
    "title": "Rename `qhub-jupyterhub-theme` -> `nebari-jupyterhub-theme` and update colors+logo",
    "body": "Source repo - https://github.com/Quansight/qhub-jupyterhub-theme\r\n\r\nShould align with nebari/design",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 96,
    "title": "[BUG] - Enum class name showing up in guided init final command instead of value name",
    "body": "### Describe the bug\n\nAfter running `nebari init --guided-init`, there's a message at the end showing what the `init` command would look like if the answers from the guided init were converted into it. For example:\r\n\r\n```\r\nFor reference, if the previous Guided Init answers were converted into a direct\r\nnebari init command, it would be:\r\n\r\n        nebari init ProviderEnum.local --project-name nebari --namespace dev\r\n--auth-provider AuthenticationEnum.password --ci-provider CiEnum.none\r\n--terraform-state TerraformStateEnum.remote\r\n```\r\n\r\nHowever, you can see that the command is not being properly rendered, as it is showing the `Enum` subclasses for some of the inputs rather than their actual values.\n\n### Expected behavior\n\nI'd expect the proper values of the inputs to be showing up in the direct `nebari init` command. Taking the example from above, I'd expect the output command to look like:\r\n\r\n```\r\nnebari init local --project-name nebari --namespace dev\r\n--auth-provider password --ci-provider none\r\n--terraform-state remote\r\n```\n\n### OS and architecture in which you are running Nebari\n\nmacOS arm64\n\n### How to Reproduce the problem?\n\nRun `nebari init --guided-init` and choose any option/pass any values for the prompts. At the end, it will render the direct `init` command and it will have the Enum class names on it.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 96,
    "title": "[BUG] - Enum class name showing up in guided init final command instead of value name",
    "body": "### Describe the bug\n\nAfter running `nebari init --guided-init`, there's a message at the end showing what the `init` command would look like if the answers from the guided init were converted into it. For example:\r\n\r\n```\r\nFor reference, if the previous Guided Init answers were converted into a direct\r\nnebari init command, it would be:\r\n\r\n        nebari init ProviderEnum.local --project-name nebari --namespace dev\r\n--auth-provider AuthenticationEnum.password --ci-provider CiEnum.none\r\n--terraform-state TerraformStateEnum.remote\r\n```\r\n\r\nHowever, you can see that the command is not being properly rendered, as it is showing the `Enum` subclasses for some of the inputs rather than their actual values.\n\n### Expected behavior\n\nI'd expect the proper values of the inputs to be showing up in the direct `nebari init` command. Taking the example from above, I'd expect the output command to look like:\r\n\r\n```\r\nnebari init local --project-name nebari --namespace dev\r\n--auth-provider password --ci-provider none\r\n--terraform-state remote\r\n```\n\n### OS and architecture in which you are running Nebari\n\nmacOS arm64\n\n### How to Reproduce the problem?\n\nRun `nebari init --guided-init` and choose any option/pass any values for the prompts. At the end, it will render the direct `init` command and it will have the Enum class names on it.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "ux/ui"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 97,
    "title": "[DOC] - Improve contribution docs",
    "body": "Our docs need some TLC for the development docs:\r\n\r\n- [x] #1079\r\n- [x] #1078",
    "actual_labels": [
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 97,
    "title": "[DOC] - Improve contribution docs",
    "body": "Our docs need some TLC for the development docs:\r\n\r\n- [x] #1079\r\n- [x] #1078",
    "actual_labels": [
      "documentation"
    ],
    "predicted_labels": "documentation, maintenance, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 98,
    "title": "Solve CVE-2024-26130 via upgrade to auth0-python==4.7.1",
    "body": "### Context\n\nNebari currently has a dependency on `cryptography==41.0.7` which is vulnerable to CVE-2024-26130.\r\nDue to the dependency tree for that package, upgrading to `auth0-python==4.7.1` will have a dependency side-effect of upgrading to `cryptography==42.0.5` and resolves the CVE.\r\n\r\nI propose the following one-line change:\r\nhttps://github.com/nebari-dev/nebari/blob/develop/pyproject.toml#L58\r\n```\r\n dependencies = [\r\n-    \"auth0-python==4.4.2\",\r\n+    \"auth0-python==4.7.1\",\r\n```\n\n### Value and/or benefit\n\nResolves CVE-2024-26130\n\n### Anything else?\n\nReverse dependency tree, before upgrade:\r\n```\r\n$ pipdeptree --reverse --packages cryptography\r\ncryptography==41.0.7\r\n\u251c\u2500\u2500 auth0-python==4.4.2 [requires: cryptography>=41.0.3,<42.0.0]\r\n\u251c\u2500\u2500 azure-identity==1.12.0 [requires: cryptography>=2.5]\r\n\u251c\u2500\u2500 msal==1.27.0 [requires: cryptography>=0.6,<45]\r\n\u2514\u2500\u2500 pyOpenSSL==23.3.0 [requires: cryptography>=41.0.5,<42]\r\n```\r\n\r\nReverse dependency tree, after upgrade:\r\n```\r\n$ pipdeptree --reverse --packages cryptography\r\ncryptography==42.0.5\r\n\u251c\u2500\u2500 auth0-python==4.7.1 [requires: cryptography>=42.0.4,<43.0.0]\r\n\u251c\u2500\u2500 azure-identity==1.12.0 [requires: cryptography>=2.5]\r\n\u2514\u2500\u2500 msal==1.27.0 [requires: cryptography>=0.6,<45]\r\n```",
    "actual_labels": [
      "security",
      "dependency-update"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 98,
    "title": "Solve CVE-2024-26130 via upgrade to auth0-python==4.7.1",
    "body": "### Context\n\nNebari currently has a dependency on `cryptography==41.0.7` which is vulnerable to CVE-2024-26130.\r\nDue to the dependency tree for that package, upgrading to `auth0-python==4.7.1` will have a dependency side-effect of upgrading to `cryptography==42.0.5` and resolves the CVE.\r\n\r\nI propose the following one-line change:\r\nhttps://github.com/nebari-dev/nebari/blob/develop/pyproject.toml#L58\r\n```\r\n dependencies = [\r\n-    \"auth0-python==4.4.2\",\r\n+    \"auth0-python==4.7.1\",\r\n```\n\n### Value and/or benefit\n\nResolves CVE-2024-26130\n\n### Anything else?\n\nReverse dependency tree, before upgrade:\r\n```\r\n$ pipdeptree --reverse --packages cryptography\r\ncryptography==41.0.7\r\n\u251c\u2500\u2500 auth0-python==4.4.2 [requires: cryptography>=41.0.3,<42.0.0]\r\n\u251c\u2500\u2500 azure-identity==1.12.0 [requires: cryptography>=2.5]\r\n\u251c\u2500\u2500 msal==1.27.0 [requires: cryptography>=0.6,<45]\r\n\u2514\u2500\u2500 pyOpenSSL==23.3.0 [requires: cryptography>=41.0.5,<42]\r\n```\r\n\r\nReverse dependency tree, after upgrade:\r\n```\r\n$ pipdeptree --reverse --packages cryptography\r\ncryptography==42.0.5\r\n\u251c\u2500\u2500 auth0-python==4.7.1 [requires: cryptography>=42.0.4,<43.0.0]\r\n\u251c\u2500\u2500 azure-identity==1.12.0 [requires: cryptography>=2.5]\r\n\u2514\u2500\u2500 msal==1.27.0 [requires: cryptography>=0.6,<45]\r\n```",
    "actual_labels": [
      "security",
      "dependency-update"
    ],
    "predicted_labels": "enhancement, maintenance, dependency-update",
    "model_name": "fine_tuned"
  },
  {
    "index": 99,
    "title": "Allow users to modify the Keycloak PVC size",
    "body": "### Describe the bug\n\nHi Nebari Dev team, \r\n\r\nI was trying to deploy Nebari with an existing Kubernetes config. I stuck at step 05: `kubernetes-keycloak`. It shows that the minimum persistent volume claim requirement is 40GB, how can I override my nebari-config to pass the keycloak stages? \r\n\r\nOur config file looks like this:\r\n\r\n```yaml\r\napiVersion: v1\r\nclusters:\r\n- cluster:\r\n    certificate-authority-data: <CERTIFICATE_AUTH_DATA>\r\n    server: <SERVER>\r\n  name: <NAME>\r\ncontexts:\r\n- context:\r\n    cluster: <CLUSTER>\r\n    user: admin\r\n  name: <NAME>\r\ncurrent-context: <CONTEXT>\r\nkind: Config\r\npreferences: {}\r\nusers:\r\n- name: admin\r\n  user:\r\n    client-certificate-data: <CLIENT_CERT_DATA>\r\n    client-key-data: <CLIENT_KEY_DATA>\r\n```\r\n\r\nMany thanks\n\n### Expected behavior\n\n- Get passed the keycloak stages or all stages in deployment. Ready to be tested locally first. \n\n### OS and architecture in which you are running Nebari\n\nMacOS M1\n\n### How to Reproduce the problem?\n\n1. Init: \r\nRun `nebari init existing --project-name <project> --domain-name <domain>\r\n--namespace <namespace> --auth-provider github --terraform-state local\r\n--kubernetes-version  --ssl-cert-email <email>`\r\n\r\n2. Deployment: \r\nRun `nebari deploy -c nebari-config.yaml --disable-checks`\r\n(I wanted to skip taking IP address from the load balancer for the domain, which is currently being used so not ready yet)\r\n\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "question",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 99,
    "title": "Allow users to modify the Keycloak PVC size",
    "body": "### Describe the bug\n\nHi Nebari Dev team, \r\n\r\nI was trying to deploy Nebari with an existing Kubernetes config. I stuck at step 05: `kubernetes-keycloak`. It shows that the minimum persistent volume claim requirement is 40GB, how can I override my nebari-config to pass the keycloak stages? \r\n\r\nOur config file looks like this:\r\n\r\n```yaml\r\napiVersion: v1\r\nclusters:\r\n- cluster:\r\n    certificate-authority-data: <CERTIFICATE_AUTH_DATA>\r\n    server: <SERVER>\r\n  name: <NAME>\r\ncontexts:\r\n- context:\r\n    cluster: <CLUSTER>\r\n    user: admin\r\n  name: <NAME>\r\ncurrent-context: <CONTEXT>\r\nkind: Config\r\npreferences: {}\r\nusers:\r\n- name: admin\r\n  user:\r\n    client-certificate-data: <CLIENT_CERT_DATA>\r\n    client-key-data: <CLIENT_KEY_DATA>\r\n```\r\n\r\nMany thanks\n\n### Expected behavior\n\n- Get passed the keycloak stages or all stages in deployment. Ready to be tested locally first. \n\n### OS and architecture in which you are running Nebari\n\nMacOS M1\n\n### How to Reproduce the problem?\n\n1. Init: \r\nRun `nebari init existing --project-name <project> --domain-name <domain>\r\n--namespace <namespace> --auth-provider github --terraform-state local\r\n--kubernetes-version  --ssl-cert-email <email>`\r\n\r\n2. Deployment: \r\nRun `nebari deploy -c nebari-config.yaml --disable-checks`\r\n(I wanted to skip taking IP address from the load balancer for the domain, which is currently being used so not ready yet)\r\n\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "question",
      "configuration"
    ],
    "predicted_labels": "bug, configuration, ci/cd, testing",
    "model_name": "fine_tuned"
  },
  {
    "index": 100,
    "title": "Make Dashboard App HPC resource size (RAM/CPU) configurable",
    "body": "",
    "actual_labels": [
      "enhancement",
      "configuration",
      "performance"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 100,
    "title": "Make Dashboard App HPC resource size (RAM/CPU) configurable",
    "body": "",
    "actual_labels": [
      "enhancement",
      "configuration",
      "performance"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 101,
    "title": "[ENH] - Initial scoping for local deployment architecture",
    "body": "### Feature description\n\nA local deployment needs to be scoped and initial discussion started. \n\nShould probably be in the form of an RFD\n\n\n\n### Value and/or benefit\n\n.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 101,
    "title": "[ENH] - Initial scoping for local deployment architecture",
    "body": "### Feature description\n\nA local deployment needs to be scoped and initial discussion started. \n\nShould probably be in the form of an RFD\n\n\n\n### Value and/or benefit\n\n.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 102,
    "title": "Ingress throwing time out error on GCP deployment",
    "body": "Trying to deploy to GCP with the current branch of qhub master and terraform `v0.13.5` results in the following error:\r\n```\r\n Error: timed out waiting for the condition\r\n\r\n  on .terraform/modules/kubernetes-ingress/modules/kubernetes/ingress/main.tf line 7, in resource \"helm_release\" \"ingress\":\r\n   7: resource \"helm_release\" \"ingress\" {\r\n```",
    "actual_labels": [
      "bug",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 102,
    "title": "Ingress throwing time out error on GCP deployment",
    "body": "Trying to deploy to GCP with the current branch of qhub master and terraform `v0.13.5` results in the following error:\r\n```\r\n Error: timed out waiting for the condition\r\n\r\n  on .terraform/modules/kubernetes-ingress/modules/kubernetes/ingress/main.tf line 7, in resource \"helm_release\" \"ingress\":\r\n   7: resource \"helm_release\" \"ingress\" {\r\n```",
    "actual_labels": [
      "bug",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "bug, aws, gcp, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 103,
    "title": "Add jupyterhub-ssh",
    "body": "## Description\r\nAdd https://github.com/yuvipanda/jupyterhub-ssh to qhub-cloud",
    "actual_labels": [
      "enhancement"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 103,
    "title": "Add jupyterhub-ssh",
    "body": "## Description\r\nAdd https://github.com/yuvipanda/jupyterhub-ssh to qhub-cloud",
    "actual_labels": [
      "enhancement"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 104,
    "title": "[BUG] - Jupyter configs should be verified in E2E tests",
    "body": "### Describe the bug\n\nCurrently E2E tests verify that `overrides.json` have an effect on the deployed application (see e8a723ee02c527390f56cbba4c09b0a5e2814e73 introduced in https://github.com/nebari-dev/nebari/pull/2249); the configs in `jupyter_server_config.py` and configs for jupyter_pioneer do not have similar checks.\n\n### Expected behavior\n\nThese checks do not need to verify all options, but one or two just to ensure that the configuration file actually is loaded up in the deployment (is in correct place, etc). https://github.com/nebari-dev/nebari/pull/2286 adds a very simple check against syntax errors in the config files but it does not verify if the files get placed in the correct place.\n\n### OS and architecture in which you are running Nebari\n\n2024.1.2\n\n### How to Reproduce the problem?\n\nSee that CI on `develop` was green even though the config files had syntax errors.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "testing",
      "ci/cd"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 104,
    "title": "[BUG] - Jupyter configs should be verified in E2E tests",
    "body": "### Describe the bug\n\nCurrently E2E tests verify that `overrides.json` have an effect on the deployed application (see e8a723ee02c527390f56cbba4c09b0a5e2814e73 introduced in https://github.com/nebari-dev/nebari/pull/2249); the configs in `jupyter_server_config.py` and configs for jupyter_pioneer do not have similar checks.\n\n### Expected behavior\n\nThese checks do not need to verify all options, but one or two just to ensure that the configuration file actually is loaded up in the deployment (is in correct place, etc). https://github.com/nebari-dev/nebari/pull/2286 adds a very simple check against syntax errors in the config files but it does not verify if the files get placed in the correct place.\n\n### OS and architecture in which you are running Nebari\n\n2024.1.2\n\n### How to Reproduce the problem?\n\nSee that CI on `develop` was green even though the config files had syntax errors.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "testing",
      "ci/cd"
    ],
    "predicted_labels": "bug, ci/cd, testing, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 105,
    "title": "GPU worker scheduler uses the same nodegroup as the workers",
    "body": "Normally it's not a problem that the scheduler is the same nodegroup as a the workers. In the case of GPU workers, this means that an extra GPU will be spun up for the scheduler that can't be used for GPU calculations.",
    "actual_labels": [
      "bug",
      "performance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 105,
    "title": "GPU worker scheduler uses the same nodegroup as the workers",
    "body": "Normally it's not a problem that the scheduler is the same nodegroup as a the workers. In the case of GPU workers, this means that an extra GPU will be spun up for the scheduler that can't be used for GPU calculations.",
    "actual_labels": [
      "bug",
      "performance",
      "configuration"
    ],
    "predicted_labels": "enhancement, performance, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 106,
    "title": "[BUG] - scaling issues in GCP K8s cluster",
    "body": "### OS system and architecture in which you are running Nebari\n\nGCP K8s cluster\n\n### Expected behavior\n\nK8s cluster is online and nodes are scaling as expected.\n\n### Actual behavior\n\n2 scaling issues seen in GCP K8s Engine page:\r\n\r\n<img width=\"638\" alt=\"Screen Shot 2022-11-11 at 6 35 40 PM\" src=\"https://user-images.githubusercontent.com/118010469/201452742-f4a87ce4-682c-415a-ac1f-0b2569353c32.png\">\r\n\r\n<img width=\"613\" alt=\"Screen Shot 2022-11-11 at 6 35 20 PM\" src=\"https://user-images.githubusercontent.com/118010469/201452746-c3d612ad-3b3c-427d-9b79-2d9faae9b0f5.png\">\r\n\r\n<img width=\"616\" alt=\"Screen Shot 2022-11-11 at 6 35 25 PM\" src=\"https://user-images.githubusercontent.com/118010469/201452749-cbba17d3-22d8-40b9-bf79-0314b025cdaa.png\">\r\n\n\n### How to Reproduce the problem?\n\nCreate a Nebari K8s cluster in GCP using default `nebari-config.yaml`.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n- `1.23.8-gke.1900` ([link](https://cloud.google.com/kubernetes-engine/docs/release-notes))\r\n- Nebari version `v2022.10.1`\n\n### Compute environment\n\nGCP\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "gcp",
      "performance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 106,
    "title": "[BUG] - scaling issues in GCP K8s cluster",
    "body": "### OS system and architecture in which you are running Nebari\n\nGCP K8s cluster\n\n### Expected behavior\n\nK8s cluster is online and nodes are scaling as expected.\n\n### Actual behavior\n\n2 scaling issues seen in GCP K8s Engine page:\r\n\r\n<img width=\"638\" alt=\"Screen Shot 2022-11-11 at 6 35 40 PM\" src=\"https://user-images.githubusercontent.com/118010469/201452742-f4a87ce4-682c-415a-ac1f-0b2569353c32.png\">\r\n\r\n<img width=\"613\" alt=\"Screen Shot 2022-11-11 at 6 35 20 PM\" src=\"https://user-images.githubusercontent.com/118010469/201452746-c3d612ad-3b3c-427d-9b79-2d9faae9b0f5.png\">\r\n\r\n<img width=\"616\" alt=\"Screen Shot 2022-11-11 at 6 35 25 PM\" src=\"https://user-images.githubusercontent.com/118010469/201452749-cbba17d3-22d8-40b9-bf79-0314b025cdaa.png\">\r\n\n\n### How to Reproduce the problem?\n\nCreate a Nebari K8s cluster in GCP using default `nebari-config.yaml`.\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n- `1.23.8-gke.1900` ([link](https://cloud.google.com/kubernetes-engine/docs/release-notes))\r\n- Nebari version `v2022.10.1`\n\n### Compute environment\n\nGCP\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "gcp",
      "performance",
      "configuration"
    ],
    "predicted_labels": "bug, gcp, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 107,
    "title": "[fix] Formating issues from #1003",
    "body": "## Describe the problem\r\nWithin the latest changes made for #1003, some scripts didn't pass the formatting standards required upon merge. The issue was found over `test.yaml` action. Further developments and new workflows of testing will be included within #1007 \r\n\r\n## Expected behavior\r\n\r\n- Green passing tests on main",
    "actual_labels": [
      "bug",
      "ci/cd"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 107,
    "title": "[fix] Formating issues from #1003",
    "body": "## Describe the problem\r\nWithin the latest changes made for #1003, some scripts didn't pass the formatting standards required upon merge. The issue was found over `test.yaml` action. Further developments and new workflows of testing will be included within #1007 \r\n\r\n## Expected behavior\r\n\r\n- Green passing tests on main",
    "actual_labels": [
      "bug",
      "ci/cd"
    ],
    "predicted_labels": "bug, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 108,
    "title": "[BUG] - Upgrading nebari to `2025.2.1rc1` leads to broken conda-store due to remaints of Docker builds",
    "body": "### Describe the bug\n\nAnother issue, in the latest conda-store migration, in `[2025.1.1](https://github.com/conda-incubator/conda-store/releases/tag/2025.1.1)` the DOCKER_BLOB column schema for the docker builds [was removed](https://github.com/conda-incubator/conda-store/pull/1017), I initially assumed alambic would take care of any post DB update, but was wrong. \n\n![Image](https://github.com/user-attachments/assets/f31e1e2e-f5c8-4f34-b0ce-4db670ca4c98)\n\nIn case any entries are present in the DB, the whole conda-store server breaks as seen below:\n\n```shell\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/views/api.py\", line 724, in api_list_environments_v1\n    |     return paginated_api_response(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/views/api.py\", line 100, in paginated_api_response\n    |     object_schema.model_validate(_).model_dump(exclude=exclude)\n    |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/pydantic/main.py\", line 627, in model_validate\n    |     return cls.__pydantic_validator__.validate_python(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    | pydantic_core._pydantic_core.ValidationError: 1 validation error for Environment\n    | current_build.build_artifacts\n    |   Error extracting attribute: LookupError: 'DOCKER_BLOB' is not among the defined enum values. Enum name: buildartifacttype. Possible values: DIRECTORY, LOCKFILE, LOGS, ..., _ [type=get_attribute_error, input_value=<unprintable Build object>, input_type=Build]\n    |     For further information visit https://errors.pydantic.dev/2.10/v/get_attribute_error\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/base.py\", line 177, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n                                   ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/base.py\", line 179, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/app.py\", line 257, in conda_store_middleware\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/base.py\", line 154, in call_next\n    raise app_exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/sessions.py\", line 85, in __call__\n    await self.app(scope, receive, send_wrapper)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/views/api.py\", line 724, in api_list_environments_v1\n    return paginated_api_response(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/views/api.py\", line 100, in paginated_api_response\n    object_schema.model_validate(_).model_dump(exclude=exclude)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/pydantic/main.py\", line 627, in model_validate\n    return cls.__pydantic_validator__.validate_python(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for Environment\ncurrent_build.build_artifacts\n  Error extracting attribute: LookupError: 'DOCKER_BLOB' is not among the defined enum values. Enum name: buildartifacttype. Possible values: DIRECTORY, LOCKFILE, LOGS, ..., _ [type=get_attribute_error, input_value=<unprintable Build object>, input_type=Build]\n    For further information visit https://errors.pydantic.dev/2.10/v/get_attribute_error\n```\n\nFortunately removing all affected builds in the DB through the API solves the issue, I am planning on encapsulating this as an automated step in the upgrade command that will perform the following:\n- Identify affected packages/builds (whose build_artifact is DOCKER_BLOB)\n- Preview the removal as a note to the user, and present with the option to save locally all builds' lock-files in case the user is interested in keeping it.\n- \n\n### Expected behavior\n\nfully functional conda-store server execution\n\n### OS and architecture in which you are running Nebari\n\nLinux\n\n### How to Reproduce the problem?\n\nUsing any nebari deployment prior to `2025.2.1rc1` build a conda-store environment that requires a docker registry, then update nebari to its RC \n\n### Command output\n\n```bash session\n\n```\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nGCP\n\n### Integrations\n\nconda-store\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "gcp",
      "maintenance",
      "dependency-update"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n\nAllowed labels with descriptions",
    "model_name": "base"
  },
  {
    "index": 108,
    "title": "[BUG] - Upgrading nebari to `2025.2.1rc1` leads to broken conda-store due to remaints of Docker builds",
    "body": "### Describe the bug\n\nAnother issue, in the latest conda-store migration, in `[2025.1.1](https://github.com/conda-incubator/conda-store/releases/tag/2025.1.1)` the DOCKER_BLOB column schema for the docker builds [was removed](https://github.com/conda-incubator/conda-store/pull/1017), I initially assumed alambic would take care of any post DB update, but was wrong. \n\n![Image](https://github.com/user-attachments/assets/f31e1e2e-f5c8-4f34-b0ce-4db670ca4c98)\n\nIn case any entries are present in the DB, the whole conda-store server breaks as seen below:\n\n```shell\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/views/api.py\", line 724, in api_list_environments_v1\n    |     return paginated_api_response(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/views/api.py\", line 100, in paginated_api_response\n    |     object_schema.model_validate(_).model_dump(exclude=exclude)\n    |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    |   File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/pydantic/main.py\", line 627, in model_validate\n    |     return cls.__pydantic_validator__.validate_python(\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    | pydantic_core._pydantic_core.ValidationError: 1 validation error for Environment\n    | current_build.build_artifacts\n    |   Error extracting attribute: LookupError: 'DOCKER_BLOB' is not among the defined enum values. Enum name: buildartifacttype. Possible values: DIRECTORY, LOCKFILE, LOGS, ..., _ [type=get_attribute_error, input_value=<unprintable Build object>, input_type=Build]\n    |     For further information visit https://errors.pydantic.dev/2.10/v/get_attribute_error\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/uvicorn/protocols/http/h11_impl.py\", line 403, in run_asgi\n    result = await app(  # type: ignore[func-returns-value]\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py\", line 60, in __call__\n    return await self.app(scope, receive, send)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/applications.py\", line 1054, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/applications.py\", line 112, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 187, in __call__\n    raise exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/base.py\", line 177, in __call__\n    with recv_stream, send_stream, collapse_excgroups():\n                                   ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/base.py\", line 179, in __call__\n    response = await self.dispatch_func(request, call_next)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/app.py\", line 257, in conda_store_middleware\n    response = await call_next(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/base.py\", line 154, in call_next\n    raise app_exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/sessions.py\", line 85, in __call__\n    await self.app(scope, receive, send_wrapper)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/cors.py\", line 85, in __call__\n    await self.app(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 715, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 735, in app\n    await route.handle(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n               ^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/views/api.py\", line 724, in api_list_environments_v1\n    return paginated_api_response(\n           ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/conda_store_server/_internal/server/views/api.py\", line 100, in paginated_api_response\n    object_schema.model_validate(_).model_dump(exclude=exclude)\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/conda/envs/conda-store-server/lib/python3.12/site-packages/pydantic/main.py\", line 627, in model_validate\n    return cls.__pydantic_validator__.validate_python(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\npydantic_core._pydantic_core.ValidationError: 1 validation error for Environment\ncurrent_build.build_artifacts\n  Error extracting attribute: LookupError: 'DOCKER_BLOB' is not among the defined enum values. Enum name: buildartifacttype. Possible values: DIRECTORY, LOCKFILE, LOGS, ..., _ [type=get_attribute_error, input_value=<unprintable Build object>, input_type=Build]\n    For further information visit https://errors.pydantic.dev/2.10/v/get_attribute_error\n```\n\nFortunately removing all affected builds in the DB through the API solves the issue, I am planning on encapsulating this as an automated step in the upgrade command that will perform the following:\n- Identify affected packages/builds (whose build_artifact is DOCKER_BLOB)\n- Preview the removal as a note to the user, and present with the option to save locally all builds' lock-files in case the user is interested in keeping it.\n- \n\n### Expected behavior\n\nfully functional conda-store server execution\n\n### OS and architecture in which you are running Nebari\n\nLinux\n\n### How to Reproduce the problem?\n\nUsing any nebari deployment prior to `2025.2.1rc1` build a conda-store environment that requires a docker registry, then update nebari to its RC \n\n### Command output\n\n```bash session\n\n```\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nGCP\n\n### Integrations\n\nconda-store\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "gcp",
      "maintenance",
      "dependency-update"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 109,
    "title": "Unify y/N questions with an \"Enter\" key press throughout",
    "body": "## General context\r\n\r\nThis issue is a part of a broader effort to improve the experience around Nebari's `guided-init` CLI.\r\n\r\nYou can check out this interface on [installing Nebari](https://www.nebari.dev/docs/get-started/installing-nebari) and running `nebari init --guided-init`.\r\n\r\n## Details around issue\r\n\r\nThere are some Yes/No questions in the guided-init questionnaire. These get completed on pressing \"Y\" or \"N\", before you press \"Enter\". However, the other questions where you need to select from a dropdown, or type-in something do require you to press \"Enter\" to continue. This is an inconsistent experience.\r\n\r\nOriginally posted here: https://github.com/nebari-dev/nebari/issues/1478#issuecomment-1357966287\r\n\r\n## How to resolve?\r\n\r\nFor Yes/No questions, wait for the user to press \"Enter\" before moving on\r\n\r\nRelevant section of codebase: https://github.com/nebari-dev/nebari/tree/develop/nebari/cli\r\n\r\n",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 109,
    "title": "Unify y/N questions with an \"Enter\" key press throughout",
    "body": "## General context\r\n\r\nThis issue is a part of a broader effort to improve the experience around Nebari's `guided-init` CLI.\r\n\r\nYou can check out this interface on [installing Nebari](https://www.nebari.dev/docs/get-started/installing-nebari) and running `nebari init --guided-init`.\r\n\r\n## Details around issue\r\n\r\nThere are some Yes/No questions in the guided-init questionnaire. These get completed on pressing \"Y\" or \"N\", before you press \"Enter\". However, the other questions where you need to select from a dropdown, or type-in something do require you to press \"Enter\" to continue. This is an inconsistent experience.\r\n\r\nOriginally posted here: https://github.com/nebari-dev/nebari/issues/1478#issuecomment-1357966287\r\n\r\n## How to resolve?\r\n\r\nFor Yes/No questions, wait for the user to press \"Enter\" before moving on\r\n\r\nRelevant section of codebase: https://github.com/nebari-dev/nebari/tree/develop/nebari/cli\r\n\r\n",
    "actual_labels": [
      "enhancement",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 110,
    "title": "Fix jupyter-ssh instructions in 01_interface.ipynb to account for '@' in usernames",
    "body": "\r\n> Open a terminal and ssh with port 8022 `ssh <username>@demo.nebari.dev -p 8022`\r\n\r\nThis will break with default Nebari usernames that are based on email addresses (i.e. username = name@domain.com) because of of the extra `@` symbol. \r\n\r\nInstead use: `ssh demo.nebari.dev -l <username> -p 8022`\r\n\r\n\r\n",
    "actual_labels": [
      "bug",
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 110,
    "title": "Fix jupyter-ssh instructions in 01_interface.ipynb to account for '@' in usernames",
    "body": "\r\n> Open a terminal and ssh with port 8022 `ssh <username>@demo.nebari.dev -p 8022`\r\n\r\nThis will break with default Nebari usernames that are based on email addresses (i.e. username = name@domain.com) because of of the extra `@` symbol. \r\n\r\nInstead use: `ssh demo.nebari.dev -l <username> -p 8022`\r\n\r\n\r\n",
    "actual_labels": [
      "bug",
      "documentation"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 111,
    "title": "[BUG] - jhub-apps entering loop of failed requests",
    "body": "### Describe the bug\r\n\r\n@marcelovilla found an issue with the latest release (`2024.11.1`). When users visit `/hub`, they get stuck in a loop of failed requests:\r\n\r\nhttps://github.com/user-attachments/assets/7ab1c099-4598-44ba-aa40-fca059fdf196\r\n\r\nExamining the browser\u2019s console logs reveals that all requests to `services/japps/user` and `services/japps/spawner-profiles` are returning `401 Unauthenticated` errors. \r\n![WhatsApp Image 2024-11-22 at 19 53 13](https://github.com/user-attachments/assets/fd1ed3ec-51db-4f6b-9bb5-b21920dd8efb)\r\n\r\nAfter a quick debugging, the error seems to originate from the current docker images, relabeling and pushing new Docker images from the previous release should resolve the issue for that version. --  It\u2019s possible that the `build-and-push` action targeted the wrong branch during the build process. \r\n\r\nWe observed the same behavior on both GCP and AWS deployments using the `2024.11.1` images. I was also able to reproduce the issue on a local deployment from `main`, which means that the problem isn\u2019t limited to this release and might be related to the update of `jhub-apps` to `2024.10.1`.\r\n\r\n### Required actions:\r\n- [x] Fix bug in source code of jhub-apps, if identified as the error root cause\r\n- [ ] re-tag previous `2024.9.1` docker image as `2024.11.1` (this might be a manual process) \r\n\r\n### Expected behavior\r\n\r\nuser should be able to access the /hub pages successfully when using jhub-appsUser\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nLinux\r\n\r\n### How to Reproduce the problem?\r\n\r\nDeploy nebari on local using `2024.11.1` or `main`, since this is likely a problem with the images, just using the current image in any recent release of nebari might also trigger the problem.\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\nAfter checking the git history for the Docker images and the Nebari source code, the only significant change was upgrading `jupyterlab` from version `4.2.5` to `4.2.6`.\r\n\r\nTo see the differences between the image versions, I used [diffoci](https://github.com/reproducible-containers/diffoci) with this command:\r\n\r\n```shell\r\ndiffoci diff quay.io/nebari/nebari-jupyterhub:2024.9.1 quay.io/nebari/nebari-jupyterhub:2024.11.1 --platform=linux/amd64 > report.diff\r\n```\r\n\r\nThe same diff reported the same version of `jhub-apps` in both images, so I am not entirely sure how the deployment ended up in such state. \r\n```\r\nFile     opt/conda/lib/python3.9/site-packages/jhub_apps-2024.8.1.dist-info/REQUESTED                                                                                         2024-09-27 19:08:57 -0300 -03                                                                                                                                               2024-11-21 12:40:24 -0300 -03\r\n```\r\nAlso, the actual build logs for both images corroborate the above:\r\n\r\n[2024.11.1](https://github.com/nebari-dev/nebari-docker-images/actions/runs/11956586188/job/33331687802)\r\n```diff\r\n25 81.91 jhsingle-native-proxy     0.8.2                    pypi_0    pypi\r\n+ 25 81.91 jhub-apps                 2024.8.1                 pypi_0    pypi\r\n25 81.91 jinja2                    3.1.4              pyhd8ed1ab_0    conda-forge\r\n25 81.91 json5                     0.9.25                   pypi_0    pypi\r\n25 81.91 jsonpointer               3.0.0            py39hf3d152e_1    conda-forge\r\n25 81.91 jsonschema                4.23.0             pyhd8ed1ab_0    conda-forge\r\n25 81.91 jsonschema-specifications 2023.12.1          pyhd8ed1ab_0    conda-forge\r\n25 81.91 jsonschema-with-format-nongpl 4.23.0               hd8ed1ab_0    conda-forge\r\n+ 25 81.91 jupyter                   1.1.1                    pypi_0    pypi\r\n25 81.91 jupyter-client            8.6.3                    pypi_0    pypi\r\n25 81.91 jupyter-console           6.6.3                    pypi_0    pypi\r\n25 81.91 jupyter-core              5.7.2                    pypi_0    pypi\r\n25 81.91 jupyter-lsp               2.2.5                    pypi_0    pypi\r\n25 81.91 jupyter-server            2.14.2                   pypi_0    pypi\r\n25 81.91 jupyter-server-terminals  0.5.3                    pypi_0    pypi\r\n25 81.91 jupyter_events            0.10.0             pyhd8ed1ab_0    conda-forge\r\n+ 25 81.91 jupyterhub                5.1.0              pyh31011fe_0    conda-forge\r\n25 81.91 jupyterhub-base           5.1.0              pyh31011fe_0    conda-forge\r\n25 81.91 jupyterhub-idle-culler    1.2.1              pyhd8ed1ab_0    conda-forge\r\n25 81.91 jupyterhub-kubespawner    4.2.0              pyhd8ed1ab_0    conda-forge\r\n+ 25 81.91 jupyterlab                4.2.5                    pypi_0    pypi\r\n25 81.91 jupyterlab-pygments       0.3.0                    pypi_0    pypi\r\n25 81.91 jupyterlab-server         2.27.3                   pypi_0    pypi\r\n25 81.91 jupyterlab-widgets        3.0.13                   pypi_0    pypi\r\n```\r\n\r\n[2024.9.1](https://github.com/nebari-dev/nebari-docker-images/actions/runs/11078303931/job/30785369171)\r\n\r\n```diff\r\n19 80.77 jhsingle-native-proxy     0.8.2                    pypi_0    pypi\r\n+ 19 80.77 jhub-apps                 2024.8.1                 pypi_0    pypi\r\n19 80.77 jinja2                    3.1.4              pyhd8ed1ab_0    conda-forge\r\n19 80.77 json5                     0.9.28                   pypi_0    pypi\r\n19 80.77 jsonpointer               3.0.0            py39hf3d152e_1    conda-forge\r\n19 80.77 jsonschema                4.23.0             pyhd8ed1ab_0    conda-forge\r\n19 80.77 jsonschema-specifications 2024.10.1          pyhd8ed1ab_0    conda-forge\r\n19 80.77 jsonschema-with-format-nongpl 4.23.0               hd8ed1ab_0    conda-forge\r\n+ 19 80.77 jupyter                   1.1.1                    pypi_0    pypi\r\n19 80.77 jupyter-client            8.6.3                    pypi_0    pypi\r\n19 80.77 jupyter-console           6.6.3                    pypi_0    pypi\r\n19 80.77 jupyter-core              5.7.2                    pypi_0    pypi\r\n19 80.77 jupyter-lsp               2.2.5                    pypi_0    pypi\r\n19 80.77 jupyter-server            2.14.2                   pypi_0    pypi\r\n19 80.77 jupyter-server-terminals  0.5.3                    pypi_0    pypi\r\n19 80.77 jupyter_events            0.10.0             pyhd8ed1ab_0    conda-forge\r\n+ 19 80.77 jupyterhub                5.1.0              pyh31011fe_0    conda-forge\r\n19 80.77 jupyterhub-base           5.1.0              pyh31011fe_0    conda-forge\r\n19 80.77 jupyterhub-idle-culler    1.2.1              pyhd8ed1ab_0    conda-forge\r\n19 80.77 jupyterhub-kubespawner    4.2.0              pyhd8ed1ab_0    conda-forge\r\n+ 19 80.77 jupyterlab                4.2.6                    pypi_0    pypi\r\n19 80.77 jupyterlab-pygments       0.3.0                    pypi_0    pypi\r\n19 80.77 jupyterlab-server         2.27.3                   pypi_0    pypi\r\n19 80.77 jupyterlab-widgets        3.0.13                   pypi_0    pypi\r\n```",
    "actual_labels": [
      "bug",
      "release",
      "ci/cd",
      "aws",
      "gcp",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 111,
    "title": "[BUG] - jhub-apps entering loop of failed requests",
    "body": "### Describe the bug\r\n\r\n@marcelovilla found an issue with the latest release (`2024.11.1`). When users visit `/hub`, they get stuck in a loop of failed requests:\r\n\r\nhttps://github.com/user-attachments/assets/7ab1c099-4598-44ba-aa40-fca059fdf196\r\n\r\nExamining the browser\u2019s console logs reveals that all requests to `services/japps/user` and `services/japps/spawner-profiles` are returning `401 Unauthenticated` errors. \r\n![WhatsApp Image 2024-11-22 at 19 53 13](https://github.com/user-attachments/assets/fd1ed3ec-51db-4f6b-9bb5-b21920dd8efb)\r\n\r\nAfter a quick debugging, the error seems to originate from the current docker images, relabeling and pushing new Docker images from the previous release should resolve the issue for that version. --  It\u2019s possible that the `build-and-push` action targeted the wrong branch during the build process. \r\n\r\nWe observed the same behavior on both GCP and AWS deployments using the `2024.11.1` images. I was also able to reproduce the issue on a local deployment from `main`, which means that the problem isn\u2019t limited to this release and might be related to the update of `jhub-apps` to `2024.10.1`.\r\n\r\n### Required actions:\r\n- [x] Fix bug in source code of jhub-apps, if identified as the error root cause\r\n- [ ] re-tag previous `2024.9.1` docker image as `2024.11.1` (this might be a manual process) \r\n\r\n### Expected behavior\r\n\r\nuser should be able to access the /hub pages successfully when using jhub-appsUser\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nLinux\r\n\r\n### How to Reproduce the problem?\r\n\r\nDeploy nebari on local using `2024.11.1` or `main`, since this is likely a problem with the images, just using the current image in any recent release of nebari might also trigger the problem.\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\n_No response_\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\nAfter checking the git history for the Docker images and the Nebari source code, the only significant change was upgrading `jupyterlab` from version `4.2.5` to `4.2.6`.\r\n\r\nTo see the differences between the image versions, I used [diffoci](https://github.com/reproducible-containers/diffoci) with this command:\r\n\r\n```shell\r\ndiffoci diff quay.io/nebari/nebari-jupyterhub:2024.9.1 quay.io/nebari/nebari-jupyterhub:2024.11.1 --platform=linux/amd64 > report.diff\r\n```\r\n\r\nThe same diff reported the same version of `jhub-apps` in both images, so I am not entirely sure how the deployment ended up in such state. \r\n```\r\nFile     opt/conda/lib/python3.9/site-packages/jhub_apps-2024.8.1.dist-info/REQUESTED                                                                                         2024-09-27 19:08:57 -0300 -03                                                                                                                                               2024-11-21 12:40:24 -0300 -03\r\n```\r\nAlso, the actual build logs for both images corroborate the above:\r\n\r\n[2024.11.1](https://github.com/nebari-dev/nebari-docker-images/actions/runs/11956586188/job/33331687802)\r\n```diff\r\n25 81.91 jhsingle-native-proxy     0.8.2                    pypi_0    pypi\r\n+ 25 81.91 jhub-apps                 2024.8.1                 pypi_0    pypi\r\n25 81.91 jinja2                    3.1.4              pyhd8ed1ab_0    conda-forge\r\n25 81.91 json5                     0.9.25                   pypi_0    pypi\r\n25 81.91 jsonpointer               3.0.0            py39hf3d152e_1    conda-forge\r\n25 81.91 jsonschema                4.23.0             pyhd8ed1ab_0    conda-forge\r\n25 81.91 jsonschema-specifications 2023.12.1          pyhd8ed1ab_0    conda-forge\r\n25 81.91 jsonschema-with-format-nongpl 4.23.0               hd8ed1ab_0    conda-forge\r\n+ 25 81.91 jupyter                   1.1.1                    pypi_0    pypi\r\n25 81.91 jupyter-client            8.6.3                    pypi_0    pypi\r\n25 81.91 jupyter-console           6.6.3                    pypi_0    pypi\r\n25 81.91 jupyter-core              5.7.2                    pypi_0    pypi\r\n25 81.91 jupyter-lsp               2.2.5                    pypi_0    pypi\r\n25 81.91 jupyter-server            2.14.2                   pypi_0    pypi\r\n25 81.91 jupyter-server-terminals  0.5.3                    pypi_0    pypi\r\n25 81.91 jupyter_events            0.10.0             pyhd8ed1ab_0    conda-forge\r\n+ 25 81.91 jupyterhub                5.1.0              pyh31011fe_0    conda-forge\r\n25 81.91 jupyterhub-base           5.1.0              pyh31011fe_0    conda-forge\r\n25 81.91 jupyterhub-idle-culler    1.2.1              pyhd8ed1ab_0    conda-forge\r\n25 81.91 jupyterhub-kubespawner    4.2.0              pyhd8ed1ab_0    conda-forge\r\n+ 25 81.91 jupyterlab                4.2.5                    pypi_0    pypi\r\n25 81.91 jupyterlab-pygments       0.3.0                    pypi_0    pypi\r\n25 81.91 jupyterlab-server         2.27.3                   pypi_0    pypi\r\n25 81.91 jupyterlab-widgets        3.0.13                   pypi_0    pypi\r\n```\r\n\r\n[2024.9.1](https://github.com/nebari-dev/nebari-docker-images/actions/runs/11078303931/job/30785369171)\r\n\r\n```diff\r\n19 80.77 jhsingle-native-proxy     0.8.2                    pypi_0    pypi\r\n+ 19 80.77 jhub-apps                 2024.8.1                 pypi_0    pypi\r\n19 80.77 jinja2                    3.1.4              pyhd8ed1ab_0    conda-forge\r\n19 80.77 json5                     0.9.28                   pypi_0    pypi\r\n19 80.77 jsonpointer               3.0.0            py39hf3d152e_1    conda-forge\r\n19 80.77 jsonschema                4.23.0             pyhd8ed1ab_0    conda-forge\r\n19 80.77 jsonschema-specifications 2024.10.1          pyhd8ed1ab_0    conda-forge\r\n19 80.77 jsonschema-with-format-nongpl 4.23.0               hd8ed1ab_0    conda-forge\r\n+ 19 80.77 jupyter                   1.1.1                    pypi_0    pypi\r\n19 80.77 jupyter-client            8.6.3                    pypi_0    pypi\r\n19 80.77 jupyter-console           6.6.3                    pypi_0    pypi\r\n19 80.77 jupyter-core              5.7.2                    pypi_0    pypi\r\n19 80.77 jupyter-lsp               2.2.5                    pypi_0    pypi\r\n19 80.77 jupyter-server            2.14.2                   pypi_0    pypi\r\n19 80.77 jupyter-server-terminals  0.5.3                    pypi_0    pypi\r\n19 80.77 jupyter_events            0.10.0             pyhd8ed1ab_0    conda-forge\r\n+ 19 80.77 jupyterhub                5.1.0              pyh31011fe_0    conda-forge\r\n19 80.77 jupyterhub-base           5.1.0              pyh31011fe_0    conda-forge\r\n19 80.77 jupyterhub-idle-culler    1.2.1              pyhd8ed1ab_0    conda-forge\r\n19 80.77 jupyterhub-kubespawner    4.2.0              pyhd8ed1ab_0    conda-forge\r\n+ 19 80.77 jupyterlab                4.2.6                    pypi_0    pypi\r\n19 80.77 jupyterlab-pygments       0.3.0                    pypi_0    pypi\r\n19 80.77 jupyterlab-server         2.27.3                   pypi_0    pypi\r\n19 80.77 jupyterlab-widgets        3.0.13                   pypi_0    pypi\r\n```",
    "actual_labels": [
      "bug",
      "release",
      "ci/cd",
      "aws",
      "gcp",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "fine_tuned"
  },
  {
    "index": 112,
    "title": "[BUG] - CDSDashboards permissions denied when sharing with additional users",
    "body": "### OS system and architecture in which you are running QHub\n\nLinux (will occur on any OS)\n\n### Expected behavior\n\nShould be able to share dashboards with other users.\n\n### Actual behavior\n\n503 permissions denied when attempting\n\n### How to Reproduce the problem?\n\nShare a dashboard\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\ndashboards\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 112,
    "title": "[BUG] - CDSDashboards permissions denied when sharing with additional users",
    "body": "### OS system and architecture in which you are running QHub\n\nLinux (will occur on any OS)\n\n### Expected behavior\n\nShould be able to share dashboards with other users.\n\n### Actual behavior\n\n503 permissions denied when attempting\n\n### How to Reproduce the problem?\n\nShare a dashboard\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\ndashboards\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 113,
    "title": "Scoping out compile cluster for QHub and QT conda-forge",
    "body": "# Notes \r\n\r\nRalf, Tania, compilation of QT within QHub. Set up servers and tear\r\nthem down when not being used. Could we do this on QHub.\r\n\r\nUsing NFS mounts for storage. Compilation will suck.\r\n\r\nRequirements:\r\n - attaching a scratch volume with high speed ssd\r\n - having docker or being able to mount a docker image\r\n - they have some scripts that download and run everything inside of\r\n   the image. Must use images\r\n - the libraries are linked\r\n - =docker run /bin/bash=, download and run everything inside of image\r\n - the development environment\r\n - resources 32 RAM, 8-16 cpus\r\n - arm64 compilation (only linux)\r\n - sooner is better\r\n",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "performance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 113,
    "title": "Scoping out compile cluster for QHub and QT conda-forge",
    "body": "# Notes \r\n\r\nRalf, Tania, compilation of QT within QHub. Set up servers and tear\r\nthem down when not being used. Could we do this on QHub.\r\n\r\nUsing NFS mounts for storage. Compilation will suck.\r\n\r\nRequirements:\r\n - attaching a scratch volume with high speed ssd\r\n - having docker or being able to mount a docker image\r\n - they have some scripts that download and run everything inside of\r\n   the image. Must use images\r\n - the libraries are linked\r\n - =docker run /bin/bash=, download and run everything inside of image\r\n - the development environment\r\n - resources 32 RAM, 8-16 cpus\r\n - arm64 compilation (only linux)\r\n - sooner is better\r\n",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "performance",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 114,
    "title": "Support for multiple environment.yaml directories with conda-store (one editable by users)",
    "body": "",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 114,
    "title": "Support for multiple environment.yaml directories with conda-store (one editable by users)",
    "body": "",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 115,
    "title": "Testing checklist for 2025.3.1-rc1",
    "body": "# Testing Checklist\n\n_Use/modify this checklist to capture the platform's core services that need to be manually tested ._\n\n## Manual testing: core services\n\nIf the integration tests for all of the cloud providers are successful, that is a good sign!\nHowever, the following core services still need to be manually validated (until we can automate them).\n\nAt minimum, the following services will need to be tested:\n\n- [x] [Log into keycloak as root user](https://www.nebari.dev/docs/how-tos/configuring-keycloak/#change-keycloak-root-password)\n  - [x] [Add a user](https://www.nebari.dev/docs/how-tos/configuring-keycloak/#adding-a-nebari-user)\n- [x] [Log into conda-store and create](https://www.nebari.dev/docs/tutorials/creating-new-environments)\n  - [x] a conda environment in a shared namespace and,\n  - [x] a conda environment in your personal namespace\n- [x] [Launch dask-gateway cluster, test auto-scaler and](https://www.nebari.dev/docs/tutorials/using_dask)\n  - [x] [Validate that the dask-labextention is working](https://www.nebari.dev/docs/tutorials/using_dask/#step-4---understand-dasks-diagnostic-tools)\n- [x] [Confirm that a notebook can be submitted via Jupyter-Scheduler](https://nebari.dev/docs/tutorials/jupyter-scheduler)\n- [x] [Open VS-Code extension](https://www.nebari.dev/docs/how-tos/using-vscode)\n  - [x] [Add the Python extension](https://www.nebari.dev/docs/how-tos/using-vscode#adding-extensions)\n  - [x] [Create a `.py` file and run it](https://www.nebari.dev/docs/how-tos/using-vscode#running-python-code)\n",
    "actual_labels": [
      "release",
      "testing"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 115,
    "title": "Testing checklist for 2025.3.1-rc1",
    "body": "# Testing Checklist\n\n_Use/modify this checklist to capture the platform's core services that need to be manually tested ._\n\n## Manual testing: core services\n\nIf the integration tests for all of the cloud providers are successful, that is a good sign!\nHowever, the following core services still need to be manually validated (until we can automate them).\n\nAt minimum, the following services will need to be tested:\n\n- [x] [Log into keycloak as root user](https://www.nebari.dev/docs/how-tos/configuring-keycloak/#change-keycloak-root-password)\n  - [x] [Add a user](https://www.nebari.dev/docs/how-tos/configuring-keycloak/#adding-a-nebari-user)\n- [x] [Log into conda-store and create](https://www.nebari.dev/docs/tutorials/creating-new-environments)\n  - [x] a conda environment in a shared namespace and,\n  - [x] a conda environment in your personal namespace\n- [x] [Launch dask-gateway cluster, test auto-scaler and](https://www.nebari.dev/docs/tutorials/using_dask)\n  - [x] [Validate that the dask-labextention is working](https://www.nebari.dev/docs/tutorials/using_dask/#step-4---understand-dasks-diagnostic-tools)\n- [x] [Confirm that a notebook can be submitted via Jupyter-Scheduler](https://nebari.dev/docs/tutorials/jupyter-scheduler)\n- [x] [Open VS-Code extension](https://www.nebari.dev/docs/how-tos/using-vscode)\n  - [x] [Add the Python extension](https://www.nebari.dev/docs/how-tos/using-vscode#adding-extensions)\n  - [x] [Create a `.py` file and run it](https://www.nebari.dev/docs/how-tos/using-vscode#running-python-code)\n",
    "actual_labels": [
      "release",
      "testing"
    ],
    "predicted_labels": "testing, ci/cd, testing",
    "model_name": "fine_tuned"
  },
  {
    "index": 116,
    "title": "MAINT - Update our CI/CD schematic",
    "body": "@trallard  worked on diagrams for our CI/CD pipelines last year.\r\nBefore proceeding with enhancements to our CI/CD pipelines, we need to ensure these diagrams are up-to-date.\r\n\r\nWe will then use these to decide what needs to be changed and updated as part of #1128 \r\n\r\n@pavithraes do you think it would be worth adding to our docs for maintainers?\r\n\r\n### Links\r\n- [Lucid schematic file](https://lucid.app/lucidchart/b8dfb349-d199-4034-bfb3-e21d6fac48b9/edit?invitationId=inv_9c7c7e09-ca21-4026-a1b3-8e4da7647b0e)",
    "actual_labels": [
      "maintenance",
      "ci/cd",
      "documentation"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 116,
    "title": "MAINT - Update our CI/CD schematic",
    "body": "@trallard  worked on diagrams for our CI/CD pipelines last year.\r\nBefore proceeding with enhancements to our CI/CD pipelines, we need to ensure these diagrams are up-to-date.\r\n\r\nWe will then use these to decide what needs to be changed and updated as part of #1128 \r\n\r\n@pavithraes do you think it would be worth adding to our docs for maintainers?\r\n\r\n### Links\r\n- [Lucid schematic file](https://lucid.app/lucidchart/b8dfb349-d199-4034-bfb3-e21d6fac48b9/edit?invitationId=inv_9c7c7e09-ca21-4026-a1b3-8e4da7647b0e)",
    "actual_labels": [
      "maintenance",
      "ci/cd",
      "documentation"
    ],
    "predicted_labels": "maintenance, ci/cd, testing, release",
    "model_name": "fine_tuned"
  },
  {
    "index": 117,
    "title": "argo-workflows",
    "body": "",
    "actual_labels": [
      "question",
      "ci/cd"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 117,
    "title": "argo-workflows",
    "body": "",
    "actual_labels": [
      "question",
      "ci/cd"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 118,
    "title": "Adding Azure Support",
    "body": "",
    "actual_labels": [
      "enhancement",
      "azure"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 118,
    "title": "Adding Azure Support",
    "body": "",
    "actual_labels": [
      "enhancement",
      "azure"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 119,
    "title": "Deployment Issues on Existing Kubernetes cluster",
    "body": "Theoretically if we can deploy on Minikube for testing, we should be able to deploy to an existing Kubernetes cluster as well.\r\nhttps://docs.qhub.dev/en/latest/source/06_developers_contrib_guide/04_tests.html#local-testing\r\n\r\n@djhoese raised some issues he had trying to do the same, here are his comments:\r\n\r\n>@aktech do you want me to just tell you about issues here? I decided to ignore \"actual\" work and play with qhub. Here's what I'm doing:\r\n> - git clone of main qhub. Running on conda-forge based python 3.8 environment.\r\n> - Ran the \"qhub init\" command with my own project name (does this reflect a project on the cluster) and a made up domain > as my work's cluster is behind a proxy at the moment (no public IP addresses).\r\n> - I have my KUBECONFIG setup with a context that has the proper tokens to talk directly to the cluster via kubectl. I did some grepping and thought I could add kube_context: mycontext in my qhub-config.yaml but then got an error about an extra key in the YAML when I tried to deploy.\r\n> - I commented out the kube_context key and then got an error when deploying because the namespace terraform wants to create already exists. I don't think I have permission to create a namespace outside of the administration webUI for my cluster (I'll try in a bit).\r\n\r\n>Looks like some of the terraform stuff could be helped if I used: https://registry.terraform.io/providers/rancher/rke/latest but I have very very very little terraform experience\r\n\r\n### Complications faced by him\r\n\r\nSome complicates with me trying to do this:\r\n\r\n> I do not have full permissions on this cluster. We are experimenting with giving people a decent level of permissions in their own namespace(s), but beyond that it won't be allowed.\r\n\r\n> I'll have to check with the sys admin, but this particular cluster may not have an ingress controller installed. I ran terraform planand got some errors that seem a little ingress related:\r\n\r\n\r\n```\r\n(qhub) davidh@janet:~/repos/git/qhub/infrastructure$ /tmp/terraform/0.14.9/terraform plan\r\n\r\nError: Failed to determine GroupVersionResource for manifest\r\n\r\n  on .terraform/modules/qhub/modules/kubernetes/services/dask-gateway/middleware.tf line 1, in resource \"kubernetes_manifest\" \"gateway-middleware\":\r\n   1: resource \"kubernetes_manifest\" \"gateway-middleware\" {\r\n\r\nno matches for kind \"Middleware\" in group \"traefik.containo.us\"\r\n\r\n\r\nError: Failed to determine GroupVersionResource for manifest\r\n\r\n  on .terraform/modules/qhub/modules/kubernetes/services/dask-gateway/middleware.tf line 22, in resource \"kubernetes_manifest\" \"cluster-middleware\":\r\n  22: resource \"kubernetes_manifest\" \"cluster-middleware\" {\r\n\r\nno matches for kind \"Middleware\" in group \"traefik.containo.us\"\r\n\r\n```",
    "actual_labels": [
      "bug",
      "configuration",
      "question"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\n```",
    "model_name": "base"
  },
  {
    "index": 119,
    "title": "Deployment Issues on Existing Kubernetes cluster",
    "body": "Theoretically if we can deploy on Minikube for testing, we should be able to deploy to an existing Kubernetes cluster as well.\r\nhttps://docs.qhub.dev/en/latest/source/06_developers_contrib_guide/04_tests.html#local-testing\r\n\r\n@djhoese raised some issues he had trying to do the same, here are his comments:\r\n\r\n>@aktech do you want me to just tell you about issues here? I decided to ignore \"actual\" work and play with qhub. Here's what I'm doing:\r\n> - git clone of main qhub. Running on conda-forge based python 3.8 environment.\r\n> - Ran the \"qhub init\" command with my own project name (does this reflect a project on the cluster) and a made up domain > as my work's cluster is behind a proxy at the moment (no public IP addresses).\r\n> - I have my KUBECONFIG setup with a context that has the proper tokens to talk directly to the cluster via kubectl. I did some grepping and thought I could add kube_context: mycontext in my qhub-config.yaml but then got an error about an extra key in the YAML when I tried to deploy.\r\n> - I commented out the kube_context key and then got an error when deploying because the namespace terraform wants to create already exists. I don't think I have permission to create a namespace outside of the administration webUI for my cluster (I'll try in a bit).\r\n\r\n>Looks like some of the terraform stuff could be helped if I used: https://registry.terraform.io/providers/rancher/rke/latest but I have very very very little terraform experience\r\n\r\n### Complications faced by him\r\n\r\nSome complicates with me trying to do this:\r\n\r\n> I do not have full permissions on this cluster. We are experimenting with giving people a decent level of permissions in their own namespace(s), but beyond that it won't be allowed.\r\n\r\n> I'll have to check with the sys admin, but this particular cluster may not have an ingress controller installed. I ran terraform planand got some errors that seem a little ingress related:\r\n\r\n\r\n```\r\n(qhub) davidh@janet:~/repos/git/qhub/infrastructure$ /tmp/terraform/0.14.9/terraform plan\r\n\r\nError: Failed to determine GroupVersionResource for manifest\r\n\r\n  on .terraform/modules/qhub/modules/kubernetes/services/dask-gateway/middleware.tf line 1, in resource \"kubernetes_manifest\" \"gateway-middleware\":\r\n   1: resource \"kubernetes_manifest\" \"gateway-middleware\" {\r\n\r\nno matches for kind \"Middleware\" in group \"traefik.containo.us\"\r\n\r\n\r\nError: Failed to determine GroupVersionResource for manifest\r\n\r\n  on .terraform/modules/qhub/modules/kubernetes/services/dask-gateway/middleware.tf line 22, in resource \"kubernetes_manifest\" \"cluster-middleware\":\r\n  22: resource \"kubernetes_manifest\" \"cluster-middleware\" {\r\n\r\nno matches for kind \"Middleware\" in group \"traefik.containo.us\"\r\n\r\n```",
    "actual_labels": [
      "bug",
      "configuration",
      "question"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 120,
    "title": "[ENH] - Enable IRSA on Nebari AWS deployments",
    "body": "### Feature description\n\nNebari should configure the OIDC provider and other resources required to enable use of the [IAM Roles for Service Accounts (IRSA)](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) feature for EKS clusters.\n\n### Value and/or benefit\n\nCurrently, enabling access to S3 or other IAM policy managed AWS resources requires either 1) attaching the policy to the node group role or 2) injecting access keys in the application environment. Neither is ideal for security purposes as keys risk exposure through Jupyter notebooks and node group polices are cluster wide, so all workloads share access to everything.\r\n\r\nEnabling IRSA would allow us to create individual role/policies and attach to Kubernetes service accounts so different workloads can employ least privilege to the extent possible.\n\n### Anything else?\n\nThe [Terraform EKS module](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master) handles this automatically, but upgrading to use this would be a challenge for maintaining the upgrade path. Extracting the relevant code from the EKS module [source code](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/666603b6e531140d5d8fbd777cd90a7fbb8247dd/main.tf#L219) would be relatively simple with minimal risk.",
    "actual_labels": [
      "enhancement",
      "aws",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 120,
    "title": "[ENH] - Enable IRSA on Nebari AWS deployments",
    "body": "### Feature description\n\nNebari should configure the OIDC provider and other resources required to enable use of the [IAM Roles for Service Accounts (IRSA)](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html) feature for EKS clusters.\n\n### Value and/or benefit\n\nCurrently, enabling access to S3 or other IAM policy managed AWS resources requires either 1) attaching the policy to the node group role or 2) injecting access keys in the application environment. Neither is ideal for security purposes as keys risk exposure through Jupyter notebooks and node group polices are cluster wide, so all workloads share access to everything.\r\n\r\nEnabling IRSA would allow us to create individual role/policies and attach to Kubernetes service accounts so different workloads can employ least privilege to the extent possible.\n\n### Anything else?\n\nThe [Terraform EKS module](https://github.com/terraform-aws-modules/terraform-aws-eks/tree/master) handles this automatically, but upgrading to use this would be a challenge for maintaining the upgrade path. Extracting the relevant code from the EKS module [source code](https://github.com/terraform-aws-modules/terraform-aws-eks/blob/666603b6e531140d5d8fbd777cd90a7fbb8247dd/main.tf#L219) would be relatively simple with minimal risk.",
    "actual_labels": [
      "enhancement",
      "aws",
      "security",
      "configuration"
    ],
    "predicted_labels": "enhancement, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 121,
    "title": "Meta issue for checking provider dependencies",
    "body": "This issue is related to #1234, based on the recent discussion we should validate that all of the current cloud providers have their configurations explicitly defined, e.g adding all `available` information we can arrange for that given resource/provider for example inspecting if AWS has the region defined in all resources/dependencies (even if its an options setting) -- this will then allows us to get better control over the resources/providers settings as well as mitigate future encounters of unexpected behaviors.\r\n\r\n### Tasks:\r\n- Validate `region` is correctly assigned to all AWS dependencies;\r\n- Inspect if there is missing optional values that could have been set to all providers",
    "actual_labels": [
      "aws",
      "configuration",
      "maintenance"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 121,
    "title": "Meta issue for checking provider dependencies",
    "body": "This issue is related to #1234, based on the recent discussion we should validate that all of the current cloud providers have their configurations explicitly defined, e.g adding all `available` information we can arrange for that given resource/provider for example inspecting if AWS has the region defined in all resources/dependencies (even if its an options setting) -- this will then allows us to get better control over the resources/providers settings as well as mitigate future encounters of unexpected behaviors.\r\n\r\n### Tasks:\r\n- Validate `region` is correctly assigned to all AWS dependencies;\r\n- Inspect if there is missing optional values that could have been set to all providers",
    "actual_labels": [
      "aws",
      "configuration",
      "maintenance"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 122,
    "title": "Testing checklist for 2024.3.2",
    "body": "# Testing Checklist\r\n\r\n_Use/modify this checklist to capture the platform's core services that need to be manually tested ._\r\n\r\n## Manual testing: core services\r\n\r\nIf the integration tests for all of the cloud providers are successful, that is a good sign!\r\nHowever, the following core services still need to be manually validated (until we can automate them).\r\n\r\nAt minimum, the following services will need to be tested:\r\n\r\n- [x] [Log into keycloak as root user](https://www.nebari.dev/docs/how-tos/configuring-keycloak/#change-keycloak-root-password)\r\n  - [x] [Add a user](https://www.nebari.dev/docs/how-tos/configuring-keycloak/#adding-a-nebari-user)\r\n- [x] [Log into conda-store and create](https://www.nebari.dev/docs/tutorials/creating-new-environments)\r\n  - [x] a conda environment in a shared namespace and,\r\n  - [x] a conda environment in your personal namespace\r\n- [x] [Launch dask-gateway cluster, test auto-scaler and](https://www.nebari.dev/docs/tutorials/using_dask)\r\n  - [x] [Validate that the dask-labextention is working](https://www.nebari.dev/docs/tutorials/using_dask/#step-4---understand-dasks-diagnostic-tools)\r\n- [x] [Confirm that a notebook can be submitted via Jupyter-Scheduler](https://nebari.dev/docs/tutorials/jupyter-scheduler)\r\n- [x] [Open VS-Code extension](https://www.nebari.dev/docs/how-tos/using-vscode)\r\n  - [x] [Add the Python extension](https://www.nebari.dev/docs/how-tos/using-vscode#adding-extensions)\r\n  - [x] [Create a `.py` file and run it](https://www.nebari.dev/docs/how-tos/using-vscode#running-python-code)\r\n",
    "actual_labels": [
      "testing",
      "release"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 122,
    "title": "Testing checklist for 2024.3.2",
    "body": "# Testing Checklist\r\n\r\n_Use/modify this checklist to capture the platform's core services that need to be manually tested ._\r\n\r\n## Manual testing: core services\r\n\r\nIf the integration tests for all of the cloud providers are successful, that is a good sign!\r\nHowever, the following core services still need to be manually validated (until we can automate them).\r\n\r\nAt minimum, the following services will need to be tested:\r\n\r\n- [x] [Log into keycloak as root user](https://www.nebari.dev/docs/how-tos/configuring-keycloak/#change-keycloak-root-password)\r\n  - [x] [Add a user](https://www.nebari.dev/docs/how-tos/configuring-keycloak/#adding-a-nebari-user)\r\n- [x] [Log into conda-store and create](https://www.nebari.dev/docs/tutorials/creating-new-environments)\r\n  - [x] a conda environment in a shared namespace and,\r\n  - [x] a conda environment in your personal namespace\r\n- [x] [Launch dask-gateway cluster, test auto-scaler and](https://www.nebari.dev/docs/tutorials/using_dask)\r\n  - [x] [Validate that the dask-labextention is working](https://www.nebari.dev/docs/tutorials/using_dask/#step-4---understand-dasks-diagnostic-tools)\r\n- [x] [Confirm that a notebook can be submitted via Jupyter-Scheduler](https://nebari.dev/docs/tutorials/jupyter-scheduler)\r\n- [x] [Open VS-Code extension](https://www.nebari.dev/docs/how-tos/using-vscode)\r\n  - [x] [Add the Python extension](https://www.nebari.dev/docs/how-tos/using-vscode#adding-extensions)\r\n  - [x] [Create a `.py` file and run it](https://www.nebari.dev/docs/how-tos/using-vscode#running-python-code)\r\n",
    "actual_labels": [
      "testing",
      "release"
    ],
    "predicted_labels": "testing, ci/cd, testing",
    "model_name": "fine_tuned"
  },
  {
    "index": 123,
    "title": "[ENH] - Programmatically added example repos cannot be deleted",
    "body": "### Feature description\n\nIf I try to delete a repo that was programmatically created via \r\n```\r\njupyterlab:\r\n  initial_repositories:\r\n    - <path/to/folder>: <github/repository/url>\r\n```\r\n\r\nI get a permission error: `rm: cannot remove 'examples/jatic-i3-demo': Permission denied`\r\n\r\n\n\n### Value and/or benefit\n\nThis is an issue since the only way to get an updated example repo is to remove the repo folder. If I can't remove the folder, I can't pull an updated copy of the examples. \n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 123,
    "title": "[ENH] - Programmatically added example repos cannot be deleted",
    "body": "### Feature description\n\nIf I try to delete a repo that was programmatically created via \r\n```\r\njupyterlab:\r\n  initial_repositories:\r\n    - <path/to/folder>: <github/repository/url>\r\n```\r\n\r\nI get a permission error: `rm: cannot remove 'examples/jatic-i3-demo': Permission denied`\r\n\r\n\n\n### Value and/or benefit\n\nThis is an issue since the only way to get an updated example repo is to remove the repo folder. If I can't remove the folder, I can't pull an updated copy of the examples. \n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 124,
    "title": "[ENH] - Missing env vars for Argo",
    "body": "### Feature description\n\nNebari currently sets the `ARGO_TOKEN` as an env var on startup, but addition env vars are required for Argo to work. \r\n\r\n```\r\nos.environ['ARGO_HTTP1'] = \"true\"\r\nos.environ['ARGO_SECURE'] = \"true\"\r\nos.environ['KUBECONFIG'] = '/dev/null'\r\nos.environ['HERA_TOKEN'] = os.environ['ARGO_TOKEN'] \r\nos.environ['GLOBAL_CONFIG_HOST'] = f\"https://{os.environ['ARGO_SERVER'].rsplit(':')[0]}{os.environ['ARGO_BASE_HREF']}/\"  # trailing slash required for proper urljoin inside of hera\r\nos.environ['GLOBAL_CONFIG_NAMESPACE'] = os.environ['ARGO_NAMESPACE']\r\n```\n\n### Value and/or benefit\n\nSetting these for the user will allow for a lower barrier to entry, minimized setup, and I believe was the original intent of setting the Argo token for the users. \r\n\r\n\n\n### Anything else?\n\nA minimal functioning example for testing:\r\n\r\n```\r\nimport os\r\nfrom hera.shared import global_config\r\n\r\nos.environ['ARGO_HTTP1'] = \"true\"\r\nos.environ['ARGO_SECURE'] = \"true\"\r\nos.environ['KUBECONFIG'] = '/dev/null'\r\nos.environ['HERA_TOKEN'] = os.environ['ARGO_TOKEN'] \r\nos.environ['GLOBAL_CONFIG_HOST'] = f\"https://{os.environ['ARGO_SERVER'].rsplit(':')[0]}{os.environ['ARGO_BASE_HREF']}/\"  # trailing slash required for proper urljoin inside of hera\r\nos.environ['GLOBAL_CONFIG_NAMESPACE'] = os.environ['ARGO_NAMESPACE']\r\n\r\nglobal_config.host = os.environ['GLOBAL_CONFIG_HOST']\r\nglobal_config.token = os.environ['HERA_TOKEN'] \r\n\r\n\r\nfrom hera.workflows import Steps, Workflow, script\r\n\r\n@script()\r\ndef echo(message: str):\r\n    print(message)\r\n\r\n\r\nwith Workflow(\r\n    generate_name=\"hello-world-hera-\",\r\n    entrypoint=\"steps\",\r\n    namespace=os.environ['GLOBAL_CONFIG_NAMESPACE'],\r\n) as w:\r\n    with Steps(name=\"steps\"):\r\n        echo(arguments={\"message\": \"Hello world!\"})\r\n\r\nw.create()\r\n```",
    "actual_labels": [
      "enhancement",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "```python\nimport os\nfrom hera.workflows import Steps, Workflow, script\nfrom hera.shared import global_config\nfrom hera.inputs import Steps\nfrom hera.inputs import Workflow\nfrom hera.inputs import",
    "model_name": "base"
  },
  {
    "index": 124,
    "title": "[ENH] - Missing env vars for Argo",
    "body": "### Feature description\n\nNebari currently sets the `ARGO_TOKEN` as an env var on startup, but addition env vars are required for Argo to work. \r\n\r\n```\r\nos.environ['ARGO_HTTP1'] = \"true\"\r\nos.environ['ARGO_SECURE'] = \"true\"\r\nos.environ['KUBECONFIG'] = '/dev/null'\r\nos.environ['HERA_TOKEN'] = os.environ['ARGO_TOKEN'] \r\nos.environ['GLOBAL_CONFIG_HOST'] = f\"https://{os.environ['ARGO_SERVER'].rsplit(':')[0]}{os.environ['ARGO_BASE_HREF']}/\"  # trailing slash required for proper urljoin inside of hera\r\nos.environ['GLOBAL_CONFIG_NAMESPACE'] = os.environ['ARGO_NAMESPACE']\r\n```\n\n### Value and/or benefit\n\nSetting these for the user will allow for a lower barrier to entry, minimized setup, and I believe was the original intent of setting the Argo token for the users. \r\n\r\n\n\n### Anything else?\n\nA minimal functioning example for testing:\r\n\r\n```\r\nimport os\r\nfrom hera.shared import global_config\r\n\r\nos.environ['ARGO_HTTP1'] = \"true\"\r\nos.environ['ARGO_SECURE'] = \"true\"\r\nos.environ['KUBECONFIG'] = '/dev/null'\r\nos.environ['HERA_TOKEN'] = os.environ['ARGO_TOKEN'] \r\nos.environ['GLOBAL_CONFIG_HOST'] = f\"https://{os.environ['ARGO_SERVER'].rsplit(':')[0]}{os.environ['ARGO_BASE_HREF']}/\"  # trailing slash required for proper urljoin inside of hera\r\nos.environ['GLOBAL_CONFIG_NAMESPACE'] = os.environ['ARGO_NAMESPACE']\r\n\r\nglobal_config.host = os.environ['GLOBAL_CONFIG_HOST']\r\nglobal_config.token = os.environ['HERA_TOKEN'] \r\n\r\n\r\nfrom hera.workflows import Steps, Workflow, script\r\n\r\n@script()\r\ndef echo(message: str):\r\n    print(message)\r\n\r\n\r\nwith Workflow(\r\n    generate_name=\"hello-world-hera-\",\r\n    entrypoint=\"steps\",\r\n    namespace=os.environ['GLOBAL_CONFIG_NAMESPACE'],\r\n) as w:\r\n    with Steps(name=\"steps\"):\r\n        echo(arguments={\"message\": \"Hello world!\"})\r\n\r\nw.create()\r\n```",
    "actual_labels": [
      "enhancement",
      "configuration",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, documentation, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 125,
    "title": "Scheduler pod for dask is also a preemptible instance on GCP ",
    "body": "When using preemptible instances with dask, the scheduler pod is also a preemptible instance type.\r\n\r\nWhile dask can handle some workers being killed, the scheduler instance being killed would cause all progress in the job to fail.\r\n",
    "actual_labels": [
      "bug",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, dask, scheduler",
    "model_name": "base"
  },
  {
    "index": 125,
    "title": "Scheduler pod for dask is also a preemptible instance on GCP ",
    "body": "When using preemptible instances with dask, the scheduler pod is also a preemptible instance type.\r\n\r\nWhile dask can handle some workers being killed, the scheduler instance being killed would cause all progress in the job to fail.\r\n",
    "actual_labels": [
      "bug",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 126,
    "title": "Migrate to new Keycloak terraform provider",
    "body": "### Context\n\nKeycloak [adopted]( https://www.keycloak.org/2024/12/terraform-provider-adoption) the mrparkers/keycloak provider we were using.  We should migrate to the officially supported one - https://github.com/keycloak/terraform-provider-keycloak.\n\n\n### Value and/or benefit\n\nOfficial support by Keycloak\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "maintenance",
      "dependency-update",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, case study, keycloak/terraform-provider-keycloak,",
    "model_name": "base"
  },
  {
    "index": 126,
    "title": "Migrate to new Keycloak terraform provider",
    "body": "### Context\n\nKeycloak [adopted]( https://www.keycloak.org/2024/12/terraform-provider-adoption) the mrparkers/keycloak provider we were using.  We should migrate to the officially supported one - https://github.com/keycloak/terraform-provider-keycloak.\n\n\n### Value and/or benefit\n\nOfficial support by Keycloak\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "maintenance",
      "dependency-update",
      "configuration"
    ],
    "predicted_labels": "enhancement, maintenance, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 127,
    "title": "Remove files related to cypress",
    "body": "",
    "actual_labels": [
      "maintenance",
      "testing"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 127,
    "title": "Remove files related to cypress",
    "body": "",
    "actual_labels": [
      "maintenance",
      "testing"
    ],
    "predicted_labels": "enhancement, maintenance, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 128,
    "title": "[BUG] - 0.4 QHub render does not product .gitignore file to ignore terraform produced downloads ",
    "body": "### OS system and architecture in which you are running QHub\n\nLinux / AWS \n\n### Expected behavior\n\nclean redeploy with github actions\n\n### Actual behavior\n\nremote: error: Trace: f7ee2b2e8ee572afea92b84d8b5108190bbd4a9a708bf89976c48eaa9ce39290        \r\nremote: error: See http://git.io/iEPt8g for more information.        \r\nremote: error: File stages/01-terraform-state/aws/.terraform/providers/registry.terraform.io/hashicorp/aws/3.73.0/linux_amd64/terraform-provider-aws_v3.73.0_x5 is 236.41 MB; this exceeds GitHub's file size limit of 100.00 MB        \r\nremote: error: File stages/03-kubernetes-initialize/.terraform/providers/registry.terraform.io/hashicorp/aws/4.7.0/linux_amd64/terraform-provider-aws_v4.7.0_x5 is 238.45 MB; this exceeds GitHub's file size limit of 100.00 MB        \r\nremote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com./        \r\nTo https://github.com/ESIPFed/qhub-maple\r\n ! [remote rejected] main -> main (pre-receive hook declined)\r\nerror: failed to push some refs to 'https://github.com/ESIPFed/qhub-maple'\r\nError: Process completed with exit code 1.\n\n### How to Reproduce the problem?\n\nnot sure\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "ci/cd",
      "configuration",
      "aws"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 128,
    "title": "[BUG] - 0.4 QHub render does not product .gitignore file to ignore terraform produced downloads ",
    "body": "### OS system and architecture in which you are running QHub\n\nLinux / AWS \n\n### Expected behavior\n\nclean redeploy with github actions\n\n### Actual behavior\n\nremote: error: Trace: f7ee2b2e8ee572afea92b84d8b5108190bbd4a9a708bf89976c48eaa9ce39290        \r\nremote: error: See http://git.io/iEPt8g for more information.        \r\nremote: error: File stages/01-terraform-state/aws/.terraform/providers/registry.terraform.io/hashicorp/aws/3.73.0/linux_amd64/terraform-provider-aws_v3.73.0_x5 is 236.41 MB; this exceeds GitHub's file size limit of 100.00 MB        \r\nremote: error: File stages/03-kubernetes-initialize/.terraform/providers/registry.terraform.io/hashicorp/aws/4.7.0/linux_amd64/terraform-provider-aws_v4.7.0_x5 is 238.45 MB; this exceeds GitHub's file size limit of 100.00 MB        \r\nremote: error: GH001: Large files detected. You may want to try Git Large File Storage - https://git-lfs.github.com./        \r\nTo https://github.com/ESIPFed/qhub-maple\r\n ! [remote rejected] main -> main (pre-receive hook declined)\r\nerror: failed to push some refs to 'https://github.com/ESIPFed/qhub-maple'\r\nError: Process completed with exit code 1.\n\n### How to Reproduce the problem?\n\nnot sure\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "ci/cd",
      "configuration",
      "aws"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 129,
    "title": "[BUG] - nebari upgrade incorrect image matching on profiles.dask_worker",
    "body": "### Describe the bug\r\n\r\n`nebari upgrade` is incorrectly expecting the `profiles.dask_worker` section to have a `kubespawner_override` subsection for image tag updates. Per the schema, the `image` value should be at the top level of the named profile.\r\n\r\n### Expected behavior\r\n\r\nCommand should successfully upgrade any matching image tags included in the `profiles.dask_worker` section to the current release.\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nUbuntu Linux, x64\r\n\r\n### How to Reproduce the problem?\r\n\r\nRun `nebari upgrade -c nebari-config.yaml` on a file with something like the following present:\r\n\r\nImage will be found in the following (incorrect) yaml:\r\n``` yaml\r\n\r\nprofiles:\r\n  dask_worker:\r\n    test:\r\n      kubespawner_override:\r\n        image: quay.io/nebari/nebari-jupyterlab:2023.4.1\r\n\r\n```\r\n\r\nImage will be missed in the following (correct) yaml:\r\n``` yaml\r\n\r\nprofiles:\r\n  dask_worker:\r\n    test:\r\n      image: quay.io/nebari/nebari-jupyterlab:2023.4.1\r\n\r\n```\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\nNebari: 2023.7.2.dev\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 129,
    "title": "[BUG] - nebari upgrade incorrect image matching on profiles.dask_worker",
    "body": "### Describe the bug\r\n\r\n`nebari upgrade` is incorrectly expecting the `profiles.dask_worker` section to have a `kubespawner_override` subsection for image tag updates. Per the schema, the `image` value should be at the top level of the named profile.\r\n\r\n### Expected behavior\r\n\r\nCommand should successfully upgrade any matching image tags included in the `profiles.dask_worker` section to the current release.\r\n\r\n### OS and architecture in which you are running Nebari\r\n\r\nUbuntu Linux, x64\r\n\r\n### How to Reproduce the problem?\r\n\r\nRun `nebari upgrade -c nebari-config.yaml` on a file with something like the following present:\r\n\r\nImage will be found in the following (incorrect) yaml:\r\n``` yaml\r\n\r\nprofiles:\r\n  dask_worker:\r\n    test:\r\n      kubespawner_override:\r\n        image: quay.io/nebari/nebari-jupyterlab:2023.4.1\r\n\r\n```\r\n\r\nImage will be missed in the following (correct) yaml:\r\n``` yaml\r\n\r\nprofiles:\r\n  dask_worker:\r\n    test:\r\n      image: quay.io/nebari/nebari-jupyterlab:2023.4.1\r\n\r\n```\r\n\r\n### Command output\r\n\r\n_No response_\r\n\r\n### Versions and dependencies used.\r\n\r\nNebari: 2023.7.2.dev\r\n\r\n### Compute environment\r\n\r\nNone\r\n\r\n### Integrations\r\n\r\n_No response_\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "bug",
      "configuration"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 130,
    "title": "Fix code scanning alert - The Kubernetes cluster does not enable surge upgrades",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/62\r\n",
    "actual_labels": [
      "bug",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 130,
    "title": "Fix code scanning alert - The Kubernetes cluster does not enable surge upgrades",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/62\r\n",
    "actual_labels": [
      "bug",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, security, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 131,
    "title": "[BUG] - GPUs unavailable on AWS due to unscheduled nvidia-device-plugin DaemonSet",
    "body": "### Describe the bug\n\nNode taints for node groups were introduced in [#2605](https://github.com/nebari-dev/nebari/pull/2605) to enable scale-to-zero functionality. As @Adam-D-Lewis [correctly pointed out](https://github.com/nebari-dev/nebari/pull/2605#issuecomment-2297568799), this change prevents certain DaemonSets from being scheduled on tainted nodes unless they explicitly tolerate the taint. One such affected DaemonSet is `aws_nvidia_installer`, which currently only includes the following tolerations: https://github.com/nebari-dev/nebari/blob/06f527dc2d61c6943a5d9e7387785523e6382e47/src/_nebari/stages/kubernetes_initialize/template/modules/nvidia-installer/aws-nvidia-installer.tf#L61-L70)\n\nAs a result, the NVIDIA driver installer is never scheduled on GPU nodes, leaving the drivers uninstalled and the GPUs effectively unusable.\n\n### Expected behavior\n\nThe `aws_nvidia_installer` DaemonSet should be scheduled on all GPU nodes, regardless of taints introduced for scale-to-zero functionality. This ensures that the NVIDIA drivers are installed automatically on GPU instances, making GPUs available for workloads.\n\n### OS and architecture in which you are running Nebari\n\nNot relevant\n\n### How to Reproduce the problem?\n\n1. Deploy Nebari to AWS, using the following node group configuration:\n```yaml\nnebari_version: 2025.6.1\n# ...\namazon_web_services:\n  # ...\n  node_groups:\n    # ...\n    gpu-t4-x1:\n      instance: g4dn.xlarge\n      min_nodes: 0\n      max_nodes: 5\n      gpu: true\nprofiles:\n  jupyterlab:\n  - display_name: T4 GPU Instance\n    description: Stable environment with 4 cpu / 16 GB ram and 1 T4 GPU / 16 GB GPU memory\n    kubespawner_override:\n      cpu_limit: 4\n      cpu_guarantee: 3\n      mem_limit: 16G\n      mem_guarantee: 10G\n      extra_resource_limits:\n        nvidia.com/gpu: 1\n      image: quay.io/nebari/nebari-jupyterlab-gpu:2025.6.1\n      node_selector:\n        \"dedicated\": \"gpu-t4-x1\"\n```\n2. Launch a JupyterLab server with the T4 GPU instance profile\n3. Run the following command from a terminal with access to the cluster:\n```bash\nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n```\n\nThe output will look like this:\n```\nNAME                                         GPU\nip-10-10-30-220.us-west-2.compute.internal   <none>\nip-10-10-49-2.us-west-2.compute.internal     <none>  # Should be 1!\n```\n\n4. From a terminal in the JupyterLab server, run the following command:\n```bash\nnvidia-smi\n```\n\nThis will result in `bash: nvidia-smi: command not found`, when it should have printed GPU information.\n\n### Command output\n\n```bash session\n\n```\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nAWS\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n\nAllowed labels with descriptions",
    "model_name": "base"
  },
  {
    "index": 131,
    "title": "[BUG] - GPUs unavailable on AWS due to unscheduled nvidia-device-plugin DaemonSet",
    "body": "### Describe the bug\n\nNode taints for node groups were introduced in [#2605](https://github.com/nebari-dev/nebari/pull/2605) to enable scale-to-zero functionality. As @Adam-D-Lewis [correctly pointed out](https://github.com/nebari-dev/nebari/pull/2605#issuecomment-2297568799), this change prevents certain DaemonSets from being scheduled on tainted nodes unless they explicitly tolerate the taint. One such affected DaemonSet is `aws_nvidia_installer`, which currently only includes the following tolerations: https://github.com/nebari-dev/nebari/blob/06f527dc2d61c6943a5d9e7387785523e6382e47/src/_nebari/stages/kubernetes_initialize/template/modules/nvidia-installer/aws-nvidia-installer.tf#L61-L70)\n\nAs a result, the NVIDIA driver installer is never scheduled on GPU nodes, leaving the drivers uninstalled and the GPUs effectively unusable.\n\n### Expected behavior\n\nThe `aws_nvidia_installer` DaemonSet should be scheduled on all GPU nodes, regardless of taints introduced for scale-to-zero functionality. This ensures that the NVIDIA drivers are installed automatically on GPU instances, making GPUs available for workloads.\n\n### OS and architecture in which you are running Nebari\n\nNot relevant\n\n### How to Reproduce the problem?\n\n1. Deploy Nebari to AWS, using the following node group configuration:\n```yaml\nnebari_version: 2025.6.1\n# ...\namazon_web_services:\n  # ...\n  node_groups:\n    # ...\n    gpu-t4-x1:\n      instance: g4dn.xlarge\n      min_nodes: 0\n      max_nodes: 5\n      gpu: true\nprofiles:\n  jupyterlab:\n  - display_name: T4 GPU Instance\n    description: Stable environment with 4 cpu / 16 GB ram and 1 T4 GPU / 16 GB GPU memory\n    kubespawner_override:\n      cpu_limit: 4\n      cpu_guarantee: 3\n      mem_limit: 16G\n      mem_guarantee: 10G\n      extra_resource_limits:\n        nvidia.com/gpu: 1\n      image: quay.io/nebari/nebari-jupyterlab-gpu:2025.6.1\n      node_selector:\n        \"dedicated\": \"gpu-t4-x1\"\n```\n2. Launch a JupyterLab server with the T4 GPU instance profile\n3. Run the following command from a terminal with access to the cluster:\n```bash\nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n```\n\nThe output will look like this:\n```\nNAME                                         GPU\nip-10-10-30-220.us-west-2.compute.internal   <none>\nip-10-10-49-2.us-west-2.compute.internal     <none>  # Should be 1!\n```\n\n4. From a terminal in the JupyterLab server, run the following command:\n```bash\nnvidia-smi\n```\n\nThis will result in `bash: nvidia-smi: command not found`, when it should have printed GPU information.\n\n### Command output\n\n```bash session\n\n```\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nAWS\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, aws, performance, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 132,
    "title": "Rest endpoints for crud operations of cluster resources",
    "body": "",
    "actual_labels": [
      "enhancement"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 132,
    "title": "Rest endpoints for crud operations of cluster resources",
    "body": "",
    "actual_labels": [
      "enhancement"
    ],
    "predicted_labels": "enhancement, maintenance, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 133,
    "title": "Volume mount affinity issues: Volume in different zone than server",
    "body": "This is the case with AWS. Imagine the following situation\r\n\r\n## Nodes\r\nWe have two nodes in following zones:\r\ngeneral -  `us-east-2a`\r\nuser_worker - `us-east-2b`\r\n\r\n## Volume\r\n\r\nNow imagine the situation when the volume mounts get deployed in `us-east-2b`\r\n\r\n## Server\r\n\r\nThe conda-store server is set to be deployed on general node:\r\n\r\nhttps://github.com/Quansight/qhub/blob/2dd321a0a5c56672398df734e4b63dc8da053e3c/qhub/template/%7B%7B%20cookiecutter.repo_directory%20%7D%7D/infrastructure/kubernetes.tf#L87-L115\r\n\r\nNow in this case conda-store-server will be stuck, because:\r\n- general (`us-east-2a`): It cannot be deployed in general node becase the volume mounts are in `us-east-2b`\r\n- user_worker (`us-east-2b`): It cannot be deployed in user_worker node because its affinity is set to general node.\r\n\r\nHence the following issue (for `dev/qhub-conda-store` pod):\r\n\r\n```\r\n\u2502 Events:                                                                                                                                                                                                                                    \u2502\r\n\u2502   Type     Reason             Age                  From                Message                                                                                                                                                             \u2502\r\n\u2502   ----     ------             ----                 ----                -------                                                                                                                                                             \u2502\r\n\u2502   Normal   NotTriggerScaleUp  96s (x208 over 46m)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) had no available volume zone, 1 node(s) didn't match node selector                  \u2502\r\n\u2502   Warning  FailedScheduling   36s (x34 over 46m)   default-scheduler   0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had volume node affinity conflict.\r\n```\r\n\r\n\r\nSame with `hub` pod:\r\n\r\n```\r\n\u2502 Events:                                                                                                                                                                                                                                    \u2502\r\n\u2502   Type     Reason             Age                    From                Message                                                                                                                                                           \u2502\r\n\u2502   ----     ------             ----                   ----                -------                                                                                                                                                           \u2502\r\n\u2502   Warning  FailedScheduling   48m (x2 over 48m)      default-scheduler   0/2 nodes are available: 1 Insufficient cpu, 1 node(s) had volume node affinity conflict.                                                                         \u2502\r\n\u2502   Normal   NotTriggerScaleUp  27m (x26 over 47m)     cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) didn't match node selector, 1 node(s) had no available volume zone                \u2502\r\n\u2502   Normal   NotTriggerScaleUp  2m48s (x219 over 47m)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) had no available volume zone, 1 node(s) didn't match node selector                \u2502\r\n\u2502   Warning  FailedScheduling   108s (x32 over 47m)    default-scheduler   0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had volume node affinity conflict.\r\n```",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n```",
    "model_name": "base"
  },
  {
    "index": 133,
    "title": "Volume mount affinity issues: Volume in different zone than server",
    "body": "This is the case with AWS. Imagine the following situation\r\n\r\n## Nodes\r\nWe have two nodes in following zones:\r\ngeneral -  `us-east-2a`\r\nuser_worker - `us-east-2b`\r\n\r\n## Volume\r\n\r\nNow imagine the situation when the volume mounts get deployed in `us-east-2b`\r\n\r\n## Server\r\n\r\nThe conda-store server is set to be deployed on general node:\r\n\r\nhttps://github.com/Quansight/qhub/blob/2dd321a0a5c56672398df734e4b63dc8da053e3c/qhub/template/%7B%7B%20cookiecutter.repo_directory%20%7D%7D/infrastructure/kubernetes.tf#L87-L115\r\n\r\nNow in this case conda-store-server will be stuck, because:\r\n- general (`us-east-2a`): It cannot be deployed in general node becase the volume mounts are in `us-east-2b`\r\n- user_worker (`us-east-2b`): It cannot be deployed in user_worker node because its affinity is set to general node.\r\n\r\nHence the following issue (for `dev/qhub-conda-store` pod):\r\n\r\n```\r\n\u2502 Events:                                                                                                                                                                                                                                    \u2502\r\n\u2502   Type     Reason             Age                  From                Message                                                                                                                                                             \u2502\r\n\u2502   ----     ------             ----                 ----                -------                                                                                                                                                             \u2502\r\n\u2502   Normal   NotTriggerScaleUp  96s (x208 over 46m)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) had no available volume zone, 1 node(s) didn't match node selector                  \u2502\r\n\u2502   Warning  FailedScheduling   36s (x34 over 46m)   default-scheduler   0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had volume node affinity conflict.\r\n```\r\n\r\n\r\nSame with `hub` pod:\r\n\r\n```\r\n\u2502 Events:                                                                                                                                                                                                                                    \u2502\r\n\u2502   Type     Reason             Age                    From                Message                                                                                                                                                           \u2502\r\n\u2502   ----     ------             ----                   ----                -------                                                                                                                                                           \u2502\r\n\u2502   Warning  FailedScheduling   48m (x2 over 48m)      default-scheduler   0/2 nodes are available: 1 Insufficient cpu, 1 node(s) had volume node affinity conflict.                                                                         \u2502\r\n\u2502   Normal   NotTriggerScaleUp  27m (x26 over 47m)     cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) didn't match node selector, 1 node(s) had no available volume zone                \u2502\r\n\u2502   Normal   NotTriggerScaleUp  2m48s (x219 over 47m)  cluster-autoscaler  pod didn't trigger scale-up (it wouldn't fit if a new node is added): 1 node(s) had no available volume zone, 1 node(s) didn't match node selector                \u2502\r\n\u2502   Warning  FailedScheduling   108s (x32 over 47m)    default-scheduler   0/2 nodes are available: 1 node(s) didn't match node selector, 1 node(s) had volume node affinity conflict.\r\n```",
    "actual_labels": [
      "bug",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, aws, performance, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 134,
    "title": "[enhancement] Optionally add RBAC permissions for AWS EKS cluster",
    "body": "## Description\r\nAfter deploying qhub, additional steps are necessary to grant RBAC permissions to other AWS administrators.\r\n\r\n## Suggestion\r\nIt would be convenient to have an option to grant permissions to other users.\r\nAnother, possibly better approach, would be to take a list of users in the config file and create an IAM group and then grant permissions to that group.\r\nThis approach seems likely to be potentially adaptable to providers other than AWS.\r\n\r\n## Context\r\nThis would help simplify the management of the cluster by ensuring designated IAM users have appropriate access to the cluster.\r\n\r\n@iameskild, @tylerpotts, can you provide additional details about how permissions need to be added currently?",
    "actual_labels": [
      "enhancement",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, cost,",
    "model_name": "base"
  },
  {
    "index": 134,
    "title": "[enhancement] Optionally add RBAC permissions for AWS EKS cluster",
    "body": "## Description\r\nAfter deploying qhub, additional steps are necessary to grant RBAC permissions to other AWS administrators.\r\n\r\n## Suggestion\r\nIt would be convenient to have an option to grant permissions to other users.\r\nAnother, possibly better approach, would be to take a list of users in the config file and create an IAM group and then grant permissions to that group.\r\nThis approach seems likely to be potentially adaptable to providers other than AWS.\r\n\r\n## Context\r\nThis would help simplify the management of the cluster by ensuring designated IAM users have appropriate access to the cluster.\r\n\r\n@iameskild, @tylerpotts, can you provide additional details about how permissions need to be added currently?",
    "actual_labels": [
      "enhancement",
      "aws",
      "configuration"
    ],
    "predicted_labels": "enhancement, aws, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 135,
    "title": "Readd traefik ingress resource support",
    "body": "Needed for clearml support along with a lot of helm charts that expect the ingress resource.",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 135,
    "title": "Readd traefik ingress resource support",
    "body": "Needed for clearml support along with a lot of helm charts that expect the ingress resource.",
    "actual_labels": [
      "enhancement",
      "configuration"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 136,
    "title": "Add premade dashboard to monitor Traefik [enhancement]",
    "body": "I'm looking into adding https://github.com/tcheronneau/grafana_dashboard to the qhub deployment for Traefik.  The dashboard doesn't work off the bat.  I'm still working on making the changes to get it working.",
    "actual_labels": [
      "enhancement",
      "performance",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 136,
    "title": "Add premade dashboard to monitor Traefik [enhancement]",
    "body": "I'm looking into adding https://github.com/tcheronneau/grafana_dashboard to the qhub deployment for Traefik.  The dashboard doesn't work off the bat.  I'm still working on making the changes to get it working.",
    "actual_labels": [
      "enhancement",
      "performance",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 137,
    "title": "[bug] dask_gateway image/Dockerfile not included during initial qhub deploy",
    "body": "## Describe the bug\r\nWhile deploying datum's new QHub on AWS, the first initial init and deploy ran successfully. However, the first push to Github resulted in a failed Docker Image Build workflow. The error messages are below, but here are the qhub commands:\r\n\r\n`qhub init aws --project datum-prod --domain datum.qhub.dev --ci-provider github-actions --auth-provider auth0 --auth-auto-provision --repository github.com/datum-tech/qhub-datum-aws-prod --repository-auto-provision`\r\n* domain has since been updated to `qhub.datumxy.com`\r\n\r\n`qhub deploy -c qhub-config.yaml --dns-auto-provision `\r\n* DNS registration was handled by a datum employee.\r\n\r\nFrom reading the documentation, specifically the section on [default images](https://docs.qhub.dev/en/latest/source/02_get_started/04_configuration.html#default-images) and comparing it to what was present in the qhub-config.yaml, it appeared that the `dask_gateway: <default image>` was missing. The `Dockerfile.dask-gateway` also appeared to be missing. \r\n\r\nOnce these were added, the Docker Image Build didn't restart but the other workflows ran successfully. And after a few minor, unrelated tweaks, the Qhub appears to be up and running. \r\n\r\n## How can we help?\r\nHelp us help you.\r\n* What are you trying to achieve?\r\n    * Reporting this bug for visibility. \r\n    * These default images still have not been pushed up to the ECR registry.\r\n* How can we reproduce the problem?\r\n    * Perhaps by trying to deploy a fresh QHub on AWS \r\n* What is the expected behaviour?\r\n* And what is currently happening?\r\n* Any error messages?\r\n    * These error messages are from the failed Docker Image Build GitHub workflow but seem somewhat unrelated to the issue described above:\r\n```\r\nname unknown: The repository with name 'datum-prod-dask-worker' does not exist in the registry with id '***'\r\nError: Process completed with exit code 1.\r\n```  \r\n> If helpful, please add any screenshots and relevant links to the description.\r\n\r\n## Your environment\r\nDescribe the environment in which you are experiencing the bug.\r\n> Include your conda version (use `conda --version`), k8s and any other relevant details.\r\n\r\n```\r\nconda 4.10.1\r\nqhub version 0.3.7\r\n# k8s\r\nClient Version: v1.20.2\r\nServer Version: v1.17.17-gke.3000\r\n```\r\n\r\n",
    "actual_labels": [
      "bug",
      "ci/cd",
      "aws",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n```",
    "model_name": "base"
  },
  {
    "index": 137,
    "title": "[bug] dask_gateway image/Dockerfile not included during initial qhub deploy",
    "body": "## Describe the bug\r\nWhile deploying datum's new QHub on AWS, the first initial init and deploy ran successfully. However, the first push to Github resulted in a failed Docker Image Build workflow. The error messages are below, but here are the qhub commands:\r\n\r\n`qhub init aws --project datum-prod --domain datum.qhub.dev --ci-provider github-actions --auth-provider auth0 --auth-auto-provision --repository github.com/datum-tech/qhub-datum-aws-prod --repository-auto-provision`\r\n* domain has since been updated to `qhub.datumxy.com`\r\n\r\n`qhub deploy -c qhub-config.yaml --dns-auto-provision `\r\n* DNS registration was handled by a datum employee.\r\n\r\nFrom reading the documentation, specifically the section on [default images](https://docs.qhub.dev/en/latest/source/02_get_started/04_configuration.html#default-images) and comparing it to what was present in the qhub-config.yaml, it appeared that the `dask_gateway: <default image>` was missing. The `Dockerfile.dask-gateway` also appeared to be missing. \r\n\r\nOnce these were added, the Docker Image Build didn't restart but the other workflows ran successfully. And after a few minor, unrelated tweaks, the Qhub appears to be up and running. \r\n\r\n## How can we help?\r\nHelp us help you.\r\n* What are you trying to achieve?\r\n    * Reporting this bug for visibility. \r\n    * These default images still have not been pushed up to the ECR registry.\r\n* How can we reproduce the problem?\r\n    * Perhaps by trying to deploy a fresh QHub on AWS \r\n* What is the expected behaviour?\r\n* And what is currently happening?\r\n* Any error messages?\r\n    * These error messages are from the failed Docker Image Build GitHub workflow but seem somewhat unrelated to the issue described above:\r\n```\r\nname unknown: The repository with name 'datum-prod-dask-worker' does not exist in the registry with id '***'\r\nError: Process completed with exit code 1.\r\n```  \r\n> If helpful, please add any screenshots and relevant links to the description.\r\n\r\n## Your environment\r\nDescribe the environment in which you are experiencing the bug.\r\n> Include your conda version (use `conda --version`), k8s and any other relevant details.\r\n\r\n```\r\nconda 4.10.1\r\nqhub version 0.3.7\r\n# k8s\r\nClient Version: v1.20.2\r\nServer Version: v1.17.17-gke.3000\r\n```\r\n\r\n",
    "actual_labels": [
      "bug",
      "ci/cd",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 138,
    "title": "END-GAME -  Close any outstanding milestones in Qhub repo",
    "body": "",
    "actual_labels": [
      "release",
      "maintenance"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 138,
    "title": "END-GAME -  Close any outstanding milestones in Qhub repo",
    "body": "",
    "actual_labels": [
      "release",
      "maintenance"
    ],
    "predicted_labels": "bug, release, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 139,
    "title": "Remove cookiecutter move towards terraform cdk",
    "body": "## Background\r\n\r\nThere are a long list of motivations towards moving away from cookie cutter detailed in https://github.com/Quansight/qhub/issues/327. Additionally cookiecutter makes it much harder to onboard new developers. We would like to use terraform cdk to generate the terraform deployment and use python to consider all the cases e.g. aws vs gcp deployment. This should also help with a lot of technical debt that we have in the project.\r\n\r\n## Expected timeline\r\n\r\n<!-- How long do you imagine this goal should be active for? This will determine the cadence with which we should check up on it to see how things are going. -->\r\n\r\n## Steps to complete this goal\r\n\r\n<!-- This can be an abstract end-point, a date, a checklist of issues, etc. Something that will let us know when to close the issue. -->\r\n<!-- Add links to related issues (in this repo or elsewhere) that are more actionable -->\r\n",
    "actual_labels": [
      "enhancement",
      "maintenance",
      "configuration",
      "aws",
      "gcp"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 139,
    "title": "Remove cookiecutter move towards terraform cdk",
    "body": "## Background\r\n\r\nThere are a long list of motivations towards moving away from cookie cutter detailed in https://github.com/Quansight/qhub/issues/327. Additionally cookiecutter makes it much harder to onboard new developers. We would like to use terraform cdk to generate the terraform deployment and use python to consider all the cases e.g. aws vs gcp deployment. This should also help with a lot of technical debt that we have in the project.\r\n\r\n## Expected timeline\r\n\r\n<!-- How long do you imagine this goal should be active for? This will determine the cadence with which we should check up on it to see how things are going. -->\r\n\r\n## Steps to complete this goal\r\n\r\n<!-- This can be an abstract end-point, a date, a checklist of issues, etc. Something that will let us know when to close the issue. -->\r\n<!-- Add links to related issues (in this repo or elsewhere) that are more actionable -->\r\n",
    "actual_labels": [
      "enhancement",
      "maintenance",
      "configuration",
      "aws",
      "gcp"
    ],
    "predicted_labels": "enhancement, maintenance, ci/cd, aws, gcp, azure, performance",
    "model_name": "fine_tuned"
  },
  {
    "index": 140,
    "title": "conda-store integration into QHub",
    "body": "## Description\r\n\r\nWould like to merge newest features of conda-store into QHub. Assigning to self. \r\n\r\n# Success Criteria\r\n\r\n - should work as well as before via conda-store\r\n - conda-store should be exposed via `/conda` endpoint.",
    "actual_labels": [
      "enhancement",
      "configuration",
      "dependency-update"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 140,
    "title": "conda-store integration into QHub",
    "body": "## Description\r\n\r\nWould like to merge newest features of conda-store into QHub. Assigning to self. \r\n\r\n# Success Criteria\r\n\r\n - should work as well as before via conda-store\r\n - conda-store should be exposed via `/conda` endpoint.",
    "actual_labels": [
      "enhancement",
      "configuration",
      "dependency-update"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 141,
    "title": "Meta issue for Conda-Store user experience ",
    "body": "Opening this issue to track all of the Conda-Store related user experience questions and issues. The goal is help users navigate the recent Conda-Store changes. Much of what is discussed below can be included in the docs eventually. \r\n\r\nQuestions:\r\n- [ ] When should new conda environments be created via the `nebari-config.yaml` and when should they be created from the `/conda-store` endpoint? And what difference does it make?\r\n- [ ] What are the steps to delete a conda environment I no longer need?\r\n- [ ] Add a feature that auto-deletes everything but the past 5 builds per env. \r\n\r\n\r\nKnown issues:\r\n- https://github.com/Quansight/qhub/issues/1158\r\n- https://github.com/Quansight/qhub/issues/1021\r\n  - related to how conda-store environments are used by CDS dashboards",
    "actual_labels": [
      "enhancement",
      "documentation",
      "question",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 141,
    "title": "Meta issue for Conda-Store user experience ",
    "body": "Opening this issue to track all of the Conda-Store related user experience questions and issues. The goal is help users navigate the recent Conda-Store changes. Much of what is discussed below can be included in the docs eventually. \r\n\r\nQuestions:\r\n- [ ] When should new conda environments be created via the `nebari-config.yaml` and when should they be created from the `/conda-store` endpoint? And what difference does it make?\r\n- [ ] What are the steps to delete a conda environment I no longer need?\r\n- [ ] Add a feature that auto-deletes everything but the past 5 builds per env. \r\n\r\n\r\nKnown issues:\r\n- https://github.com/Quansight/qhub/issues/1158\r\n- https://github.com/Quansight/qhub/issues/1021\r\n  - related to how conda-store environments are used by CDS dashboards",
    "actual_labels": [
      "enhancement",
      "documentation",
      "question",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 142,
    "title": "[ENH] - Ensure that default Dask Gateway environment matches active kernel environment",
    "body": "### Feature description\n\nCurrently, the dask gateway cluster option defaults to the first environment available rather than the environment actually being used by the notebook. If the environment doesn't have dask in it then the next stage just hangs. It is really easy not to notice that the environment being used by dask-gateway is the wrong environment when running through all the cells.\r\n\r\nI propose that we ensure the the default conda environment for dask be the one being actively being used by the jupyter kernel since that is the most sensible default.\r\n\r\nIn the example below we see that the `filesystem/dashboard` env is the default, even though the notebook is running `filesystem/dask`\r\n\r\n```python\r\nfrom dask_gateway import Gateway\r\ngateway = Gateway()\r\n\r\noptions = gateway.cluster_options()\r\noptions\r\n```\r\n\r\n<img width=\"537\" alt=\"image\" src=\"https://user-images.githubusercontent.com/561301/169437177-66ebab09-f7cd-4686-870b-7771fbfd0adf.png\">\r\n\r\n\n\n### Value and/or benefit\n\nMakes using Dask-Gateway less error prone and improves usability.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 142,
    "title": "[ENH] - Ensure that default Dask Gateway environment matches active kernel environment",
    "body": "### Feature description\n\nCurrently, the dask gateway cluster option defaults to the first environment available rather than the environment actually being used by the notebook. If the environment doesn't have dask in it then the next stage just hangs. It is really easy not to notice that the environment being used by dask-gateway is the wrong environment when running through all the cells.\r\n\r\nI propose that we ensure the the default conda environment for dask be the one being actively being used by the jupyter kernel since that is the most sensible default.\r\n\r\nIn the example below we see that the `filesystem/dashboard` env is the default, even though the notebook is running `filesystem/dask`\r\n\r\n```python\r\nfrom dask_gateway import Gateway\r\ngateway = Gateway()\r\n\r\noptions = gateway.cluster_options()\r\noptions\r\n```\r\n\r\n<img width=\"537\" alt=\"image\" src=\"https://user-images.githubusercontent.com/561301/169437177-66ebab09-f7cd-4686-870b-7771fbfd0adf.png\">\r\n\r\n\n\n### Value and/or benefit\n\nMakes using Dask-Gateway less error prone and improves usability.\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "enhancement",
      "ux/ui",
      "configuration"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 143,
    "title": "[META] Jupyter-Scheduler integration enhancements",
    "body": "### Nebari-specific enhancements\r\n- [ ] Create short-lived ARGO_TOKENs on a per user basis\r\n- [ ] Use the Keycloak API to determine which `roles` the user has instead of relying on `groups`\r\n\t- Use this to grant the appropriate level of access to Argo-Workflows\r\n\r\n### Argo-Jupyter-Scheduler enhancements\r\n- [x] Come up with a better name for this extension plugin\r\n- [ ] Generalize the code base so that this extension can be used without Nebari\r\n\t- [ ] Add traitlets to make it easier to mount additional volumes/secrets/configmaps (as a replacement for the Nebari Workflow Controller which we currently use to mount these resources to the workflow pod)\r\n\t- [ ] Inside the `task-runner`, run a clean up command that updates the database to reflect which workflows/cron-workflows are still active\r\n\t- [ ] Test this extension can be used with an external database\r\n- [ ] Limit the environments that appear in the UI to only those that have `papermill` installed\r\n- [ ] Allow modifications to `output-directory` format (for Job Definitions)\r\n  - [ ] Rename output directory to something that's easier to find\r\n- [ ] https://github.com/nebari-dev/argo-jupyter-scheduler/issues/2 \r\n\r\n### Long-term enhancements\r\n- [ ]  Make jupyter-scheduler work directly with kubernetes cron (remove reliance on argo, and more argo into a separate extension)",
    "actual_labels": [
      "enhancement",
      "security",
      "testing",
      "ux/ui",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation\nrules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.",
    "model_name": "base"
  },
  {
    "index": 143,
    "title": "[META] Jupyter-Scheduler integration enhancements",
    "body": "### Nebari-specific enhancements\r\n- [ ] Create short-lived ARGO_TOKENs on a per user basis\r\n- [ ] Use the Keycloak API to determine which `roles` the user has instead of relying on `groups`\r\n\t- Use this to grant the appropriate level of access to Argo-Workflows\r\n\r\n### Argo-Jupyter-Scheduler enhancements\r\n- [x] Come up with a better name for this extension plugin\r\n- [ ] Generalize the code base so that this extension can be used without Nebari\r\n\t- [ ] Add traitlets to make it easier to mount additional volumes/secrets/configmaps (as a replacement for the Nebari Workflow Controller which we currently use to mount these resources to the workflow pod)\r\n\t- [ ] Inside the `task-runner`, run a clean up command that updates the database to reflect which workflows/cron-workflows are still active\r\n\t- [ ] Test this extension can be used with an external database\r\n- [ ] Limit the environments that appear in the UI to only those that have `papermill` installed\r\n- [ ] Allow modifications to `output-directory` format (for Job Definitions)\r\n  - [ ] Rename output directory to something that's easier to find\r\n- [ ] https://github.com/nebari-dev/argo-jupyter-scheduler/issues/2 \r\n\r\n### Long-term enhancements\r\n- [ ]  Make jupyter-scheduler work directly with kubernetes cron (remove reliance on argo, and more argo into a separate extension)",
    "actual_labels": [
      "enhancement",
      "security",
      "testing",
      "ux/ui",
      "configuration"
    ],
    "predicted_labels": "enhancement, ux/ui, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 144,
    "title": "No Release Notes for 2023.12.1",
    "body": "### Describe the bug\n\nNo release notes for 2023.12.1 appear on the page below\r\n\r\nhttps://www.nebari.dev/docs/references/release/\n\n### Expected behavior\n\nN/A\n\n### OS and architecture in which you are running Nebari\n\nN/A\n\n### How to Reproduce the problem?\n\nN/A\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "documentation",
      "release"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update, aws, gcp, azure, security, performance, ux/",
    "model_name": "base"
  },
  {
    "index": 144,
    "title": "No Release Notes for 2023.12.1",
    "body": "### Describe the bug\n\nNo release notes for 2023.12.1 appear on the page below\r\n\r\nhttps://www.nebari.dev/docs/references/release/\n\n### Expected behavior\n\nN/A\n\n### OS and architecture in which you are running Nebari\n\nN/A\n\n### How to Reproduce the problem?\n\nN/A\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "documentation",
      "release"
    ],
    "predicted_labels": "bug, ci/cd, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 145,
    "title": "[BUG] - Why doesn't my code recognize the GPU on Nebari ?",
    "body": "### Describe the bug\n\nI purposely named this issue the name of this missing page: https://www.nebari.dev/docs/how-tos/faq#why-doesnt-my-code-recognize-the-gpus-on-nebari   \ud83d\ude04 \r\n\r\nWe deployed Nebari 2024.03.03 on AWS and we fired up a GPU server successfully (g4dx.xlarge).  \r\n\r\nWe built an environment following these excellent instructions: https://www.nebari.dev/docs/how-tos/pytorch-best-practices/   (although this page contains the broken link above)\r\n\r\nWhen we conda list the environment, it looks good:\r\n``` \r\n08:38 $ conda activate global-pangeo-ml\r\n(global-pangeo-ml) rsignell:~ \r\n08:38 $ conda list cuda\r\n# packages in environment at /home/conda/global/envs/global-pangeo-ml:\r\n#\r\n# Name                    Version                   Build  Channel\r\ncuda-cudart               11.8.89                       0    nvidia\r\ncuda-cupti                11.8.87                       0    nvidia\r\ncuda-libraries            11.8.0                        0    nvidia\r\ncuda-nvrtc                11.8.89                       0    nvidia\r\ncuda-nvtx                 11.8.86                       0    nvidia\r\ncuda-runtime              11.8.0                        0    nvidia\r\nnvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\nnvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\nnvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\npytorch-cuda              11.8                 h7e8668a_5    pytorch\r\n```\r\nbut when we run:\r\n``` \r\ntorch.cuda.is_available()\r\n```\r\nit returns `False`.   \r\n\r\nIs it clear what we did wrong?  \r\nOr what we should do to debug?\r\n\r\n\n\n### Expected behavior\n\nSee above\n\n### OS and architecture in which you are running Nebari\n\nLinux\n\n### How to Reproduce the problem?\n\nSee above\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\nconda 23.3.1\r\nkubernetes 1.29\r\nnebari 2024.03.03\n\n### Compute environment\n\nAWS\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "documentation",
      "question",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 145,
    "title": "[BUG] - Why doesn't my code recognize the GPU on Nebari ?",
    "body": "### Describe the bug\n\nI purposely named this issue the name of this missing page: https://www.nebari.dev/docs/how-tos/faq#why-doesnt-my-code-recognize-the-gpus-on-nebari   \ud83d\ude04 \r\n\r\nWe deployed Nebari 2024.03.03 on AWS and we fired up a GPU server successfully (g4dx.xlarge).  \r\n\r\nWe built an environment following these excellent instructions: https://www.nebari.dev/docs/how-tos/pytorch-best-practices/   (although this page contains the broken link above)\r\n\r\nWhen we conda list the environment, it looks good:\r\n``` \r\n08:38 $ conda activate global-pangeo-ml\r\n(global-pangeo-ml) rsignell:~ \r\n08:38 $ conda list cuda\r\n# packages in environment at /home/conda/global/envs/global-pangeo-ml:\r\n#\r\n# Name                    Version                   Build  Channel\r\ncuda-cudart               11.8.89                       0    nvidia\r\ncuda-cupti                11.8.87                       0    nvidia\r\ncuda-libraries            11.8.0                        0    nvidia\r\ncuda-nvrtc                11.8.89                       0    nvidia\r\ncuda-nvtx                 11.8.86                       0    nvidia\r\ncuda-runtime              11.8.0                        0    nvidia\r\nnvidia-cuda-cupti-cu12    12.1.105                 pypi_0    pypi\r\nnvidia-cuda-nvrtc-cu12    12.1.105                 pypi_0    pypi\r\nnvidia-cuda-runtime-cu12  12.1.105                 pypi_0    pypi\r\npytorch-cuda              11.8                 h7e8668a_5    pytorch\r\n```\r\nbut when we run:\r\n``` \r\ntorch.cuda.is_available()\r\n```\r\nit returns `False`.   \r\n\r\nIs it clear what we did wrong?  \r\nOr what we should do to debug?\r\n\r\n\n\n### Expected behavior\n\nSee above\n\n### OS and architecture in which you are running Nebari\n\nLinux\n\n### How to Reproduce the problem?\n\nSee above\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\nconda 23.3.1\r\nkubernetes 1.29\r\nnebari 2024.03.03\n\n### Compute environment\n\nAWS\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "documentation",
      "question",
      "aws",
      "configuration"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 146,
    "title": "[BUG] - Bump Conda-Store to latest version",
    "body": "### OS system and architecture in which you are running QHub\n\nLinux\n\n### Expected behavior\n\nUses latest 0.4 version of Conda-Store.\n\n### Actual behavior\n\nNot using the latest conda store version\n\n### How to Reproduce the problem?\n\nUse the latest qhub version\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "dependency-update"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 146,
    "title": "[BUG] - Bump Conda-Store to latest version",
    "body": "### OS system and architecture in which you are running QHub\n\nLinux\n\n### Expected behavior\n\nUses latest 0.4 version of Conda-Store.\n\n### Actual behavior\n\nNot using the latest conda store version\n\n### How to Reproduce the problem?\n\nUse the latest qhub version\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\n_No response_\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "dependency-update"
    ],
    "predicted_labels": "bug, ci/cd, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 147,
    "title": "[BUG] Changing the keycloak root user password results in failed redeployment",
    "body": "### OS system and architecture in which you are running QHub\n\nmacOS / Github Ubuntu runner\n\n### Expected behavior\n\nIn our docs, we strongly encourage qhub admins to change their `initial_root_password` however doing so and redeploying results in a failed deployment.\n\n### Actual behavior\n\nThe check for stage `05_kubernetes_keycloak` needs to be relaxed so that we can allow qhub admins to change their root user password without the deployment failing.\n\n### How to Reproduce the problem?\n\nChange root user password and redeploy qhub.\n\n### Command output\n\n```bash session\nINFO:qhub.provider.terraform:terraform=/tmp/terraform/1.0.5/terraform output directory=stages/05-kubernetes-keycloak\r\nINFO:qhub.provider.terraform:terraform output took 0.936 [s]\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\nAttempt 1 failed connecting to keycloak master realm\r\nAttempt 2 failed connecting to keycloak master realm\r\nAttempt 3 failed connecting to keycloak master realm\r\nAttempt 4 failed connecting to keycloak master realm\r\nAttempt 5 failed connecting to keycloak master realm\r\nAttempt 6 failed connecting to keycloak master realm\r\nAttempt 7 failed connecting to keycloak master realm\r\nAttempt 8 failed connecting to keycloak master realm\r\nAttempt 9 failed connecting to keycloak master realm\r\nAttempt 10 failed connecting to keycloak master realm\r\nERROR: unable to connect to keycloak master realm at url=https://quansight-beta.qhub.dev/auth/ with root credentials\n```\n\n\n### Versions and dependencies used.\n\nqhub - main branch\r\n\n\n### Compute environment\n\nGCP\n\n### Integrations\n\nKeycloak\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n\nAllowed labels with descriptions",
    "model_name": "base"
  },
  {
    "index": 147,
    "title": "[BUG] Changing the keycloak root user password results in failed redeployment",
    "body": "### OS system and architecture in which you are running QHub\n\nmacOS / Github Ubuntu runner\n\n### Expected behavior\n\nIn our docs, we strongly encourage qhub admins to change their `initial_root_password` however doing so and redeploying results in a failed deployment.\n\n### Actual behavior\n\nThe check for stage `05_kubernetes_keycloak` needs to be relaxed so that we can allow qhub admins to change their root user password without the deployment failing.\n\n### How to Reproduce the problem?\n\nChange root user password and redeploy qhub.\n\n### Command output\n\n```bash session\nINFO:qhub.provider.terraform:terraform=/tmp/terraform/1.0.5/terraform output directory=stages/05-kubernetes-keycloak\r\nINFO:qhub.provider.terraform:terraform output took 0.936 [s]\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\n/opt/hostedtoolcache/Python/3.8.12/x64/lib/python3.8/site-packages/urllib3/connectionpool.py:1043: InsecureRequestWarning: Unverified HTTPS request is being made to host 'quansight-beta.qhub.dev'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\r\n  warnings.warn(\r\nAttempt 1 failed connecting to keycloak master realm\r\nAttempt 2 failed connecting to keycloak master realm\r\nAttempt 3 failed connecting to keycloak master realm\r\nAttempt 4 failed connecting to keycloak master realm\r\nAttempt 5 failed connecting to keycloak master realm\r\nAttempt 6 failed connecting to keycloak master realm\r\nAttempt 7 failed connecting to keycloak master realm\r\nAttempt 8 failed connecting to keycloak master realm\r\nAttempt 9 failed connecting to keycloak master realm\r\nAttempt 10 failed connecting to keycloak master realm\r\nERROR: unable to connect to keycloak master realm at url=https://quansight-beta.qhub.dev/auth/ with root credentials\n```\n\n\n### Versions and dependencies used.\n\nqhub - main branch\r\n\n\n### Compute environment\n\nGCP\n\n### Integrations\n\nKeycloak\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "gcp",
      "configuration"
    ],
    "predicted_labels": "bug, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 148,
    "title": "Fix code scanning alert - Clusters should be configured with Labels",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/66\r\n",
    "actual_labels": [
      "bug",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 148,
    "title": "Fix code scanning alert - Clusters should be configured with Labels",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\r\n\r\nTracking issue for:\r\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/66\r\n",
    "actual_labels": [
      "bug",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, security, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 149,
    "title": "[ENH] - Remove lingering resources after destroy",
    "body": "We've seen instances where some cloud resources, such as data disks from volume mounts, are kept around after destruction. One such resource is PVs, due to them not being wholly managed by Terraform (e.g., Kubernetes might create it for a specific service/pod, but those are not explicit resources in the source code itself of nebari) thus when destroy is called, they don't show in the clean up graph, hence becoming leftovers. \r\n\r\nOn Azure, we guaranteed that all resources would be cleaned up after removal due to the hierarchy of the resources bonded to the main account. In contrast, this structure does not follow the same category as the other providers.  \r\n\r\nDue to how AWS and GCP handle user data from disk mounts, for example, the data remains even after the clusters are moved unless the project is entirely removed. To address this workaround, we've made some bash scripts and Python executables that talk to the primary providers' SDKs to forcefully handle the destruction. While this handles the problem, it was never a fix, and to follow best practices, we should've been using Terraform to create and destroy it as a whole.   \r\n\r\nIt seems like a good opportunity for us to revise this logic and make complete removal during the destruction the standard response for all providers.\r\n\r\nRight now, the main idea would be to make sure all resources are indeed removed after destruction, and if the user wants to keep them, we could have an extra flag in the destroy command to keep them.\r\n\r\n## Next steps?\r\n- List all orphan resources not handled by terraform right now\r\n- Explore if the corresponding terraform provider for the target cloud gives the option to destroy lingering resources on removal.",
    "actual_labels": [
      "enhancement",
      "maintenance",
      "aws",
      "gcp",
      "azure"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 149,
    "title": "[ENH] - Remove lingering resources after destroy",
    "body": "We've seen instances where some cloud resources, such as data disks from volume mounts, are kept around after destruction. One such resource is PVs, due to them not being wholly managed by Terraform (e.g., Kubernetes might create it for a specific service/pod, but those are not explicit resources in the source code itself of nebari) thus when destroy is called, they don't show in the clean up graph, hence becoming leftovers. \r\n\r\nOn Azure, we guaranteed that all resources would be cleaned up after removal due to the hierarchy of the resources bonded to the main account. In contrast, this structure does not follow the same category as the other providers.  \r\n\r\nDue to how AWS and GCP handle user data from disk mounts, for example, the data remains even after the clusters are moved unless the project is entirely removed. To address this workaround, we've made some bash scripts and Python executables that talk to the primary providers' SDKs to forcefully handle the destruction. While this handles the problem, it was never a fix, and to follow best practices, we should've been using Terraform to create and destroy it as a whole.   \r\n\r\nIt seems like a good opportunity for us to revise this logic and make complete removal during the destruction the standard response for all providers.\r\n\r\nRight now, the main idea would be to make sure all resources are indeed removed after destruction, and if the user wants to keep them, we could have an extra flag in the destroy command to keep them.\r\n\r\n## Next steps?\r\n- List all orphan resources not handled by terraform right now\r\n- Explore if the corresponding terraform provider for the target cloud gives the option to destroy lingering resources on removal.",
    "actual_labels": [
      "enhancement",
      "maintenance",
      "aws",
      "gcp",
      "azure"
    ],
    "predicted_labels": "enhancement, maintenance, aws, gcp, azure, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 150,
    "title": "Move to Terraform v.13.x",
    "body": "",
    "actual_labels": [
      "maintenance",
      "dependency-update",
      "ci/cd"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 150,
    "title": "Move to Terraform v.13.x",
    "body": "",
    "actual_labels": [
      "maintenance",
      "dependency-update",
      "ci/cd"
    ],
    "predicted_labels": "enhancement, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 151,
    "title": "Remove Qhub render step",
    "body": "Rationale for removing:\n\n- Removing the need for QHub render would remove a step",
    "actual_labels": [
      "maintenance",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 151,
    "title": "Remove Qhub render step",
    "body": "Rationale for removing:\n\n- Removing the need for QHub render would remove a step",
    "actual_labels": [
      "maintenance",
      "configuration"
    ],
    "predicted_labels": "enhancement, maintenance, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 152,
    "title": "Open PR to rename `qhub` -> `nebari` in `quansight/qhub`",
    "body": "open against https://github.com/Quansight/qhub/tree/release/2022.10.1",
    "actual_labels": [
      "maintenance",
      "release"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 152,
    "title": "Open PR to rename `qhub` -> `nebari` in `quansight/qhub`",
    "body": "open against https://github.com/Quansight/qhub/tree/release/2022.10.1",
    "actual_labels": [
      "maintenance",
      "release"
    ],
    "predicted_labels": "enhancement, maintenance, ci/cd",
    "model_name": "fine_tuned"
  },
  {
    "index": 153,
    "title": "[bug] Qhub destroy fails for azure deployment, missing terraform-state stage",
    "body": "## Describe the bug\r\n\r\nRelated to #1033, with the recent changes of #1043 we fixed the rendering and provision step but I ended up forgetting to add the same condition to the destroy step.\r\n\r\n- Destroy `01-terraform-state` only if certain conditions match\r\n\r\n## Actual behavior\r\n\r\n```\r\n[terraform]: INFO:qhub.provider.terraform:terraform destroy took 29.955 [s]\r\nINFO:qhub.provider.terraform:terraform=/tmp/terraform/1.0.5/terraform output directory=stages/02-infrastructure/azure\r\nINFO:qhub.provider.terraform:terraform output took 0.424 [s]\r\nINFO:qhub.provider.terraform:terraform init directory=stages/01-terraform-state/azure\r\nINFO:qhub.provider.terraform: terraform at /tmp/terraform/1.0.5/terraform\r\nTraceback (most recent call last):\r\n  File \"/home/vinicius/miniconda3/envs/qhub/bin/qhub\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/__main__.py\", line 7, in main\r\n    cli(sys.argv[1:])\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/cli/__init__.py\", line 52, in cli\r\n    args.func(args)\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/cli/destroy.py\", line 38, in handle_destroy\r\n    destroy_configuration(config)\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/destroy.py\", line 228, in destroy_configuration\r\n    destroy_01_terraform_state(config)\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/destroy.py\", line 62, in destroy_01_terraform_state\r\n    terraform.deploy(\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/provider/terraform.py\", line 57, in deploy\r\n    init(directory)\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/provider/terraform.py\", line 128, in init\r\n    run_terraform_subprocess([\"init\"], cwd=directory, prefix=\"terraform\")\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/provider/terraform.py\", line 111, in run_terraform_subprocess\r\n    if run_subprocess_cmd([terraform_path] + processargs, **kwargs):\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/utils.py\", line 81, in run_subprocess_cmd\r\n    process = subprocess.Popen(\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/subprocess.py\", line 951, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/subprocess.py\", line 1821, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'stages/01-terraform-state/azure'\r\n```\r\n\r\n## How to reproduce\r\n\r\n1. Deploy qhub from main, version `0.4.0.dev70+gfaa2252` based on [faa2252](https://github.com/Quansight/qhub/commit/faa225206077a66ef4ad41d50d656def9ab2412d)\r\n2. Deploy qhub normally for azure\r\n3. execute qhub destroy",
    "actual_labels": [
      "bug",
      "azure"
    ],
    "predicted_labels": "```\nbug, enhancement, documentation\nRules:\n- Choose any number of labels from the allowed set (including zero).\n- Do not include any extra text, code fences, or explanations. Only the CSV line.\n\nAllowed labels with descriptions",
    "model_name": "base"
  },
  {
    "index": 153,
    "title": "[bug] Qhub destroy fails for azure deployment, missing terraform-state stage",
    "body": "## Describe the bug\r\n\r\nRelated to #1033, with the recent changes of #1043 we fixed the rendering and provision step but I ended up forgetting to add the same condition to the destroy step.\r\n\r\n- Destroy `01-terraform-state` only if certain conditions match\r\n\r\n## Actual behavior\r\n\r\n```\r\n[terraform]: INFO:qhub.provider.terraform:terraform destroy took 29.955 [s]\r\nINFO:qhub.provider.terraform:terraform=/tmp/terraform/1.0.5/terraform output directory=stages/02-infrastructure/azure\r\nINFO:qhub.provider.terraform:terraform output took 0.424 [s]\r\nINFO:qhub.provider.terraform:terraform init directory=stages/01-terraform-state/azure\r\nINFO:qhub.provider.terraform: terraform at /tmp/terraform/1.0.5/terraform\r\nTraceback (most recent call last):\r\n  File \"/home/vinicius/miniconda3/envs/qhub/bin/qhub\", line 8, in <module>\r\n    sys.exit(main())\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/__main__.py\", line 7, in main\r\n    cli(sys.argv[1:])\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/cli/__init__.py\", line 52, in cli\r\n    args.func(args)\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/cli/destroy.py\", line 38, in handle_destroy\r\n    destroy_configuration(config)\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/destroy.py\", line 228, in destroy_configuration\r\n    destroy_01_terraform_state(config)\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/destroy.py\", line 62, in destroy_01_terraform_state\r\n    terraform.deploy(\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/provider/terraform.py\", line 57, in deploy\r\n    init(directory)\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/provider/terraform.py\", line 128, in init\r\n    run_terraform_subprocess([\"init\"], cwd=directory, prefix=\"terraform\")\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/provider/terraform.py\", line 111, in run_terraform_subprocess\r\n    if run_subprocess_cmd([terraform_path] + processargs, **kwargs):\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/site-packages/qhub/utils.py\", line 81, in run_subprocess_cmd\r\n    process = subprocess.Popen(\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/subprocess.py\", line 951, in __init__\r\n    self._execute_child(args, executable, preexec_fn, close_fds,\r\n  File \"/home/vinicius/miniconda3/envs/qhub/lib/python3.9/subprocess.py\", line 1821, in _execute_child\r\n    raise child_exception_type(errno_num, err_msg, err_filename)\r\nFileNotFoundError: [Errno 2] No such file or directory: 'stages/01-terraform-state/azure'\r\n```\r\n\r\n## How to reproduce\r\n\r\n1. Deploy qhub from main, version `0.4.0.dev70+gfaa2252` based on [faa2252](https://github.com/Quansight/qhub/commit/faa225206077a66ef4ad41d50d656def9ab2412d)\r\n2. Deploy qhub normally for azure\r\n3. execute qhub destroy",
    "actual_labels": [
      "bug",
      "azure"
    ],
    "predicted_labels": "bug, aws, configuration",
    "model_name": "fine_tuned"
  },
  {
    "index": 154,
    "title": "[ENH] - Remove unnecesary branch input in GHA cloud integration tests",
    "body": "### Feature description\r\n\r\nWe are currently adding `branch` as an input for our cloud provider workflows as a way to specify the _Nebari branch to deploy, test, destroy_. For example: https://github.com/nebari-dev/nebari/blob/855aa14ecc32625a98ec01b017acfc8a4af3c017/.github/workflows/test_aws_integration.yaml#L6-L12\r\n\r\nThis is not needed as [workflows that are run manually](https://docs.github.com/en/actions/managing-workflow-runs-and-deployments/managing-workflow-runs/manually-running-a-workflow#running-a-workflow) (via the `workflow_dispatch` trigger) already offer a way to select the branch from which they should run:\r\n\r\n<img width=\"1427\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f3cbfe39-2ce6-40c6-b0ba-1582db6ebda8\">\r\n\r\n### Value and/or benefit\r\n\r\nNot having duplicate UI items to specify a branch from which to run the workflow (i.e., a dropdown and a textbox) will make things more intuitive for developers triggering the workflows manually.\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "testing",
      "ux/ui"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 154,
    "title": "[ENH] - Remove unnecesary branch input in GHA cloud integration tests",
    "body": "### Feature description\r\n\r\nWe are currently adding `branch` as an input for our cloud provider workflows as a way to specify the _Nebari branch to deploy, test, destroy_. For example: https://github.com/nebari-dev/nebari/blob/855aa14ecc32625a98ec01b017acfc8a4af3c017/.github/workflows/test_aws_integration.yaml#L6-L12\r\n\r\nThis is not needed as [workflows that are run manually](https://docs.github.com/en/actions/managing-workflow-runs-and-deployments/managing-workflow-runs/manually-running-a-workflow#running-a-workflow) (via the `workflow_dispatch` trigger) already offer a way to select the branch from which they should run:\r\n\r\n<img width=\"1427\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f3cbfe39-2ce6-40c6-b0ba-1582db6ebda8\">\r\n\r\n### Value and/or benefit\r\n\r\nNot having duplicate UI items to specify a branch from which to run the workflow (i.e., a dropdown and a textbox) will make things more intuitive for developers triggering the workflows manually.\r\n\r\n### Anything else?\r\n\r\n_No response_",
    "actual_labels": [
      "enhancement",
      "ci/cd",
      "testing",
      "ux/ui"
    ],
    "predicted_labels": "enhancement, ux/ui",
    "model_name": "fine_tuned"
  },
  {
    "index": 155,
    "title": "[BUG] - PR pytests are failing, possibly because repo env vars are not available",
    "body": "### Describe the bug\n\npytests are failing for at least several PRs. Logs indicate that an exception is being raised by many tests due to various env vars not being set:\r\n```\r\n==================================== ERRORS ====================================\r\n_ ERROR at setup of test_set_config_from_environment_variables[nebari_config_options1] _\r\nTraceback (most recent call last):\r\n  File \"/usr/share/miniconda/envs/nebari-dev/lib/python3.10/site-packages/_pytest/runner.py\", line 342, in from_call\r\n  \r\n  <snip>\r\n  \r\n    File \"/usr/share/miniconda/envs/nebari-dev/lib/python3.10/os.py\", line 680, in __getitem__\r\n    raise KeyError(key) from None\r\nKeyError: 'AWS_ACCESS_KEY_ID'\r\n``` \r\nSee full logs here:  \r\nhttps://github.com/nebari-dev/nebari/actions/runs/11386526924/job/31851899812?pr=2752\n\n### Expected behavior\n\nPR Pytests should pass.\n\n### OS and architecture in which you are running Nebari\n\nThis is an issue with the existing github Nebari test pipeline.\n\n### How to Reproduce the problem?\n\nExample failing PRs:  \r\nhttps://github.com/nebari-dev/nebari/pull/2752  \r\nhttps://github.com/nebari-dev/nebari/pull/2730\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "testing",
      "ci/cd",
      "configuration",
      "aws"
    ],
    "predicted_labels": "bug, enhancement, documentation",
    "model_name": "base"
  },
  {
    "index": 155,
    "title": "[BUG] - PR pytests are failing, possibly because repo env vars are not available",
    "body": "### Describe the bug\n\npytests are failing for at least several PRs. Logs indicate that an exception is being raised by many tests due to various env vars not being set:\r\n```\r\n==================================== ERRORS ====================================\r\n_ ERROR at setup of test_set_config_from_environment_variables[nebari_config_options1] _\r\nTraceback (most recent call last):\r\n  File \"/usr/share/miniconda/envs/nebari-dev/lib/python3.10/site-packages/_pytest/runner.py\", line 342, in from_call\r\n  \r\n  <snip>\r\n  \r\n    File \"/usr/share/miniconda/envs/nebari-dev/lib/python3.10/os.py\", line 680, in __getitem__\r\n    raise KeyError(key) from None\r\nKeyError: 'AWS_ACCESS_KEY_ID'\r\n``` \r\nSee full logs here:  \r\nhttps://github.com/nebari-dev/nebari/actions/runs/11386526924/job/31851899812?pr=2752\n\n### Expected behavior\n\nPR Pytests should pass.\n\n### OS and architecture in which you are running Nebari\n\nThis is an issue with the existing github Nebari test pipeline.\n\n### How to Reproduce the problem?\n\nExample failing PRs:  \r\nhttps://github.com/nebari-dev/nebari/pull/2752  \r\nhttps://github.com/nebari-dev/nebari/pull/2730\n\n### Command output\n\n_No response_\n\n### Versions and dependencies used.\n\n_No response_\n\n### Compute environment\n\nNone\n\n### Integrations\n\n_No response_\n\n### Anything else?\n\n_No response_",
    "actual_labels": [
      "bug",
      "testing",
      "ci/cd",
      "configuration",
      "aws"
    ],
    "predicted_labels": "bug, ci/cd, testing",
    "model_name": "fine_tuned"
  },
  {
    "index": 156,
    "title": "Fix code scanning alert - Instances in a subnet should not receive a public IP address by default.",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\n\nTracking issue for:\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/155\n",
    "actual_labels": [
      "bug",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, enhancement, documentation, question, maintenance, ci/cd, testing, release, aws, gcp, security, performance, ux/ui, configuration, dependency-update",
    "model_name": "base"
  },
  {
    "index": 156,
    "title": "Fix code scanning alert - Instances in a subnet should not receive a public IP address by default.",
    "body": "<!-- Warning: The suggested title contains the alert rule name. This can expose security information. -->\n\nTracking issue for:\n- [ ] https://github.com/nebari-dev/nebari/security/code-scanning/155\n",
    "actual_labels": [
      "bug",
      "security",
      "configuration"
    ],
    "predicted_labels": "bug, security, configuration",
    "model_name": "fine_tuned"
  }
]